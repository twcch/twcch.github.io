<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>志謙&#39;s Blog</title>
    <link>http://twcch.io/</link>
    <description>Recent content on 志謙&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>zh-tw</language>
    <lastBuildDate>Tue, 12 Aug 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://twcch.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(Day 14) 決策樹 (Decision Tree)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081201/</link>
      <pubDate>Tue, 12 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081201/</guid>
      <description>&lt;p&gt;Decision Tree 是一種基於條件分支的監督式學習模型，可用於分類與回歸任務。它透過一連串的「是/否」判斷，將資料不斷切分成更純淨的子集，最終形成一個由節點 (Nodes) 與邊 (Edges) 組成的樹狀結構。直覺上，你可以將它想成一連串的決策問句:  「乘客的性別是女性嗎？」 → 是 → 「年齡是否小於 12 歲？」 → 是 → 存活機率高。其最大特色是 可解釋性高，每個決策規則都能清楚對應到特徵與閾值，便於與非技術背景的利害關係人溝通。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;p&gt;決策樹 (Decision Tree) 是一種監督式學習演算法，可用於分類與回歸任務。模型的結構類似一棵樹，由節點 (Node) 與分支 (Branch) 組成&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;內部節點 (Internal Node):  表示一個特徵的條件判斷&lt;/li&gt;&#xA;&lt;li&gt;葉節點 (Leaf Node):  對應模型的預測結果 (類別或數值)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;建構過程是「貪婪式遞迴分割 (Greedy Recursive Splitting)」，根據資訊增益 (Information Gain) 最大或吉尼不純度 (Gini Impurity) 最小的原則進行&lt;/p&gt;&#xA;&lt;h4 id=&#34;模型建構說明&#34;&gt;模型建構說明&lt;/h4&gt;&#xA;&lt;p&gt;決策樹使用「貪婪演算法」建構模型，並無使用梯度下降類優化器，主要特徵:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;每次分裂僅考慮當前最佳特徵 (不做全局最優)&lt;/li&gt;&#xA;&lt;li&gt;使用遞迴方式構建整棵樹&lt;/li&gt;&#xA;&lt;li&gt;終止條件包括:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;節點樣本數過小&lt;/li&gt;&#xA;&lt;li&gt;不再能提升資訊增益&lt;/li&gt;&#xA;&lt;li&gt;最大深度限制&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;節點的特徵選擇:  每次分裂節點時，會從候選特徵中選出最佳分割特徵，其依據如下:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DecisionTreeClassifier&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Information Gain&lt;/li&gt;&#xA;&lt;li&gt;Gini Impurity&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;DecisionTreeRegressor&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MSE 減少量&lt;/li&gt;&#xA;&lt;li&gt;MAE 減少量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;模型優缺點&#34;&gt;模型優缺點&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;優點&lt;/th&gt;&#xA;          &lt;th&gt;缺點&lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;模型結構直觀，可視覺化&lt;/td&gt;&#xA;          &lt;td&gt;易過擬合，尤其是深樹&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;對特徵縮放不敏感 (不需標準化)&lt;/td&gt;&#xA;          &lt;td&gt;對資料擾動敏感 (小變化可能改變樹結構)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;可處理數值與類別型特徵&lt;/td&gt;&#xA;          &lt;td&gt;單棵樹表現有限 (通常需配合集成方法如 Random Forest, XGBoost)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;支援特徵重要性評估&lt;/td&gt;&#xA;          &lt;td&gt;分裂偏向多取值的特徵 (需調整)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;h3 id=&#34;程式實例&#34;&gt;程式實例&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plot_tree&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 讀取資料&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;titanic.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 特徵與標籤&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Pclass&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Sex&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Fare&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 簡單處理缺失值&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_dummies&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# One-Hot Encoding&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Survived&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 訓練 / 測試切分&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;test_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 建立模型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;criterion&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gini&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;max_depth&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 評估&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Accuracy: &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 視覺化&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plot_tree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature_names&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;class_names&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Not Survived&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filled&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;執行結果&#34;&gt;執行結果&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Test Accuracy:  0.8268&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;              precision    recall  f1-score   support&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           0     0.8319    0.9000    0.8646       110&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           1     0.8167    0.7101    0.7597        69&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    accuracy                         0.8268       179&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   macro avg     0.8243    0.8051    0.8122       179&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;weighted avg     0.8260    0.8268    0.8242       179&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Top Feature Importances: &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         feature  importance&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; adult_male_True    0.588332&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     class_Third    0.185393&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            fare    0.156906&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;             age    0.045641&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    class_Second    0.014194&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           parch    0.009535&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      embarked_S    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     alone_False    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;adult_male_False    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       who_woman    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         who_man    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       who_child    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      embarked_C    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      embarked_Q    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     class_First    0.000000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;結果評估&#34;&gt;結果評估&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型整體準確率尚可，但對「正類 (存活)」的 召回率僅 0.71，代表仍有 約 29% 的存活樣本被判成未存活；若你的業務目標重視「找出潛在存活 (或正類)」的能力，現況偏保守。&lt;/li&gt;&#xA;&lt;li&gt;對正類的錯失 (FN) 明顯多於將負類誤判為正類 (FP)。在成本敏感情境（例如「漏抓到的正類」代價更高）下，這是不理想的權衡。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;下一步建議&#34;&gt;下一步建議&lt;/h4&gt;&#xA;&lt;p&gt;我會先驗證的 6 件事:&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 13) 迴歸任務驗證指標</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081101/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081101/</guid>
      <description>&lt;p&gt;分類任務有混淆矩陣作為指標的核心基礎，迴歸任務則建立在誤差分佈 (Error Distribution) 之上。所有迴歸指標，都是在真實值與預測值的差異上進行數學運算。迴歸的評估相對分類簡單，沒有多種 TP、FP 的組合，但每個指標關注的面向、對異常值的敏感度、在商業決策上的意義卻各有不同。&lt;/p&gt;&#xA;&lt;h2 id=&#34;誤差的基本概念&#34;&gt;誤差的基本概念&lt;/h2&gt;&#xA;&lt;p&gt;回歸任務中，誤差 (Error) 定義為:&#xA;$$&#xA;e_i = y_i - \hat{y}_i&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;其中:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$y_i$: 第 i 筆資料的真實值&lt;/li&gt;&#xA;&lt;li&gt;$\hat{y}_i$: 模型的預測值&lt;/li&gt;&#xA;&lt;li&gt;$e_i$: 第 i 筆的殘差 (Residual)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;所有回歸評估指標，都是將 $e_i$ 做數學運算後的結果。&lt;/p&gt;&#xA;&lt;h2 id=&#34;常見回歸驗證指標&#34;&gt;常見回歸驗證指標&lt;/h2&gt;&#xA;&lt;h3 id=&#34;mean-squared-error-mse&#34;&gt;Mean Squared Error (MSE)&lt;/h3&gt;&#xA;&lt;p&gt;將每個誤差平方後取平均。平方的動作會讓大誤差的影響成倍放大，因此 MSE 對異常值 (outlier) 非常敏感。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;特性分析&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;誤差平方 → 大誤差懲罰重，小誤差影響相對被稀釋。&lt;/li&gt;&#xA;&lt;li&gt;單位為「原單位的平方」，如價格 (元) 預測的 MSE 單位是「元²」，因此不易直接解讀大小。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;優點&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;適合用在不能容忍大誤差的場景，例如財務風險控管 (預測錯 10 倍金額的後果極其嚴重)。&lt;/li&gt;&#xA;&lt;li&gt;在模型優化 (特別是最小平方法回歸) 中，MSE 作為損失函數具有良好數學性質 (平滑、可微分)。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;缺點&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 12) 多元分類任務驗證指標</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081001/</link>
      <pubDate>Sun, 10 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081001/</guid>
      <description>&lt;p&gt;多元分類任務驗證指標就只是從二元分類任務驗證指標延伸而來，核心概念一樣是二元分類任務驗證指標，而兩者只是在計算內容上有些許差異，所以指標仍然是 Accuracy、Recall、F1-score 等，所以請讀者先將二元分類任務驗證指標熟練後再來看。&lt;/p&gt;&#xA;&lt;h2 id=&#34;指標介紹&#34;&gt;指標介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;混淆矩陣-confusion-matrix&#34;&gt;混淆矩陣 (Confusion Matrix)&lt;/h3&gt;&#xA;&lt;p&gt;假設我們有 3 個類別 [A, B, C]，模型的預測結果如下:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;實際 \ 預測&lt;/th&gt;&#xA;          &lt;th&gt;A&lt;/th&gt;&#xA;          &lt;th&gt;B&lt;/th&gt;&#xA;          &lt;th&gt;C&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;A&lt;/td&gt;&#xA;          &lt;td&gt;50&lt;/td&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;B&lt;/td&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;45&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;C&lt;/td&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;43&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;列 (row) = 實際標籤&lt;/li&gt;&#xA;&lt;li&gt;行 (column) = 模型預測&lt;/li&gt;&#xA;&lt;li&gt;對角線 (50、45、43) = 預測正確的數量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;從多元分類矩陣取出-tp--fp--fn--tn&#34;&gt;從多元分類矩陣取出 TP / FP / FN / TN&lt;/h3&gt;&#xA;&lt;p&gt;對單一類別 (One-vs-All)，例如要計算「類別 A」的指標:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TP_A = 混淆矩陣中 A–A 的數字 = 50&lt;/li&gt;&#xA;&lt;li&gt;FP_A = 預測為 A 但實際不是 A 的數字總和 = 4 (B→A) + 5 (C→A) = 9&lt;/li&gt;&#xA;&lt;li&gt;FN_A = 實際為 A 但預測不是 A 的數字總和 = 2 (A→B) + 3 (A→C) = 5&lt;/li&gt;&#xA;&lt;li&gt;TN_A = 其他所有正確分類的數字 = 全部總數 - (TP_A + FP_A + FN_A)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;這樣每個類別都能得到對應的 TP、FP、FN、TN&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 11) 二元分類任務驗證指標</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080901/</link>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080901/</guid>
      <description>&lt;p&gt;今天要介紹的是常見的分類任務驗證指標，會以二元分類問題為例，因為多元分類也是用相同的指標，只是計算方式會有所不同而已，預計會用 2-3 天的篇幅介紹完，分類與迴歸任務的驗證指標；先給各位讀者一個正確的觀念，選指標時必須回到業務背景與資料特性，不要迷信某個數值越高越好，真正有價值的模型評估，是能在技術表現與業務需求之間找到平衡。&lt;/p&gt;&#xA;&lt;h2 id=&#34;指標介紹&#34;&gt;指標介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;混淆矩陣-confusion-matrix&#34;&gt;混淆矩陣 (Confusion Matrix)&lt;/h3&gt;&#xA;&lt;p&gt;分類任務的所有核心指標，幾乎都來自 Confusion Matrix，它是用來統計分類模型在測試集上的結果，Confusion Matrix 在 Binary Classification 問題上，它是一個 2x2 表格:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;True Condition - Positive&lt;/th&gt;&#xA;          &lt;th&gt;True Condition - Negative&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Predict Outcome - Positive&lt;/td&gt;&#xA;          &lt;td&gt;TP (True Positve)&lt;/td&gt;&#xA;          &lt;td&gt;FP (False Positve) (誤報)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Predict Outcome - Negative&lt;/td&gt;&#xA;          &lt;td&gt;FN (False Negative) (漏報)&lt;/td&gt;&#xA;          &lt;td&gt;TN (True Negative)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TP: 實際是 Positive，模型也預測 Positive (預測正確)&lt;/li&gt;&#xA;&lt;li&gt;TN: 實際是 Negative，模型也預測 Negative (預測正確)&lt;/li&gt;&#xA;&lt;li&gt;FP: 實際是 Negative，但模型預測 Positive (誤報 / 假警報) (Type I error)&lt;/li&gt;&#xA;&lt;li&gt;FN: 實際是 Positive，但模型預測 Negative (漏報 / 漏檢) (Type II error)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;accuracy-準確率&#34;&gt;Accuracy (準確率)&lt;/h3&gt;&#xA;&lt;p&gt;$$&#xA;Accuracy = \frac{TP + TN}{TP + TN + FP + FN}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 10) 支援向量機 (Support Vector Machine)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080801/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080801/</guid>
      <description>&lt;p&gt;終於來到 SVM，這也是本系列介紹 Machine Learning 中分類演算法的最後一個，當然在機器學習中還有很多的監督式分類演算法，我個人認為相對沒我介紹的這幾個經典，就留給讀者自行學習。從明天開始到進入樹模型之前，我會補充一下，模型 Validation Index 的內容 (用來衡量模型結果好不好)，因為前面飆的有點快，後來有發現這部分也很重要，預計會花 2 ~ 3 天的篇幅來介紹。&lt;/p&gt;&#xA;&lt;p&gt;我們就進入正題，支援向量機 (Support Vector Machine) 是一種監督式學習演算法，泛指支援向量機演算法框架，透過在特徵空間中尋找最能分隔不同類別的超平面 (hyperplane)，並最大化分類邊界 (margin)，可應用於:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分類 (Classification)&lt;/li&gt;&#xA;&lt;li&gt;回歸 (Regression)&lt;/li&gt;&#xA;&lt;li&gt;異常檢測 (Anomaly Detection)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;但是回歸的部分非常少用到 Support Vector Regression 本系列就不說明這塊；至於異常檢測的應用又稱 OneClass SVM 目前沒有規劃，這是一種無監督式學習的技術，專門在做 Anomaly Detection 的任務，因為本系列規劃在樹模型介紹完成後，會進入深度學習篇章，所以 OneClass SVM 的部分如果後續有篇幅的話會再補充，如果沒有也請讀者自行學習；所以本篇會以 SVM 應用在分類任務 (Support Vector Classification) 上來詳細說明。&lt;/p&gt;&#xA;&lt;h2 id=&#34;svm-解決了什麼問題&#34;&gt;SVM 解決了什麼問題?&lt;/h2&gt;&#xA;&lt;p&gt;在詳細介紹 SVM 之前，要先說明一下 SVM 到底要解決什麼問題，我們先回到 Day 5 介紹的 Logistic Regression，假設同一組數據做分類，可能會發生以下狀況，我們先看到 Logistic Regression 的部分，大家會發現看起來分類正確，但是那條線怎麼切得怪怪的，這也是 Logistic Regression 的問題，會造成模型泛化性不夠好，因為 Logistic Regression 對於這部分沒有進行處理。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/twcch/drive/raw/main/images/Image_2025-08-08_13-21-09.png&#34; alt=&#34;Image_2025-08-08_13-21-09.png&#34;&gt;&#xA;&lt;a href=&#34;https://b5031631512567.medium.com/logistic-regression-%E7%BE%85%E5%90%89%E6%96%AF%E5%9B%9E%E6%AD%B8-support-vector-machine-svm-%E5%81%9Aa-b%E5%88%86%E9%A1%9E-82aa5e5edaf8&#34;&gt;圖片來源: https://b5031631512567.medium.com/logistic-regression-羅吉斯回歸-support-vector-machine-svm-做a-b分類-82aa5e5edaf8&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 9) 樸素貝氏分類器 (Naive Bayes Classifier)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080701/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080701/</guid>
      <description>&lt;p&gt;前幾天的討論中，我們已經探討了迴歸分析、邏輯迴歸，以及最近兩天介紹的 K-Nearest Neighbors (KNN)。今天要討論的是另一種基礎且直覺性極強的分類演算法: 樸素貝氏分類器 (Naive Bayes Classifier)。儘管樸素貝氏分類器的基本原理非常簡單，甚至經常被視為基礎模型，但在實務應用中，它仍然是許多場合的首選，尤其是在文本分類領域，例如垃圾郵件分類與情感分析。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;p&gt;Naive Bayes 的核心思想來自貝氏定理 (Bayes&amp;rsquo; Theorem):&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;P(y|X) = \frac{P(X|y)P(y)}{P(X)}&#xA;$$&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$P(y|X)$: 在給定特徵 $X$ 下的目標 $y$ 的後驗機率 (posterior probability)&lt;/li&gt;&#xA;&lt;li&gt;$P(X|y)$: 在已知目標 $y$ 下觀察到特徵 $X$ 的可能性 (likelihood)&lt;/li&gt;&#xA;&lt;li&gt;$P(y)$: 目標 $y$ 的先驗機率 (prior probability)&lt;/li&gt;&#xA;&lt;li&gt;$P(X)$: 觀察到特徵 $X$ 的總體機率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;但直接計算 $P(X|y)$ 是困難的，尤其當特徵數量龐大且互相關聯時。因此 Naive Bayes 做了一個極簡的假設——「條件獨立假設 (Conditional Independence Assumption)」，即假設特徵之間彼此獨立:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;P(X|y) = P(x_1|y) \times P(x_2|y) \times \cdots \times P(x_n|y)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;這個假設大幅簡化了問題，讓計算變得非常快速且易於實現。雖然這個假設在現實世界中往往不成立，但 Naive Bayes 的實務表現卻通常仍然相當穩健。&lt;/p&gt;&#xA;&lt;h4 id=&#34;naive-bayes-常見種類&#34;&gt;Naive Bayes 常見種類&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Gaussian Naive Bayes (高斯樸素貝氏): 假設特徵為連續數值，並服從高斯分布。&lt;/li&gt;&#xA;&lt;li&gt;Multinomial Naive Bayes (多項式樸素貝氏): 特別適用於文本數據，特徵通常為計數 (例如詞頻)。&lt;/li&gt;&#xA;&lt;li&gt;Bernoulli Naive Bayes (伯努利樸素貝氏): 特徵為二元變數 (例如詞的出現與否)。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;適用情境&#34;&gt;適用情境&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;特徵數量大且離散，尤其文本分類&lt;/li&gt;&#xA;&lt;li&gt;需要模型快速訓練與預測&lt;/li&gt;&#xA;&lt;li&gt;基準模型 (Baseline Model) 的建立&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;限制條件&#34;&gt;限制條件&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;特徵之間存在強烈相關性時，效果可能較差&lt;/li&gt;&#xA;&lt;li&gt;無法捕捉特徵之間的交互作用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;p&gt;本次實作會以多項式 Naive Bayes 為例，因為它在文本分類中表現卓越，並且可展示 Naive Bayes 的強項: 速度快、表現穩定且容易理解。我們將使用經典的 SMS Spam Collection 資料集，透過 Naive Bayes 分辨垃圾訊息與正常訊息，這個過程就不過多敘述。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 8) K-近鄰 (K-Nearest Neighbors)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080601/</link>
      <pubDate>Wed, 06 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080601/</guid>
      <description>&lt;p&gt;K-近鄰 (K-Nearest Neighbors; KNN) 是一種很直學的機器學習演算法。它沒有模型參數、沒有訓練過程，卻可以在某些任務上有不錯的效果。它的核心理念只有一句話: 「你是誰，由你周圍最像你的人決定。」&lt;/p&gt;&#xA;&lt;p&gt;K-近鄰的預測邏輯其實就是投票機制。當一筆新資料進來時，K-近鄰會計算它與訓練集中每一筆資料的距離，選出最近的 K 筆，根據這些鄰居的標籤來進行分類或回歸。&lt;/p&gt;&#xA;&lt;p&gt;舉個例子，如果你住進一個新的社區，而這個社區 5 戶人家中有 4 戶都是教師，那麼你很可能也被視為教師。這就是K-近鄰的基本邏輯：用「距離」定義相似度，用「投票」進行預測。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;無需訓練、實作簡單&lt;/li&gt;&#xA;&lt;li&gt;可處理多類別分類問題&lt;/li&gt;&#xA;&lt;li&gt;非常適合 baseline 模型或少量資料的場景&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;h4 id=&#34;運作原理&#34;&gt;運作原理&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;定義距離度量: 最常見的是歐幾里得距離。&lt;/li&gt;&#xA;&lt;li&gt;標準化資料: 避免不同特徵尺度影響距離計算。&lt;/li&gt;&#xA;&lt;li&gt;選擇 K 值: K 值太小容易過擬合，太大容易欠擬合。&lt;/li&gt;&#xA;&lt;li&gt;查找最近鄰: 找出距離最近的 K 筆資料。&lt;/li&gt;&#xA;&lt;li&gt;分類或回歸: 分類就多數決，回歸就取平均。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;模型評估指標&#34;&gt;模型評估指標&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Accuracy: 整體正確率&lt;/li&gt;&#xA;&lt;li&gt;Precision / Recall / F1-score: 評估正例預測品質與召回&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;適用情境&#34;&gt;適用情境&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;資料量不大、特徵數量低的任務&lt;/li&gt;&#xA;&lt;li&gt;資料本身具備明顯群聚性質&lt;/li&gt;&#xA;&lt;li&gt;需要快速做出初步 baseline 的時候&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;限制條件&#34;&gt;限制條件&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;計算成本高 (尤其資料量大時)&lt;/li&gt;&#xA;&lt;li&gt;對資料標準化非常敏感&lt;/li&gt;&#xA;&lt;li&gt;高維度下效果會大幅下降 (維度災難)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;p&gt;這個 K-近鄰的案例，我們來聊聊簡單的操參數實驗，我們先準備一組資料，這個過程就不過多敘述。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;seaborn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sns&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;make_classification&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.neighbors&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KNeighborsClassifier&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;accuracy_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_val_score&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 資料產生&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;make_classification&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;n_samples&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_informative&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_redundant&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;n_clusters_per_class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;class_sep&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 資料分割與標準化&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;test_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train_std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在建模的部分就跟之前不一樣，而是在外層寫了一個迴圈，因為 K-近鄰的 K 值，沒有人知道要用多少，K=1 表示我只抓最近的一個來比，完全就沒有那種投票的概念，所以 k 不應該選 1，再來是怕有平票的問題所以 k 會以奇數為主，而且 k 如果太小會有個問題，容易過擬合，越小越準，那怎麼辦? 所以這邊搭配了 Cross Validation 做設計，可以避免這個問題 (Cross Validation 請讀者自行找資源學習)。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 7) 回顧迴歸：從線性邏輯到學習本質</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080501/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080501/</guid>
      <description>&lt;p&gt;前面 5 天我們聚焦於「回歸系列」模型: 線性迴歸 (Linear Regression)、多項式迴歸 (Polynomial Regression)、正則化迴歸 (Lasso / Ridge / ElasticNet Regression) 以及邏輯迴歸 (Logistic Regression)。雖然它們名稱上都掛著「Regression」，實則涵蓋了連續值預測與分類任務兩大主題。&lt;/p&gt;&#xA;&lt;p&gt;在正式進入其他學習範式前，我想透過這篇文章做一個小結，幫助讀者重新理解「迴歸模型的核心精神」，並進一步延伸思考「什麼是機器學習的學習」。&lt;/p&gt;&#xA;&lt;h2 id=&#34;迴歸模型統整與對比&#34;&gt;迴歸模型統整與對比&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;模型&lt;/th&gt;&#xA;          &lt;th&gt;任務類型&lt;/th&gt;&#xA;          &lt;th&gt;是否可擴展非線性&lt;/th&gt;&#xA;          &lt;th&gt;是否有正則化&lt;/th&gt;&#xA;          &lt;th&gt;適用場景&lt;/th&gt;&#xA;          &lt;th&gt;代表限制&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Linear Regression&lt;/td&gt;&#xA;          &lt;td&gt;迴歸&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;數據關係明確線性、特徵少時&lt;/td&gt;&#xA;          &lt;td&gt;對離群值、共線性敏感&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Polynomial Regression&lt;/td&gt;&#xA;          &lt;td&gt;迴歸&lt;/td&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;存在非線性曲線關係時&lt;/td&gt;&#xA;          &lt;td&gt;過度擬合風險高&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Lasso / Ridge / ElasticNet&lt;/td&gt;&#xA;          &lt;td&gt;迴歸&lt;/td&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;✅&lt;/td&gt;&#xA;          &lt;td&gt;高維度資料、需特徵選擇時&lt;/td&gt;&#xA;          &lt;td&gt;模型可解釋性略減&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Logistic Regression&lt;/td&gt;&#xA;          &lt;td&gt;分類&lt;/td&gt;&#xA;          &lt;td&gt;否&lt;/td&gt;&#xA;          &lt;td&gt;✅ (可搭配)&lt;/td&gt;&#xA;          &lt;td&gt;二元分類、機率預測、可解釋性要求高場景&lt;/td&gt;&#xA;          &lt;td&gt;不適合複雜非線性邊界&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;這四種模型本質上都假設資料可以被一個「參數化的函數」所建模，且可以透過某種「最小化損失」的方式來進行學習。而這種最小化行為，正是機器學習中最常見的學習模式: 梯度下降法 (Gradient Descent)。&lt;/p&gt;&#xA;&lt;h2 id=&#34;為什麼梯度下降能學習&#34;&gt;為什麼梯度下降能「學習」?&lt;/h2&gt;&#xA;&lt;p&gt;這是一個我自己也還在思考的問題。梯度下降看似只是數學上的最小化技巧，但其實它蘊含了學習的邏輯核心: 錯誤導向的自我修正。&lt;/p&gt;&#xA;&lt;p&gt;每一次模型的預測錯了，就利用這個錯誤的方向與程度，去修正模型的參數，使下一次預測更好。這種機制背後隱含的三個條件，值得特別點出:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;✅ 存在可微分的損失函數&lt;/li&gt;&#xA;&lt;li&gt;✅ 模型是參數化的 (parameters 可調整)&lt;/li&gt;&#xA;&lt;li&gt;✅ 可以反覆試誤 (迭代優化)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;符合上述條件，模型便可以「學習」。也正因如此，這四個回歸模型雖然類型不同 (分類 / 迴歸)、形式不同 (線性 / 非線性 / 正則化)，但都共享「透過梯度下降調整參數」這一關鍵本質。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 6) 邏輯迴歸 (多項式 &#43; 正規化)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080401/</link>
      <pubDate>Mon, 04 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080401/</guid>
      <description>&lt;p&gt;在上一篇中，我們深入介紹了邏輯迴歸的模型邏輯、損失函數與分類行為。這篇則要進一步延伸這個經典模型，回答一個關鍵問題: 邏輯迴歸能否結合多項式特徵與正規化機制，來對抗非線性與過擬合問題?&lt;/p&gt;&#xA;&lt;p&gt;在實務中，這樣的需求非常常見，但你可能很少看到「多項式邏輯迴歸」或「正規化邏輯迴歸」這樣的說法。雖然命名不常見，但本質上邏輯迴歸完全可以與這兩個技巧結合使用，而且這種搭配在複雜資料下是極具威力的實務技巧。&lt;/p&gt;&#xA;&lt;h2 id=&#34;為什麼邏輯迴歸可以搭配多項式與正規化&#34;&gt;為什麼邏輯迴歸可以搭配多項式與正規化?&lt;/h2&gt;&#xA;&lt;h3 id=&#34;邏輯迴歸其實是線性模型&#34;&gt;邏輯迴歸其實是線性模型&lt;/h3&gt;&#xA;&lt;p&gt;邏輯迴歸雖然應用在分類任務f，但本質仍是一種「線性模型」:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\hat{y} = \sigma(\beta_0 + \mathbf{x}^\top \boldsymbol{\beta})&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;這表示它只能建構一條線性的 decision boundary。當你的資料本身具有非線性邊界時，例如 XOR 類型的資料，這條邏輯迴歸線就顯得力不從心。&lt;/p&gt;&#xA;&lt;p&gt;解法之一，就是在原始特徵上做多項式擴展 (Polynomial Feature Expansion)——也就是增加特徵空間的非線性組合，例如 $x_1^2$、$x_1 \cdot x_2$ 等，來幫助模型在更高維度中建立線性可分的邊界。&lt;/p&gt;&#xA;&lt;p&gt;這與之前我們在線性迴歸所談的邏輯迴歸原理一樣，只是這次應用在分類問題中。&lt;/p&gt;&#xA;&lt;h3 id=&#34;邏輯迴歸也容易過擬合&#34;&gt;邏輯迴歸也容易過擬合&lt;/h3&gt;&#xA;&lt;p&gt;一旦你使用多項式特徵，特徵數暴增，就可能發生過擬合，這時就需要正規化 (Regularization) 機制來抑制模型複雜度。&lt;/p&gt;&#xA;&lt;p&gt;與 Linear Regression 一樣，邏輯迴歸可以透過 L1 或 L2 懲罰項達到正規化的目的:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;L2 (Ridge): 抑制權重值變得太大&lt;/li&gt;&#xA;&lt;li&gt;L1 (Lasso): 推動部分權重變為 0，具有特徵選擇效果&lt;/li&gt;&#xA;&lt;li&gt;Elastic Net: L1 + L2 混合調整&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;值得注意的是，在 PyTorch 中，optimizer 的 weight_decay 只對應 L2，若要做 L1，則需自行加上額外的懲罰項。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;p&gt;這個案例也一樣，使用 PyTorch 來實現，透過這段程式碼來窺探 Logistic Regression 的細節。但是還是要再次聲明一下，不論是機器學習演算法，還是說什麼排序的那些算法，你自己寫的打概率打不過這種主流套件做出來的方法，因為這些方法可能經過 10 幾年以上的迭代，不斷地維護與優化產生的，所以如果是學習的話可以自己做，但是正式要使用的話還是建議直接用這些現成的方法，表現往往更加優秀。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 5) 邏輯迴歸 (Logistic Regression)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080301/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080301/</guid>
      <description>&lt;p&gt;邏輯迴歸 (Logistic Regression) 是一種常見的分類模型，主要用於預測二元分類或多元分類，有別於先前的線性迴歸是用來預測無邊界的連數據值，而邏輯迴歸間單來說就是預測有邊界的不連續數值，如 [0, 1], [1, 2, 3]。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;p&gt;那邏輯回歸是如何運作? 其實不論是哪種邏輯迴歸，底層都是先透過線性迴歸來預測，只是分別透過不同的激活函數與損失函數來處理，但是邏輯迴歸一般來說還是比較常用於二元分類，來看看以下流程:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假設有一條線性迴歸方程式: $\hat{y} = \beta_0 + \mathbf{x}^\top \boldsymbol{\beta}$。(注意: 這條不是最佳的線性迴歸線)&lt;/li&gt;&#xA;&lt;li&gt;會針對前述的線性迴歸方程式結果，透過 sigmoid 函數，將結果轉換成 [0, 1]&lt;/li&gt;&#xA;&lt;li&gt;假設損失函數 (Cost Function): Binary Cross Entropy&lt;/li&gt;&#xA;&lt;li&gt;最後使用梯度下降 (Batch Gradient Descent) 來最小化損失函數，找出最佳的邏輯迴歸線&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;以上就是二元分類邏輯迴歸的原理，那麼我們來看看多元分類邏輯迴歸是如何處理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假設有一條線性迴歸方程式: $\hat{y} = \beta_0 + \mathbf{x}^\top \boldsymbol{\beta}$。(注意: 這條不是最佳的線性迴歸線)&lt;/li&gt;&#xA;&lt;li&gt;會針對前述的線性迴歸方程式結果，透過 softmax 函數，將結果轉換成機率總和為 1 的組合&lt;/li&gt;&#xA;&lt;li&gt;假設損失函數 (Cost Function): Categorical Cross Entropy&lt;/li&gt;&#xA;&lt;li&gt;最後使用梯度下降 (Batch Gradient Descent) 來最小化損失函數，找出最佳的邏輯迴歸線&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;可以看出不同的邏輯迴歸，只是分別透過不同的激活函數與損失函數來處理，雖然邏輯迴歸可以用於多元分類，但是一般來說還是比較常用於二元分類。&lt;/p&gt;&#xA;&lt;h4 id=&#34;模型評估指標&#34;&gt;模型評估指標&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Accuracy: 整體正確率&lt;/li&gt;&#xA;&lt;li&gt;Precision / Recall / F1-score: 評估正例預測品質與召回&lt;/li&gt;&#xA;&lt;li&gt;ROC-AUC: 考量不同閾值下模型分類能力&lt;/li&gt;&#xA;&lt;li&gt;Confusion Matrix: TP、TN、FP、FN 分佈&lt;/li&gt;&#xA;&lt;li&gt;Log Loss: 概率預測與實際標籤差異&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;適用情境&#34;&gt;適用情境&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Target 為二元分類 (0/1、是/否) 或多元分類&lt;/li&gt;&#xA;&lt;li&gt;需要同時獲得概率估計與可解釋性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;限制條件&#34;&gt;限制條件&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多重共線性: 高度相關特徵會影響係數穩定性&lt;/li&gt;&#xA;&lt;li&gt;極端值敏感: 離群點可能顯著扭曲模型&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;p&gt;這個案例開始為了讓讀者有更好的感覺模型的過程，會分別使用 sklearn 與 PyTorch 來建模。但是必須先聲明，無論是手動撰寫或是透過 PyTorch 來模擬出來，都不一定有辦法比 sklearn 提供的演算法來得更優秀，所以除非有特殊目的，否則使用 sklearn 提供的演算法效能與準確性都會較高。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 4) 正規化迴歸 (Regularization Regression)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080201/</link>
      <pubDate>Sat, 02 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080201/</guid>
      <description>&lt;p&gt;延續昨日的多項式迴歸中，我們觀察到一個現象: 雖然二次特徵提升了模型的表現，但同時也引入過擬合 (Overfitting) 風險。這是因為當特徵數量暴增，模型就會變得過於「貪婪」，試圖將每個資料點都擬合得極好，結果反而喪失了在新資料上的泛化 (Generalization) 能力。&lt;/p&gt;&#xA;&lt;p&gt;那怎麼辦? 就是在多項式迴歸的基礎上，限制模型的自由度，也就是今天要介紹的——正則化回歸 (Regularized Regression)。&lt;/p&gt;&#xA;&lt;p&gt;這是一種透過在模型參數加上限制，以提升泛化能力 (該操作並非為了提高準確度)，讓它在「解釋資料」與「控制複雜度」間取得平衡。最常見的三種正則化技術分別為:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;套索回歸 (Lasso Regression): L1 Normalization&lt;/li&gt;&#xA;&lt;li&gt;脊回歸 (Ridge Regression): L2 Normalization&lt;/li&gt;&#xA;&lt;li&gt;Elastic Net Regression: L1 + L2 Normalization&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;p&gt;先回到 Day 2 的線性迴歸，線性迴歸如何找出最佳的迴歸線?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;先設定損失函數 (Cost Function) 假設為 $MSE = \frac{1}{2n} \sum\limits_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}$。&lt;/li&gt;&#xA;&lt;li&gt;再使用梯度下降 (Batch Gradient Descent) 來最小化損失函數。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;而所謂的正規化迴歸就是在損失函數加上懲罰項，而前述那些不同的正規化迴歸名稱，就只是懲罰項的差異而已，以下是正規化迴歸的懲罰項:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;套索迴歸: $\lambda \sum |\beta_i|$&lt;/li&gt;&#xA;&lt;li&gt;脊迴歸: $\lambda \sum \beta_i^2$&lt;/li&gt;&#xA;&lt;li&gt;Elastic Net Regression: $\lambda_1 \sum |\beta_i| + \lambda_2 \sum \beta_i^2$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;我們先來看看這幾種正規化的效果差異:&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 3) 多項式迴歸 (Polynomial Regression)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080101/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080101/</guid>
      <description>&lt;p&gt;昨天介紹了線性迴歸 (Linear Regression)，它適合用來處理特徵與目標之間為線性關係的情境。然而，真實世界的資料往往並非純粹線性，而是呈現複雜的非線性關係，例如曲線、拋物線、甚至更複雜的波動趨勢。&lt;/p&gt;&#xA;&lt;p&gt;就有了多項式特徵 (Polynomial Feature) 的出現，而線性迴歸搭配多項式特徵，就是所謂的多項式迴歸 (Polynomial Regression)，便是為了解決線性模型難以處理的非線性問題。它的核心概念非常簡單就是透過對特徵進行多項式轉換，使模型能夠捕捉非線性趨勢。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;p&gt;這塊幾乎與昨天介紹的線性迴歸一樣，重複的部分就不多做介紹。因為多項式迴歸本質上仍是線性迴歸，但特徵空間經過非線性轉換，讓模型能擬合更複雜的曲線。以下為多項式迴歸的公式:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d&#xA;$$&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;d 稱為 polynomial degree (多項式階數)，是模型中最重要的超參數之一。&lt;/li&gt;&#xA;&lt;li&gt;特徵不只可以加入單一變數的高次項，也可加入多個變數間的交互項 (例如 $x_1x_2$)。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;運作原理&#34;&gt;運作原理&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假設方程式 (degree = 3): $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$&#xA;&lt;ul&gt;&#xA;&lt;li&gt;透過將輸入特徵 $x$ 映射為高階次多項式 (如 $x^2, x^3, \dots$)，使模型能擬合彎曲或非線性趨勢，特徵會經過變換形成新的變數，然後再應用一般線性回歸模型進行估計。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;degree = 3 (對所有 features 做所有「總次數 ≤ 3」的項次組合)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多項式特徵的處理會產生新的特徵&lt;/li&gt;&#xA;&lt;li&gt;要特別注意，如果在特徵工程有人工建立交互項，不可直接使用 PolynomialFeatures 來處理，因為不會辨識你手動做出的交互項，會產生重複或邏輯不一致的問題，要特別處理。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;舉例: 假設有一組資料，特徵有 [&amp;lsquo;x1&amp;rsquo;, &amp;lsquo;x2&amp;rsquo;]，設定 degree=3 做 PolynomialFeatures，這組資料的特徵會變成 [&amp;lsquo;x1&amp;rsquo;, &amp;lsquo;x2&amp;rsquo;, &amp;lsquo;x1^2&amp;rsquo;, &amp;lsquo;x1 x2&amp;rsquo;, &amp;lsquo;x2^2&amp;rsquo;, &amp;lsquo;x1^3&amp;rsquo;, &amp;lsquo;x1^2 x2&amp;rsquo;, &amp;lsquo;x1 x2^2&amp;rsquo;, &amp;lsquo;x2^3&amp;rsquo;]，他會自動做交互項處理，如果有手動生成交互項就不能再做 PolynomialFeatures&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;適用情境&#34;&gt;適用情境&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;當資料呈現曲線趨勢時，線性回歸無法捕捉其變化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;限制條件&#34;&gt;限制條件&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;degree 過高，容易導致 Overfitting (尤其在資料量小時)&lt;/li&gt;&#xA;&lt;li&gt;高維度下容易產生特徵爆炸&lt;/li&gt;&#xA;&lt;li&gt;對比 Linear Regression 其模型可解釋性下降&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型實作&#34;&gt;模型實作&lt;/h2&gt;&#xA;&lt;h3 id=&#34;資料集介紹&#34;&gt;資料集介紹&lt;/h3&gt;&#xA;&lt;p&gt;將使用經典的 Boston Housing Dataset 為例。由於 scikit-learn 已移除該資料集，我們改採自 Carnegie Mellon University 所提供的公開版本。樣本內容如下:&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 2) 線性迴歸 (Linear Regression)</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073101/</link>
      <pubDate>Thu, 31 Jul 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073101/</guid>
      <description>&lt;p&gt;線性迴歸 (Linear Regression) 是統計學中的一種預測方法，主要分為簡單線性迴歸 (Simple Linear Regression) 與多元線性迴歸 (Multiple Linear Regression)，又稱複迴歸，以及其他變形的迴歸等，但在線性迴歸中，通常會有 1~N 個自變數 (Independent Variable) X，也可以稱作特徵 (Feature)；和 1 個因變數 (Dependent Variable) Y，也可以稱作目標 (Target)。而最終目的就是找出一條最佳迴歸線，來擬合這些數據點，便可以用來預測未來的數據點。&lt;/p&gt;&#xA;&lt;h2 id=&#34;模型介紹&#34;&gt;模型介紹&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型邏輯與核心概念&#34;&gt;模型邏輯與核心概念&lt;/h3&gt;&#xA;&lt;h4 id=&#34;線性迴歸假設&#34;&gt;線性迴歸假設&lt;/h4&gt;&#xA;&lt;p&gt;統計學線性迴歸的經典的五大假設:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;線性關係: 自變數與因變數之間存在線性關係&lt;/li&gt;&#xA;&lt;li&gt;誤差項獨立 (Independence): 誤差項之間沒有相互關係&lt;/li&gt;&#xA;&lt;li&gt;同標準差性 (Homoscedasticity): 對於所有的自變數，誤差項具有相同的標準差&lt;/li&gt;&#xA;&lt;li&gt;誤差項常態性 (Normality of Errors): 誤差項應該成常態分佈&lt;/li&gt;&#xA;&lt;li&gt;高度共線性 (Multicollinearity): 自變數間高度線性相關&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;看到這邊會想說，為什麼要特別註明統計學? 跟機器學習無關? 先記住一句話「統計學重推論，機器學習重預測」，很多假設跟機器學習中的線性迴歸模型還真的沒有太大的關係，但是也不代表，機器學習模型完全沒有假設，但是相對比較不重要，這也是為什麼很多仿間的機器學習教材都會忽略假設這塊。&lt;/p&gt;&#xA;&lt;p&gt;總而言之，機器學習模型不像統計學模型需要那麼嚴謹的假設，但是若違反某些假設，也是會影響機器學習模型的表現，也會使得模型只能用於預測，無法用於推論，以下簡單整理假設對統計模型與機器學習模型的影響:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;假設&lt;/th&gt;&#xA;          &lt;th&gt;對傳統統計模型影響&lt;/th&gt;&#xA;          &lt;th&gt;對機器學習影響&lt;/th&gt;&#xA;          &lt;th&gt;建議處理方式&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;線性關係&lt;/td&gt;&#xA;          &lt;td&gt;✅ 極高 (核心假設)&lt;/td&gt;&#xA;          &lt;td&gt;❌ 可忽略 (可透過特徵轉換處理)&lt;/td&gt;&#xA;          &lt;td&gt;用非線性模型 / 特徵轉換&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;誤差獨立性&lt;/td&gt;&#xA;          &lt;td&gt;✅ 高 (推論與解釋需此條件支持)&lt;/td&gt;&#xA;          &lt;td&gt;✅ 高 (對 generalization 有直接影響)&lt;/td&gt;&#xA;          &lt;td&gt;使用適當資料分割策略&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;同變異性&lt;/td&gt;&#xA;          &lt;td&gt;✅ 中高 (影響參數估計的信度)&lt;/td&gt;&#xA;          &lt;td&gt;❌ 可忽略 (模型的估計值仍然準，但 p-value、CI 失真)&lt;/td&gt;&#xA;          &lt;td&gt;變數轉換、加權最小平方法&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;誤差常態性&lt;/td&gt;&#xA;          &lt;td&gt;✅ 中高 (特定推論工具須常態性支持)&lt;/td&gt;&#xA;          &lt;td&gt;❌ 可忽略&lt;/td&gt;&#xA;          &lt;td&gt;若僅做預測可忽略&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;共線性&lt;/td&gt;&#xA;          &lt;td&gt;✅ 高 (嚴重影響模型可解釋性與推論)&lt;/td&gt;&#xA;          &lt;td&gt;❌ 可忽略 (但建議修正以利解釋)&lt;/td&gt;&#xA;          &lt;td&gt;VIF、降維、正則化&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;運作原理&#34;&gt;運作原理&lt;/h4&gt;&#xA;&lt;p&gt;我們先回到線性迴歸的用途與目的，簡單來說就是「找出一條最佳直線，來擬合這些數據點，便可以用來預測未來的數據點」，如何找出最佳直線? 本文會簡單的介紹一下，詳細過程與原理，再請讀者自行尋找其他資源暸解。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Day 1) 介紹與準備</title>
      <link>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073001/</link>
      <pubDate>Wed, 30 Jul 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073001/</guid>
      <description>&lt;p&gt;在學習機器學習 (Machine Learning) 的過程中，可能會陷入兩種極端，一種是只會調用套件 (套模)，模型背後的機制一知半解，遇到問題只能「換模型試試看」，或者是過度陷入數學細節，花大量時間推導公式，卻無法轉化為實際應用與模型選擇能力。&lt;/p&gt;&#xA;&lt;p&gt;我本身是從商業分析背景轉入人工智慧領域的研究者。這段轉型過程中，逐漸體會到: 真正困難的不是學會用模型，而是理解模型為什麼有效、什麼時候該用、什麼時候該換、用了之後該觀察什麼訊號。這促使我開始重新梳理各類常見演算法的行為與應用邏輯。&lt;/p&gt;&#xA;&lt;p&gt;因此，我決定透過這次 iThome 鐵人賽的機會，整理與統整常見演算法的核心概念，並將每一篇視為一場與模型的深度對談。&lt;/p&gt;&#xA;&lt;h2 id=&#34;系列架構說明&#34;&gt;系列架構說明&lt;/h2&gt;&#xA;&lt;p&gt;本系列分為兩大部分:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;經典機器學習模型: 聚焦於 Regression、Classification、Clustering 等常見方法，強調模型背後的核心邏輯、適用情境與評估指標。&lt;/li&gt;&#xA;&lt;li&gt;深度學習模型: 介紹常見神經網路架構，如全連接神經網路 (FCNN)、CNN、RNN、Transformer 等，並探討它們對資料型態、任務種類的適應性與限制。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;每篇文章皆會包含模型概念說明與簡潔的 Python 範例實作，並聚焦於模型本身的行為與選擇策略，不深入探討資料前處理、特徵工程、模型調參、數學推導等高階內容，以避免模糊焦點。&lt;/p&gt;&#xA;&lt;h2 id=&#34;技術範圍與預期對象&#34;&gt;技術範圍與預期對象&lt;/h2&gt;&#xA;&lt;p&gt;本系列預設讀者已具備以下條件:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;具備基礎統計學與資料科學知識&lt;/li&gt;&#xA;&lt;li&gt;具備基本 Python 語法能力&lt;/li&gt;&#xA;&lt;li&gt;具備 scikit-learn, PyTorch, TensorFlow, Keras 基本建模流程&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;學習深度定位-聚焦在-level-23-之間&#34;&gt;學習深度定位: 聚焦在 Level 2–3 之間&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;等級&lt;/th&gt;&#xA;          &lt;th&gt;定義&lt;/th&gt;&#xA;          &lt;th&gt;在本系列的實踐目標&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Level 1&lt;/td&gt;&#xA;          &lt;td&gt;會用套件建模&lt;/td&gt;&#xA;          &lt;td&gt;✅ 使用 sklearn 等工具快速建模&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Level 2&lt;/td&gt;&#xA;          &lt;td&gt;理解模型的概念與原理&lt;/td&gt;&#xA;          &lt;td&gt;✅ 說得出每個模型的邏輯與核心機制&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Level 3&lt;/td&gt;&#xA;          &lt;td&gt;能比較模型優劣與應用場景選擇&lt;/td&gt;&#xA;          &lt;td&gt;✅ 理解適用時機、模型之間的 trade-off&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Level 4+&lt;/td&gt;&#xA;          &lt;td&gt;深入優化與理論推導&lt;/td&gt;&#xA;          &lt;td&gt;🚫 本系列不會深入涵蓋，建議另尋高階資源&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;系列預告與進展節奏&#34;&gt;系列預告與進展節奏&lt;/h2&gt;&#xA;&lt;p&gt;本系列將以「一日一模型」為目標，每篇聚焦於一個經典或常見模型，從實用視角出發說明其:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;核心邏輯與設計理念&lt;/li&gt;&#xA;&lt;li&gt;適用情境與限制條件&lt;/li&gt;&#xA;&lt;li&gt;與其他模型的比較與選擇策略&lt;/li&gt;&#xA;&lt;li&gt;Python 範例實作與評估觀察&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;預計涵蓋模型範圍包括: Linear Regression、Polynomial Regression、Logistic Regression、SVM、KNN、Decision Tree、Random Forest、XGBoost、PCA、KMeans、FCNN、CNN、RNN、Transformer &amp;hellip; 等。&lt;/p&gt;</description>
    </item>
    <item>
      <title>學歷無用? 我仍相信學習的價值</title>
      <link>http://twcch.io/posts/articles_25071601/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/articles_25071601/</guid>
      <description>&lt;p&gt;近年來，我觀察到無論在台灣還是中國，「學歷無用論」的聲音愈發強烈。許多人開始質疑讀書是否還有意義，認為不靠學歷反而更能致富，網紅、直播、投資客、白手起家的商人充斥版面，讀書人反倒被視為落後者、被剝削者、社會規訓的犧牲品，但我有不同的看法。&lt;/p&gt;&#xA;&lt;h2 id=&#34;為什麼學歷無用論會出現&#34;&gt;為什麼「學歷無用論」會出現？&lt;/h2&gt;&#xA;&lt;p&gt;我認為這不是單純的個人選擇，而是結構性問題:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;教育擴張讓學歷貶值，碩博士已成基本門檻&lt;/li&gt;&#xA;&lt;li&gt;階級複製讓弱勢者難以翻身，「努力不再保證回報」&lt;/li&gt;&#xA;&lt;li&gt;高回報的機會集中在少數風口行業，炒短線者當道&lt;/li&gt;&#xA;&lt;li&gt;社群媒體製造「一夜暴富」神話，反智氛圍蔓延&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;「讀書沒用」已經不只是判斷，更是一種情緒&lt;strong&gt;對制度失望、對未來無感、對努力失信&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;那讀書究竟還有沒有價值&#34;&gt;那讀書究竟還有沒有價值?&lt;/h2&gt;&#xA;&lt;p&gt;如果只探討「賺多少錢」作為唯一標準，那的確讀書沒有用，因為讀書不一定有最即時的回報。但若把時間尺度拉長、把價值層次拉高，會發現:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;學習不是為了立刻賺錢，而是為了讓你能夠分辨真偽、建立邏輯、理解世界、保有尊嚴地思考與行動&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;工具層面: 讓你擁有專業能力，立足於社會&lt;/li&gt;&#xA;&lt;li&gt;認知層面: 訓練你思辨、整合、表達的能力&lt;/li&gt;&#xA;&lt;li&gt;存在層面: 引導你認識自己、世界與人生的關係&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;簡單來說，我 30 年的人生觀告訴我，讀書可以讓我的境界提升，能夠自我昇華&lt;/p&gt;&#xA;&lt;h2 id=&#34;我們為什麼會覺得努力應該有回報&#34;&gt;我們為什麼會覺得「努力應該有回報」?&lt;/h2&gt;&#xA;&lt;p&gt;許多陷入犬儒的人會說：「我不是不努力，我努力過了，沒用。」&lt;/p&gt;&#xA;&lt;p&gt;這恰恰反映出對努力的誤解，努力是會失敗的，而且也從來不會立即兌現，它更像是一種長期累積的複利，過程中你會改變視角、強化心智，最終與眾不同&lt;/p&gt;&#xA;&lt;p&gt;真正的努力，不只是行為上的執行，更是認知上的轉變。不是用熱血硬幹，而是用策略、用反思、用節奏走出自己的路。&lt;/p&gt;&#xA;&lt;h2 id=&#34;那我們應該怎麼做&#34;&gt;那我們應該怎麼做？&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不要盲目追求文憑，但也不要過早放棄學習&lt;/li&gt;&#xA;&lt;li&gt;看懂社會結構的變動，但也要打造自己可控的核心能力&lt;/li&gt;&#xA;&lt;li&gt;知道短期內「投機」可能勝出，但&lt;strong&gt;長期是價值與認知力的勝利&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;一段送給未來自己的話&#34;&gt;一段送給未來自己的話&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;讀書不是為了成功，而是為了不被世界輕易騙走&#xA;當社會用最廉價的快樂來交換你一生的時間時&#xA;教育與自省，是你最後的防線&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;哪怕這個時代看起來對知識不再禮遇，我仍願意選擇學習，因為&lt;strong&gt;我相信厚積才能薄發，深耕才能穿透表象，真正理解世界，並在其中活出自己的姿態&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>視覺化專案 - 200 個國家 200 百年 4 分鐘</title>
      <link>http://twcch.io/posts/projects/articles_25070901/</link>
      <pubDate>Wed, 09 Jul 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/projects/articles_25070901/</guid>
      <description>&lt;p&gt;這是一個資料視覺化專案——「Dynamic Visualization: 200 Countries, 200 Years, 4 Minutes」。它將涵蓋 1816 至 2016 年，200 個國家的歷史變遷以互動動畫呈現，整體動畫長度約四分鐘，旨在結合「時間」與「地理」維度，提供用戶沉浸式的歷史視覺體驗。&lt;/p&gt;&#xA;&lt;p&gt;成品呈現頁面: &lt;a href=&#34;https://twcch.io/TwoHundredYearsTwoHundredCountries/views.html&#34;&gt;https://twcch.io/TwoHundredYearsTwoHundredCountries/views.html&lt;/a&gt;&lt;br&gt;&#xA;GitHub 原始碼: &lt;a href=&#34;https://github.com/twcch/TwoHundredYearsTwoHundredCountries&#34;&gt;https://github.com/twcch/TwoHundredYearsTwoHundredCountries&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;專案目標-動態傳遞跨時代趨勢&#34;&gt;專案目標: 動態傳遞跨時代趨勢&lt;/h2&gt;&#xA;&lt;p&gt;我這次的核心目的，是打造一段「高品質又美觀」的互動式動畫。相比靜態圖表，此動畫能讓使用者更直覺地感受到全球歷史變化的脈絡與節奏。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;跨國維度: 一次呈現 200 國家在相同指標上的變化&lt;/li&gt;&#xA;&lt;li&gt;跨年代視角: 覆蓋整整兩個世紀&lt;/li&gt;&#xA;&lt;li&gt;互動與美感: 最終以 Plotly Express 強化動畫的動態感與互動性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;這是一個典型的「Proof of Concept」，驗證我能用純 Python 開源工具在本地完成動態資料視覺化，而不是依賴商業軟體。&lt;/p&gt;&#xA;&lt;h2 id=&#34;處理流程解析&#34;&gt;處理流程解析&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;資料擷取與清理&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用 pandas 從 Gapminder 或其他開源來源讀入年份、國家與指標。&lt;/li&gt;&#xA;&lt;li&gt;透過 core/data.py 標準化欄位名稱、處理缺值、並轉換為長型結構，以利後續分析。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;寫入 SQLite&#xA;&lt;ul&gt;&#xA;&lt;li&gt;為了方便查詢與存取，我用 core/sqlite_db.py 將清理後的資料匯入 SQLite 資料庫，一併記錄 metadata。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;產生視覺化資料表&#xA;&lt;ul&gt;&#xA;&lt;li&gt;scripts/build_view_table.py 將資料按年與國家展開，組合成完整用於視覺化的 DataFrame。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;動態驗證：matplotlib 原型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在 proof_of_concept.py 中，以 matplotlib 建立由靜態圖逐幀拼湊的基本動畫，確認播放邏輯與視覺節奏。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;互動動畫：Plotly Express&#xA;&lt;ul&gt;&#xA;&lt;li&gt;最終在 plot_with_px.py 中改以 Plotly Express，產出包含滑動條、國家標籤、時間軸與音效的四分鐘互動畫面，並輸出至 docs/views.html。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;技術選擇與實務考量&#34;&gt;技術選擇與實務考量&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;資料處理: pandas 濾除缺值、重塑表格、處理 metadata，全套操作都在 pandas 中完成。&lt;/li&gt;&#xA;&lt;li&gt;儲存管理: 使用 SQLite 儲存資料，方便查詢與重複執行，而不用每次都從頭開 CSV。&lt;/li&gt;&#xA;&lt;li&gt;動畫原型: matplotlib 可迅速驗證概念、調整幀率與時間間隔。&lt;/li&gt;&#xA;&lt;li&gt;互動視覺化: Plotly Express 能更快速加入滑桿、hover 標籤，動畫更加流暢美觀，也更適合網頁展示。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;展示成果&#34;&gt;展示成果&lt;/h2&gt;&#xA;&lt;p&gt;最終輸出是一個 HTML 檔，內嵌動態 html5 視覺化:&lt;/p&gt;</description>
    </item>
    <item>
      <title>實戰專案 - Titanic 生存預測專案</title>
      <link>http://twcch.io/posts/projects/articles_25063001/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/projects/articles_25063001/</guid>
      <description>&lt;p&gt;這是一個資料科學專案，目標是透過 Kaggle 經典的 Titanic 生存預測題目，建立一套結構清晰、模組化的預測系統。我不只是想交出一份準確的預測結果，更希望藉由這個專案練習:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如何設計可擴充、可維護的資料分析架構&lt;/li&gt;&#xA;&lt;li&gt;如何把模型訓練與推論流程標準化&lt;/li&gt;&#xA;&lt;li&gt;如何用設定檔 (config-driven) 控制整個 pipeline&lt;/li&gt;&#xA;&lt;li&gt;如何實踐工程導向的資料科學流程&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;GitHub 原始碼: &lt;a href=&#34;https://github.com/twcch/TitanicSurvivalPrediction&#34;&gt;https://github.com/twcch/TitanicSurvivalPrediction&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;專案定位不只是解題而是設計一套解法系統&#34;&gt;專案定位：不只是「解題」，而是「設計一套解法系統」&lt;/h2&gt;&#xA;&lt;p&gt;我不滿足於單純把資料丟進模型調整參數。我希望打造的是一個「可重複使用的機器學習預測框架」，因此我做了以下幾點設計:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;架構模組化: 依照功能拆分為 data/, features/, models/, utils/，程式碼清楚分工&lt;/li&gt;&#xA;&lt;li&gt;流程自動化: 所有步驟都由 main.py 控制，方便一鍵執行與重現實驗&lt;/li&gt;&#xA;&lt;li&gt;設定檔驅動: 核心設定集中管理於 config.json，可以快速切換特徵、模型參數與輸出路徑&lt;/li&gt;&#xA;&lt;li&gt;可擴充性設計: 未來若要換模型、加特徵、改評估指標，幾乎不需改動主程式碼&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;這些設計不只是在技術上提升效率，也讓我在做資料科學時，更接近實務工作者的思維模式&lt;/p&gt;&#xA;&lt;h2 id=&#34;資料前處理與特徵工程-每個欄位都要能解釋&#34;&gt;資料前處理與特徵工程: 每個欄位都要能「解釋」&lt;/h2&gt;&#xA;&lt;p&gt;我對特徵的要求是: 不只要對模型有用，更要有邏輯、可解釋&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;處理缺失值&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Age 用中位數填補&lt;/li&gt;&#xA;&lt;li&gt;Embarked 用眾數填補&lt;/li&gt;&#xA;&lt;li&gt;Fare 缺值極少，仍完整處理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;創造新特徵&#xA;&lt;ul&gt;&#xA;&lt;li&gt;FamilySize = SibSp + Parch: 模擬家庭是否有互助效果&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;選定使用特徵&#xA;&lt;ul&gt;&#xA;&lt;li&gt;類別型: Pclass, Sex, Embarked, Title&lt;/li&gt;&#xA;&lt;li&gt;數值型: Age, Fare, FamilySize&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型設定-我選擇-xgboost但更重視可控性&#34;&gt;模型設定: 我選擇 XGBoost，但更重視可控性&lt;/h2&gt;&#xA;&lt;p&gt;雖然這個任務可以用很多模型解，但我選擇以 XGBoost 為主模型，理由如下:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Tree-based 模型不需要特徵標準化，工程處理更簡潔&lt;/li&gt;&#xA;&lt;li&gt;對類別特徵與數值特徵的混合表現良好&lt;/li&gt;&#xA;&lt;li&gt;在 Kaggle 類似任務中表現穩定，可作為 baseline&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;模型訓練與推論流程&#34;&gt;模型訓練與推論流程&lt;/h2&gt;&#xA;&lt;p&gt;整個流程包含以下幾步，由 main.py 控制:&lt;/p&gt;</description>
    </item>
    <item>
      <title>當老師只靠 ChatGPT 回信：我們期待的不是答案，而是理解</title>
      <link>http://twcch.io/posts/articles_25062501/</link>
      <pubDate>Wed, 25 Jun 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/articles_25062501/</guid>
      <description>&lt;p&gt;最近，我正在參加一門職訓課程。本來對這堂課滿懷期待，尤其是對某位老師的專業背景很感興趣。不過，隨著課程進行，我漸漸感到一股說不出的落差感：每當我主動提出深入問題，收到的回信卻幾乎都像是 ChatGPT 生成的答案——格式漂亮、邏輯完整、語氣中立，但就是少了「人味」與「針對性」。&lt;/p&gt;&#xA;&lt;p&gt;是的，我知道他不是完全照抄。他有修改、有加註、有整合，但整體感受依然強烈：「這不是一個人對我問題的理解回應，而是一個工具對所有人都能複製的輸出。」&lt;/p&gt;&#xA;&lt;p&gt;這讓我很困惑，甚至有些失望。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我不是反對使用-ai事實上我自己也在用&#34;&gt;我不是反對使用 AI，事實上我自己也在用&lt;/h2&gt;&#xA;&lt;p&gt;先聲明，我並不是那種抗拒 AI 的人。相反地，我本身就是資料分析背景，也有使用 ChatGPT 作為輔助工具的習慣。無論是整理技術架構、釐清概念、或產出初步內容，我完全理解 LLM 在學習與知識組織方面的強大價值。&lt;/p&gt;&#xA;&lt;p&gt;但關鍵在於：「角色不同、責任也不同。」&lt;/p&gt;&#xA;&lt;p&gt;身為學習者，我使用 AI 是為了提升效率與學習深度。但作為老師、講師、顧問，使用 AI 不應該只是「產生回答」這麼簡單。&lt;/p&gt;&#xA;&lt;h2 id=&#34;教學不是交付答案而是理解問題的脈絡&#34;&gt;教學不是交付答案，而是理解問題的脈絡&lt;/h2&gt;&#xA;&lt;p&gt;作為學生，我真正期待的，不是單純的一段知識回答，而是來自老師對我所處困境的共鳴與理解。我希望老師能理解我提問背後的「背景」、「盲點」與「問題設計的目的」，並根據這些脈絡回應，而不是直接貼上一段 ChatGPT 輸出的技術解釋。&lt;/p&gt;&#xA;&lt;p&gt;因為我相信，一個真正理解我問題的老師，會根據我當下的能力、背景、甚至目標給出回應——這種回應，不是任何一個 AI 可以「直接」產出的。&lt;/p&gt;&#xA;&lt;p&gt;而當老師只是當 ChatGPT 是一個快捷鍵，那麼學生也很快會意識到：你不是在回答我，你只是在轉寄一份資訊而已。&lt;/p&gt;&#xA;&lt;h2 id=&#34;當教學淪為貼文產出學生會停止問問題&#34;&gt;當教學淪為「貼文產出」，學生會停止問問題&lt;/h2&gt;&#xA;&lt;p&gt;更嚴重的影響是：這樣的互動會直接打擊學生的提問動力。&lt;/p&gt;&#xA;&lt;p&gt;當我發現提問後收到的回應只是套用模板、換個措辭、格式一致卻無深入探討時，我會懷疑：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;我這麼認真思考的問題，真的值得你花時間思考嗎？&lt;/li&gt;&#xA;&lt;li&gt;還是我只是你輸入框中的另一個 prompt？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;久而久之，學生開始不再問問題，也不再相信提問能帶來真正的理解與對話。這對整個學習場域，是一種靜默但致命的傷害。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ai-是輔助不是教學本體&#34;&gt;AI 是輔助，不是教學本體&lt;/h2&gt;&#xA;&lt;p&gt;AI 可以作為老師教學的輔助工具：幫助蒐集資料、釐清知識、快速構思。但它不應該代替老師對學習者的思考與理解責任。在這個知識容易複製的年代，真正無法取代的價值，其實是「對個別學習者的回應能力」。&lt;/p&gt;&#xA;&lt;p&gt;我們當然不會要求每個老師都要一封封親筆手寫、寫出三千字的回信。但至少請不要用 ChatGPT 當成唯一的內容產出來源，更不要用它來「掩蓋」缺乏投入的回應。學習者看得出來，也感受得到。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我寫這篇文章不是為了批評老師而是為了保護教學&#34;&gt;我寫這篇文章，不是為了批評老師，而是為了保護教學&lt;/h2&gt;&#xA;&lt;p&gt;我知道那位老師並不是惡意。他可能工作繁忙、學生太多、壓力很大。我甚至相信他是出於「想要給一個完整答案」的好意才選擇這樣回覆。但我們必須正視一件事：當我們過度依賴工具，而忘記了教學的本質是人與人之間的理解與連結，那麼再強大的 AI 也只會讓教育變得更冷漠、更廉價。&lt;/p&gt;&#xA;&lt;p&gt;我寫這篇文章，是希望提醒每一位教學者：你的價值，不在於你給的答案有多完整，而在於你有多願意理解學生的問題。因為 AI 可以幫你教知識，但唯有你能教會「怎麼成長」。&lt;/p&gt;</description>
    </item>
    <item>
      <title>為什麼用 AI 技術檢測企業舞弊，比想像中更困難？</title>
      <link>http://twcch.io/posts/articles_25060201/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/articles_25060201/</guid>
      <description>&lt;p&gt;在資料科學領域中，對企業進行舞弊檢測 (Fraud Detection) 被視為是一種分類問題: 輸入企業相關的數據，輸出舞弊或非舞弊。然而，真正投入研究後會發現，這個問題很難解決，非常具挑戰性。&lt;/p&gt;&#xA;&lt;p&gt;我目前主要研究方向，是運用人工智慧 (Artificial Intelligence) 技術，來解決企業進行財務報表舞弊的問題。這類型的議題與銀行信用卡詐欺、保險業中的理賠舞弊、甚至洗錢行為有相似之處，都是稀有事件、後知後覺、動態進化的「敵對性問題」。&lt;/p&gt;&#xA;&lt;h2 id=&#34;為什麼這不是一個單純的分類問題&#34;&gt;為什麼這不是一個單純的分類問題？&lt;/h2&gt;&#xA;&lt;p&gt;在傳統機器學習框架下，分類問題的成功往往來自於充足的標記數據、清晰的邊界條件與相對穩定的資料分佈。然而，舞弊行為恰恰違反了這三項假設。&lt;/p&gt;&#xA;&lt;p&gt;可以從以下幾點具體說明：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;極度不平衡的資料 (Class Imbalance)&lt;/p&gt;&#xA;&lt;p&gt;在實務資料中，舞弊案件往往只佔所有資料的極小比例，可能是千分之一、甚至萬分之一。這意味著如果你採用傳統的精確度 (accuracy) 作為衡量指標，模型即使完全忽略舞弊也能達到 99% 以上的準確率，但這顯然毫無意義。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;標籤不完整且滯後揭露 (Label Latency &amp;amp; Missing Labels)&lt;/p&gt;&#xA;&lt;p&gt;很多舞弊行為要經過數月、甚至數年後才會被調查揭露，更遑論那些永遠未被發現的案件。這使得訓練資料的標籤具有高度不確定性，導致模型容易學到錯誤的決策邊界。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;舞弊技術持續演化 (Concept Drift)&lt;/p&gt;&#xA;&lt;p&gt;犯罪者會根據監管與模型檢測方式持續更新手法，導致模型在部署後迅速失效。這使得即使當下訓練準確的模型，也難以長期維持效能。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;異常並非來自單一特徵，而是整體脈絡的矛盾 (Contextual Inconsistency)&lt;/p&gt;&#xA;&lt;p&gt;財報舞弊往往不是單一財務指標異常，而是多個指標之間出現結構性不一致。例如: 營收大增但現金流卻大減、獲利提升但存貨異常膨脹。這種多變量脈絡異常，遠比簡單的 outlier detection 更為複雜。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;問題不是模型選得不夠好而是問題設定錯了&#34;&gt;問題不是模型選得不夠好，而是問題設定錯了&lt;/h2&gt;&#xA;&lt;p&gt;如果僅停留在「用哪個模型比較準」、「要不要用 XGBoost 還是 LSTM」這種層級的思考，只會陷入技術細節的死胡同，無法解決核心困難。相反地，我認為更關鍵的兩個研究方向是：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;如何讓 AI 自己找到潛在的舞弊標籤？&lt;/p&gt;&#xA;&lt;p&gt;採用自監督學習 (Self-Supervised Learning)，不依賴人工標註，而是讓模型自行從大量正常樣本中學習「常態結構」，再對偏離常態的資料進行異常評分，進一步推論出可能的舞弊行為。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;如何讓深度模型的決策可以被人類審計人員理解？&lt;/p&gt;&#xA;&lt;p&gt;深度學習模型雖然強大，但往往是黑箱。導入可解釋性方法 (如 SHAP、LIME、Attention 可視化)，可以提升金融監理與內部稽核部門的信任與採用意願，也為模型導入實務場域鋪路。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;這不只是建模問題更是科學問題&#34;&gt;這不只是建模問題，更是科學問題&lt;/h2&gt;&#xA;&lt;p&gt;用 AI 解決舞弊，不是一場簡單的技術堆疊競賽，而是對整個金融風險邏輯、舞弊行為模式、以及資料特性深刻理解的綜合挑戰。這將是我博士研究的起點，從理解問題本質出發，探索如何用 AI 技術建立可行的風險偵測系統，不只是要「分類得準」，更要讓人「信得過」。我認為這是一條難走的路，但也因此充滿價值。&lt;/p&gt;</description>
    </item>
    <item>
      <title>為什麼 AI 正在快速削弱低階 Business Analyst 的價值？</title>
      <link>http://twcch.io/posts/articles_25040701/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0800</pubDate>
      <guid>http://twcch.io/posts/articles_25040701/</guid>
      <description>&lt;p&gt;五年前，我踏入壽險產業，成為一名 Business Analyst (BA)。當時的工作內容相當清晰：需求文件撰寫、報表製作、簡單的數據分析與溝通協調，是我每天的日常。那時候，這些任務仍需靠人力一項一項完成，效率與品質全憑個人經驗與熟練度。但如今，這些「核心能力」正快速被人工智慧工具重塑，甚至取代。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我親身感受到的衝擊工具進步得比我想像中快&#34;&gt;我親身感受到的衝擊：工具進步得比我想像中快&lt;/h2&gt;&#xA;&lt;p&gt;當我首次使用大型語言模型 (LLM) 工具進行報告撰寫與 Python 代碼產出時，內心的震撼難以言喻。曾經需要數小時才能完成的分析報告，在幾分鐘內生成雛形；曾經為了釐清邏輯關係而反覆修改的流程圖，如今只需一句指令就能完成。&lt;/p&gt;&#xA;&lt;p&gt;我逐漸意識到，這並不是單一任務被加速，而是整個 BA 工作流程正被結構性重塑。換言之，AI 正在壓縮 BA 的邊際價值。這不只是主觀體感，更有明確的研究支持。根據美國 OpenAI 與賓州大學的聯合研究，約有 80% 的職業至少有 10% 的工作內容將受到 LLM 工具的影響；其中，高達 19% 的職業，其超過一半的工作可由 AI 完成。而 BA——尤其是負責初階文件處理、標準報表、流程規劃等工作的分析師，被列為高曝險族群。&lt;/p&gt;&#xA;&lt;p&gt;原因很明確因為 BA 所擅長的文字組織、資料彙整與需求敘述，正是 AI 最擅長模仿與執行的任務。&lt;/p&gt;&#xA;&lt;h2 id=&#34;市場變化正在發生不是未來式而是現在進行式&#34;&gt;市場變化正在發生，不是未來式，而是現在進行式&lt;/h2&gt;&#xA;&lt;p&gt;這樣的趨勢已經開始反映在勞動市場上：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;BA 的職缺增速放緩&lt;/p&gt;&#xA;&lt;p&gt;企業在內部導入 LLM 工具後，發現許多重複性任務可由 AI 初步完成，人力需求自然下降。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;初階 BA 的薪資競爭力下降&lt;/p&gt;&#xA;&lt;p&gt;對於只熟悉基本分析任務、無法主動創造洞察的人才，企業的願意支付薪資上升空間有限。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;高階 BA 的需求反而上升&lt;/p&gt;&#xA;&lt;p&gt;企業更看重能駕馭 AI 工具、快速整合資訊、提出具策略意義建議的分析人才。所謂「會用 AI 的人」，正逐步取代「被 AI 取代的人」。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;這代表，AI 並不是取代所有 BA，而是取代了不進化的 BA。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我的轉型從反應式執行者到主動創造者&#34;&gt;我的轉型：從反應式執行者，到主動創造者&lt;/h2&gt;&#xA;&lt;p&gt;面對這樣的劇變，我無法視若無睹。&lt;/p&gt;&#xA;&lt;p&gt;於是我開始盤點自己的能力，明白單靠產業 Know-how 或不夠專精的技能，將難以應對未來的競爭。因此，我選擇投入更深層的技能學習: 博士班訓練，深度的掌握資料科學、機器學習與深度學習領域，建立自身的核心競爭力。不只是會用工具或套模，而是要能理解這些如何運作、能應用在哪些情境、又有何種侷限。這樣的技能，不僅能讓我在日常分析中脫穎而出，也為我開啟進入 AI 應用領域的可能性。&lt;/p&gt;&#xA;&lt;h2 id=&#34;結語ai-不會毀滅職涯但它會重寫價值分佈&#34;&gt;結語：AI 不會毀滅職涯，但它會重寫價值分佈&lt;/h2&gt;&#xA;&lt;p&gt;AI 並不會取代 BA，但它會重新定義 BA 的角色。&lt;/p&gt;</description>
    </item>
    <item>
      <title>專欄文章</title>
      <link>http://twcch.io/column_article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://twcch.io/column_article/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;../tags/30-%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/&#34;&gt;iThome 鐵人賽 2025 - 30 天入門常見的機器學習演算法&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>研究經歷</title>
      <link>http://twcch.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://twcch.io/research/</guid>
      <description>&lt;h4 id=&#34;academic-thesis&#34;&gt;Academic Thesis&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Hsieh, C. C.&lt;/strong&gt; and Hsu, F. J. (2018). A firm’s financial risk and tax avoidance behavior. Master’s thesis, Department of Insurance and&#xA;Finance, National Taichung University of Science and Technology, Taichung, Taiwan.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;research-presentations&#34;&gt;Research Presentations&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hsu, F. J., Chen, S. H., and &lt;strong&gt;Hsieh, C. C.&lt;/strong&gt; (2020). Sweet candy or bitter poison: The default risk under US Federal&#xA;Reserve’s quantitative easing. The International Conference on Innovative Computing and Management Science (ICMS&#xA;2020), July 29, 2020, Yilan, Taiwan.&lt;/li&gt;&#xA;&lt;li&gt;Hsu, F. J., Chen, S. H., and &lt;strong&gt;Hsieh, C. C.&lt;/strong&gt; (2020). Corporate financial risk, financial market conditions, and firm’s&#xA;tax avoidance behavior. The International Conference on Innovative Computing and Management Science (ICMS 2020), July&#xA;29, 2020, Yilan, Taiwan.&lt;/li&gt;&#xA;&lt;li&gt;Hsu, F. J., and &lt;strong&gt;Hsieh, C. C.&lt;/strong&gt; (2018). Financial risk and tax avoidance. The 21st Conference of Finance Theory and&#xA;Practice, May 30, 2018, Taichung, Taiwan.&lt;/li&gt;&#xA;&lt;li&gt;Hsu, F. J., and &lt;strong&gt;Hsieh, C. C.&lt;/strong&gt; (2018). Macroeconomic conditions and firm’s tax avoidance behavior. The Annual Conference&#xA;of Modern Accounting Literature Association, May 4, 2018, Taichung, Taiwan.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;research-projects&#34;&gt;Research Projects&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;以巨量探勘與科研能量為基礎之我國金融科技架構發展(1/2)，科技部計畫編號 MOST 106-2634-F-025-001-，計畫參與人員，執行期間 2017/07/01–2018/06/30。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>聯絡我</title>
      <link>http://twcch.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://twcch.io/contact/</guid>
      <description>&lt;p&gt;如果你有任何問題，或是想要交流，可以透過 email 與我聯繫。&lt;/p&gt;&#xA;&lt;p&gt;Email: twcch1218 [at] gmail.com&lt;/p&gt;</description>
    </item>
    <item>
      <title>關於我</title>
      <link>http://twcch.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://twcch.io/about/</guid>
      <description>&lt;p&gt;我是志謙，目前就讀國立成功大學的博士生，以前是在金融業的商業分析師。目前主力是在研究機器學習與深度學習，希望可以跟各位一起學習成長。&lt;/p&gt;&#xA;&lt;h4 id=&#34;主要經歷&#34;&gt;主要經歷&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;[2022.04 – 2025.06] 富邦人壽保險股份有限公司 專案襄理 (商業分析師)&lt;/li&gt;&#xA;&lt;li&gt;[2020.04 – 2022.03] 南山人壽保險股份有限公司 專員 (商業分析師)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;技術與框架&#34;&gt;技術與框架&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Python (PyTorch, Flask, Flask-RESTful)&lt;/li&gt;&#xA;&lt;li&gt;Java (Spring Framework)&lt;/li&gt;&#xA;&lt;li&gt;SQL (PostgreSQL, MySQL)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;專業認證&#34;&gt;專業認證&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;美國壽險管理師, 2025&lt;/li&gt;&#xA;&lt;li&gt;國際專案管理師, 2024&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
