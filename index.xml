<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>志謙&#39;s Blog</title>
    <link>https://twcch.io/</link>
    <description>Recent content on 志謙&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>zh-heat</language>
    <lastBuildDate>Fri, 24 Oct 2025 00:00:00 +0800</lastBuildDate>
    <atom:link href="https://twcch.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning - Apriori</title>
      <link>https://twcch.io/posts/machine_learning/apriori/</link>
      <pubDate>Fri, 24 Oct 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/machine_learning/apriori/</guid>
      <description>&lt;p&gt;關聯規則探勘的問題，看似朴素，但實際上卻相當複雜。它不僅涉及到大量的數據處理，還需要考慮到各種不同的度量標準，以便從中提取出有價值的資訊。因為在成千上萬筆交易中，哪些項目「總是一起」出現? 真正困難在於組合爆炸，若有 10000 件商品，潛在組合多到無法枚舉。而 Apriori 的偉大之處是把「可搜尋性」建立在反單調性 (anti-monotonicity) 之上，讓我們能以數學性質剃除海量不必要的候選，保留真有機會成為規則的組合。&lt;/p&gt;&#xA;&lt;div class=&#34;callout note &#34;&gt;&#xA;  單調性 (monotonicity): 指的是某個屬性隨集合擴大而增加，而「反單調性」則是隨集合擴大而減少的特性。在關聯規則中，當你把項目集變大 (例如 {牛奶} → {牛奶, 麵包})，它的出現次數 (Support) 只會相同或更小，不可能變多，這個「越加項，支持度越低」的特性，就是 Support 的反單調性。&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;問題定義從交易到規則&#34;&gt;問題定義：從交易到規則&lt;/h2&gt;&#xA;&lt;p&gt;資料型態: 一組交易 $T={t_1,\dots,t_n}$，每筆交易 $t$ 是項目集合 $I={i_1,\dots,i_m}$ 的子集。&lt;/p&gt;&#xA;&lt;p&gt;我們尋找的對象是關聯規則: $X \rightarrow Y$，其中 $X, Y \subseteq I$、$X \cap Y = \varnothing$。&lt;/p&gt;&#xA;&lt;p&gt;三個基本度量：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support: $\text{supp}(X) = \frac{|{t \in T: X \subseteq t}|}{|T|}$，即 $X$ 在所有交易中的出現比例。&lt;/li&gt;&#xA;&lt;li&gt;Confidence: $\text{conf}(X \rightarrow Y) = \frac{\text{supp}(X \cup Y)}{\text{supp}(X)}$，觀察買了 $X$ 之後同時買 $Y$ 的機率。&lt;/li&gt;&#xA;&lt;li&gt;Lift: $\text{lift}(X \rightarrow Y) = \frac{\text{supp}(X \cup Y)}{\text{supp}(X)\text{supp}(Y)}$，衡量相對於獨立假設的加成（&amp;gt;1 表正向關聯）。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;延伸常用度量 (利於排序與風控):&lt;/p&gt;</description>
    </item>
    <item>
      <title>學習這件事，沒有人能替你完成</title>
      <link>https://twcch.io/posts/2025/learning-is-your-own-responsibility/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/learning-is-your-own-responsibility/</guid>
      <description>&lt;p&gt;我最近擔任一門人工智慧課程的助教。這門課的對象是碩士在職專班的學生，理應具備一定的專業基礎，但讓人訝異的是，課程進行到第七週，竟仍有人不會安裝 Python 環境。&lt;/p&gt;&#xA;&lt;p&gt;這不是技術問題，而是態度問題，因為在這個時代，解決問題的資源早已無所不在，缺的不是教材，而是「想學」的意願。&lt;/p&gt;&#xA;&lt;h2 id=&#34;被動學習-現代教育的幻覺&#34;&gt;被動學習: 現代教育的幻覺&lt;/h2&gt;&#xA;&lt;p&gt;我們常把「學習」誤解為「被教導」，從小到大，學生被安排課表、被分配教材、被要求完成考試，在這樣的制度裡，學習變成一種被動活動：你只要出席、抄筆記、交作業，就被視為「有在學」，久而久之，學生養成一種錯覺——只要有人教，我就會學；沒人教，我就無能為力。&lt;/p&gt;&#xA;&lt;p&gt;但真實世界不會照課綱走，沒有人會站在你身邊、一步步教你安裝環境、講解指令、或提醒你該查什麼關鍵字，在專業領域裡，這種等待別人餵食的態度，等同於自我放棄，學習的本質不是「接受」，而是「探索」，當你把「不會」當作理由，而非起點，你就已經錯過了學習的第一步。&lt;/p&gt;&#xA;&lt;h2 id=&#34;知識不再稀缺主動才稀有&#34;&gt;知識不再稀缺，主動才稀有&lt;/h2&gt;&#xA;&lt;p&gt;在資訊稀缺的年代，老師是知識的守門人，但今天，我們活在一個前所未有的開放時代: 任何語言、工具、演算法、乃至完整的學程，都能在網路上免費取得。&lt;/p&gt;&#xA;&lt;p&gt;你可以用 YouTube 看世界頂尖教授講解機器學習，可以在 GitHub 上下載實作範例，可以問 ChatGPT、Stack Overflow、Reddit，甚至只需一個搜尋，就能解決 80% 的初學問題。&lt;/p&gt;&#xA;&lt;p&gt;然而，矛盾的是，學習者的依賴性反而更強，不是因為學不會，而是因為學得太容易以後，主動性反而被削弱。當知識成為隨手可得的商品，人們開始懶得去追，他們不再為理解奮鬥，只想快速被餵飽，結果，不是資訊太少，而是思考太懶。&lt;/p&gt;&#xA;&lt;h2 id=&#34;學習不是懂了沒而是想不想&#34;&gt;學習不是「懂了沒」，而是「想不想」&lt;/h2&gt;&#xA;&lt;p&gt;學習從來不是一個知識問題，而是一個態度問題，會不會裝環境、會不會寫程式、會不會查錯誤訊息，這些都不是障礙，真正的障礙是「我不想學」、「我等人教」，這是一種心智的懶散，一種責任的逃避。&lt;/p&gt;&#xA;&lt;p&gt;在人工智慧課堂上，我看到的並不是學生的能力落差，而是對學習的誠意落差。有些人願意花夜晚兩小時嘗試、犯錯、再嘗試；另一些人連回家搜尋都懶得做，上課才拼命問，這不是顯得你很好學，而是暴露你不願負責的態度，或者只丟一句:「我不會」，這樣的對話不是求助，而是退場宣言。&lt;/p&gt;&#xA;&lt;p&gt;在研究與技術領域裡，「不會」是起點，不是句點，真正的學習者會說：「我不會，但我想試。」，而不是:「我不會，所以我等你教我。」&lt;/p&gt;&#xA;&lt;h2 id=&#34;學習是一種行動而不是身分&#34;&gt;學習是一種行動，而不是身分&lt;/h2&gt;&#xA;&lt;p&gt;許多學生把「我在學習」當成身分標籤，而不是實際行為，他們參加課程、報名訓練、加入社群，但實際上學得很少。因為學習的定義被誤解為「參與」——只要我在場，我就在學，但真正的學習，不發生在課堂，而發生在課堂之後。&lt;/p&gt;&#xA;&lt;p&gt;當你為了解決問題而主動查資料、比對不同說法、實際操作、失敗再修正，那才叫學習，學習不是「吸收知識」，而是「改變自己」，沒有行動的學習，只是自我安慰，你以為自己在前進，其實只是待在原地聽別人走過的路。&lt;/p&gt;&#xA;&lt;h2 id=&#34;教育的失敗不在老師而在態度&#34;&gt;教育的失敗，不在老師，而在態度&lt;/h2&gt;&#xA;&lt;p&gt;許多人批評老師教得不好、教材太難、進度太快，但在我看來，這些都只是表象，真正的問題是: 學生早已把學習的責任外包給他人，老師永遠無法替你思考，即使教學再細、再用心，也無法讓不願理解的人學會，學習的主體性若不存在，所有知識都只是過眼雲煙。&lt;/p&gt;&#xA;&lt;p&gt;在我協助的這門人工智慧課程裡，課綱內容明顯的呈現出為「進階應用」，學生卻連最基本的開發環境都不會，這不是課程錯配，而是態度錯位，這群學生不是學不會，而是不願意學會，就是又笨又懶，因為「學」意味著承擔，「不會」意味著責任，只要他們能把責任丟給老師，就能繼續維持那份舒適的無能。&lt;/p&gt;&#xA;&lt;h2 id=&#34;學習是反脆弱的鍛鍊&#34;&gt;學習是反脆弱的鍛鍊&lt;/h2&gt;&#xA;&lt;p&gt;學習從來不是順利的，它是一種不斷碰壁、不斷修正的過程，而正是在那些不舒服、挫敗、甚至想放棄的時刻，學習才真正發生，這種「反脆弱性」正是學習的核心價值。&lt;/p&gt;&#xA;&lt;p&gt;主動學習的人，會在錯誤中成長；被動學習的人，會在錯誤前退縮，AI、工具、老師都只是輔助，沒有任何東西能代替你的意志，如果你害怕犯錯，就永遠無法獨立思考，如果你拒絕主動，就永遠只能等待被教。&lt;/p&gt;&#xA;&lt;h2 id=&#34;主動學習是專業的底線&#34;&gt;主動學習，是專業的底線&lt;/h2&gt;&#xA;&lt;p&gt;在專業領域，學習不是個人興趣，而是責任，你可以不聰明，但不能不主動，因為世界的變化太快，沒有人有義務等你跟上，學習是對職業的尊重，也是對自己的尊嚴。&lt;/p&gt;&#xA;&lt;p&gt;身為助教，我並不期待學生都懂 AI 模型的原理，但我至少期待他們願意嘗試、願意查資料、願意自己動手，那是學習的最低標準，若連這都做不到，拿再多學分、上再多課，也只是徒具形式。&lt;/p&gt;&#xA;&lt;h2 id=&#34;結語-學習的誠意&#34;&gt;結語: 學習的誠意&lt;/h2&gt;&#xA;&lt;p&gt;學習是一種誠意——對知識的誠意、對努力的誠意、對自己的誠意，沒有人能替你完成學習，就像沒有人能替你變成更好的自己，在這個人人都能獲得知識的時代，真正的差距，不是誰知道得多，而是誰願意多走一步。&lt;/p&gt;&#xA;&lt;p&gt;所以，當學生問我：「老師，這個我不會怎麼辦？」，我會回答：「那你打算怎麼學？」，因為，學習從來不是被教出來的，是你自己走出來的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>數據分析師的洞察之眼關聯分析</title>
      <link>https://twcch.io/posts/2025/when-data-starts-to-connect/</link>
      <pubDate>Sat, 06 Sep 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/when-data-starts-to-connect/</guid>
      <description>&lt;p&gt;關聯規則探勘，是一門讓資料自己開口說話的技術。它不預測未來，也不分類人群，而是靜靜地從無數筆交易中，尋找那些「總是一起出現」的痕跡。對數據分析師而言，這是一種不同於統計假設的思考方式——它不帶偏見，不設方向，只追隨共現的規律。&lt;/p&gt;&#xA;&lt;h2 id=&#34;啤酒與尿布的啟示&#34;&gt;啤酒與尿布的啟示&lt;/h2&gt;&#xA;&lt;p&gt;許多人第一次聽到「關聯規則探勘」 (Association Rule Mining) 這個名詞時，總會想起那個幾乎被引用成神話的故事。美國某家超市在分析顧客購物資料時，發現一個奇特的現象: 購買尿布的男性顧客，常常會順手買一瓶啤酒。&lt;/p&gt;&#xA;&lt;p&gt;當時的分析師並沒有先入為主地設定假設，他們只是單純讓演算法在龐大的交易資料中尋找共現的規律。結果揭露了一種不易察覺的生活節奏——年輕父親在深夜購物時，會同時買尿布與啤酒。這個行為的背後，不是消費理性，而是情緒的慰藉。&lt;/p&gt;&#xA;&lt;p&gt;這便是關聯規則探勘的精神所在: 它讓資料說話，而不是讓人去強迫資料符合預設的故事。在數據分析的世界裡，這種從共現關係中發現行為模式的方法，被廣泛應用於零售、金融、醫療、甚至網路內容推薦系統中。&lt;/p&gt;&#xA;&lt;h2 id=&#34;從購物籃看行為&#34;&gt;從購物籃看行為&lt;/h2&gt;&#xA;&lt;p&gt;想像一間超市的銷售資料表，每一筆交易就像一個購物籃 (basket)，裡面放著當次顧客購買的所有商品。我們想知道的問題其實很簡單:「哪些商品會經常一起被買走?」在資料的語言裡，這樣的問題就是「項目之間的共現關係」。&lt;/p&gt;&#xA;&lt;div class=&#34;callout note &#34;&gt;&#xA;  &lt;p&gt;若 20% 的交易同時出現牛奶與麵包，那麼這個組合的支持度 (support) 就是 0.2。如果在所有買牛奶的顧客中，有 80% 也買了麵包，則這條規則的信賴度 (confidence) 是 0.8。&lt;/p&gt;&#xA;&lt;p&gt;但僅憑這兩個數值，我們還無法判斷這是否只是巧合。因此分析師還會觀察第三個指標「提升度 (lift)」，即實際共現機率與隨機共現機率的比值。&lt;/p&gt;&#xA;&lt;p&gt;當 lift 大於 1，代表兩個項目之間的關聯比隨機還強；若小於 1，則可能互斥。這三個指標——support、confidence、lift——構成了關聯規則探勘的基礎語彙。&lt;/p&gt;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;apriori-從最常見的開始&#34;&gt;Apriori: 從最常見的開始&lt;/h2&gt;&#xA;&lt;p&gt;在技術層面上，最經典的演算法是 Apriori。它的核心思想很直觀:「若一個項目集是頻繁的 (Frequent Itemset)，那麼它的子集一定也是頻繁的。」&lt;/p&gt;&#xA;&lt;p&gt;這個原則讓我們得以從最基本的一項商品開始，逐步生成兩項、三項組合，並在每一層過濾不常見的組合，直到再也無法擴展。&lt;/p&gt;&#xA;&lt;p&gt;舉例來說，我們可以用 Python 的 mlxtend 套件進行實作:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mlxtend.frequent_patterns&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;apriori&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;association_rules&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 模擬交易資料 (每列為一筆交易，每欄為商品)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;milk&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;bread&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;beer&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;butter&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;diaper&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Step 1: 找出頻繁項集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;frequent_itemsets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;apriori&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_support&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use_colnames&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Step 2: 產生關聯規則&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rules&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;association_rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frequent_itemsets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metric&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;lift&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_threshold&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rules&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;lift&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;antecedents&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;consequents&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;support&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;confidence&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;lift&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;輸出結果可能如下:&lt;/p&gt;</description>
    </item>
    <item>
      <title>讀書無用論，真的無用嗎?</title>
      <link>https://twcch.io/posts/2025/the-fallacy-of-anti-education/</link>
      <pubDate>Tue, 02 Sep 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/the-fallacy-of-anti-education/</guid>
      <description>&lt;p&gt;前陣子在一個群組裡，我看到一個案例。一位年僅 19 歲的年輕人，沒有繼續升學，而是直接進入一家小型乙方軟體公司工作。他對此洋洋自得，甚至把「不讀書」當成一種優越感，強調「讀書無用」，彷彿他提早進入職場就是成功的證明。&lt;/p&gt;&#xA;&lt;p&gt;我有花時間看一下他分享的 project，很明顯並不是所謂的「天才作品」，甚至只能算是初階學習者的水平。但他卻以此為榮，並進一步推廣讀書無用論。這讓我不禁反思: 他真正的問題，不在於選擇不讀書，而在於被自己的有限眼界所困。&lt;/p&gt;&#xA;&lt;h2 id=&#34;讀書不是唯一但沒有讀書會少掉很多東西&#34;&gt;讀書不是唯一，但沒有讀書會少掉很多東西&lt;/h2&gt;&#xA;&lt;p&gt;我們必須承認，學校教育不是通往成功的唯一途徑。許多傑出的人才也選擇了不同的路徑。然而，不代表讀書就沒有價值。讀書帶來的，其實是結構化的思維、批判能力、知識廣度與深度。這些東西，未必能在一間小公司裡快速累積。反而，若只停留在低門檻的任務，眼界與能力會被環境綁死。&lt;/p&gt;&#xA;&lt;h2 id=&#34;眼界的限制會製造出錯誤的自信&#34;&gt;眼界的限制，會製造出錯誤的自信&lt;/h2&gt;&#xA;&lt;p&gt;我認為這位年輕人會覺得「讀書無用」，是因為他的視野只停留在眼前的工作場域。他所在的公司或許把簡單的專案包裝成「實戰」，讓他誤以為自己已經站上了高地。但一旦跳出這個小環境，他就會發現自己其實什麼都不是。&lt;/p&gt;&#xA;&lt;p&gt;這就是所謂的「井底之蛙效應」: 當你沒看過更廣闊的世界，就會以為眼前就是全部。&lt;/p&gt;&#xA;&lt;h2 id=&#34;真正該自豪的是學習的韌性與格局&#34;&gt;真正該自豪的，是學習的韌性與格局&lt;/h2&gt;&#xA;&lt;p&gt;19 歲選擇進入職場沒有錯，但把它當成「勝利宣言」卻是幼稚的。&lt;/p&gt;&#xA;&lt;p&gt;真正值得驕傲的，不是早一點領薪水，而是能持續提升自己的能力，無論透過學校、工作或自學。尤其在科技領域，學習的速度與深度往往決定了未來的格局。&lt;/p&gt;&#xA;&lt;p&gt;一個人若因短期的工作成就而看輕讀書，長遠來看，他可能會成為被取代的一群人，而不是推動產業的人。&lt;/p&gt;&#xA;&lt;h2 id=&#34;結語&#34;&gt;結語&lt;/h2&gt;&#xA;&lt;p&gt;「讀書無用論」往往不是知識真的無用，而是說出這句話的人，因為眼界不足，誤以為無用。讀書不保證成功，但它能拓展思維邊界，避免我們陷入自以為是的舒適圈。因此，問題不在於他選擇 19 歲進入小公司，而在於他拒絕承認自己還有更大的學習空間。這種自滿，才是真正的危險。&lt;/p&gt;</description>
    </item>
    <item>
      <title>在數據中尋找秩序，資料探勘的藝術</title>
      <link>https://twcch.io/posts/2025/order-in-the-chaos-of-data/</link>
      <pubDate>Sat, 02 Aug 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/order-in-the-chaos-of-data/</guid>
      <description>&lt;p&gt;資料探勘 (Data Mining) 是一場從混沌到洞察的旅程，它的核心不只是技術，而是一種思考方式——如何讓資料自己說話，如何從龐大的資訊中找出值得理解的模式，對數據分析師而言，資料探勘不只是「處理資料」，而是「與資料對話」。&lt;/p&gt;&#xA;&lt;h2 id=&#34;資料探勘的先備知識&#34;&gt;資料探勘的先備知識&lt;/h2&gt;&#xA;&lt;p&gt;在開始資料探勘之前，我們需要理解它的理論基礎，資料探勘並非單一技術，而是多個學科的交會點:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;統計學 (Statistics): 提供對資料分佈、變異與相關的量化理解，確保模型結果具可解釋性。&lt;/li&gt;&#xA;&lt;li&gt;機器學習 (Machine Learning): 提供自動化演算法，從資料中學習模式並進行預測。&lt;/li&gt;&#xA;&lt;li&gt;資料庫系統 (Database Systems): 讓我們能有效率地儲存、檢索與處理大規模資料。&lt;/li&gt;&#xA;&lt;li&gt;資訊視覺化 (Data Visualization): 幫助分析師以直覺方式觀察結構與異常，將複雜資料轉化為可理解的圖像。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;這些領域共同構成資料探勘的基礎，使分析師既能理解數學邏輯，也能掌握資料結構與商業語境，換言之，資料探勘的專業是「跨界的知識整合」。&lt;/p&gt;&#xA;&lt;h2 id=&#34;從資料到知識-探勘的本質&#34;&gt;從資料到知識: 探勘的本質&lt;/h2&gt;&#xA;&lt;p&gt;在商業世界裡，資料無所不在，交易紀錄、網站點擊、感測器訊號、醫療紀錄、社群互動——  這些原始資料就像尚未雕琢的石塊，只有經過提煉與建模，才能顯露出價值。&lt;/p&gt;&#xA;&lt;p&gt;資料探勘的任務，正是讓這些看似無序的碎片，拼出意義的圖像，我認為資料探勘的終極目標，不是找到答案，而是找到能啟發問題的模式，這種模式可能是一個潛在客群、一種異常行為、一組重複的事件序列，或是一個被忽略的關聯。它們都是資料在對我們說：「這裡有值得關注的事。」&lt;/p&gt;&#xA;&lt;h2 id=&#34;資料建模流程-從混沌到結構&#34;&gt;資料建模流程: 從混沌到結構&lt;/h2&gt;&#xA;&lt;p&gt;一個成熟的資料探勘專案，通常包含以下五個階段，這不僅是一套技術流程，更是一種分析思維的框架:&lt;/p&gt;&#xA;&lt;h3 id=&#34;問題定義-problem-definition&#34;&gt;問題定義 (Problem Definition)&lt;/h3&gt;&#xA;&lt;p&gt;資料探勘的起點從來不是資料，而是問題，分析師必須明確回答：「我要解決什麼？」、「我關心的現象是什麼？」，因為清晰的問題定義是所有模型的方向盤。&lt;/p&gt;&#xA;&lt;p&gt;如在商業場景中，這可能是「提高留存率」、「預測顧客流失」或「辨識詐欺行為」；在研究場景中，則可能是「發現新的疾病特徵」或「理解群體行為模式」。&lt;/p&gt;&#xA;&lt;h3 id=&#34;資料蒐集與整合-data-collection--integration&#34;&gt;資料蒐集與整合 (Data Collection &amp;amp; Integration)&lt;/h3&gt;&#xA;&lt;p&gt;資料通常分散在不同來源: 資料庫、API、感測器、或外部平台，探勘的第一步是將這些異質資料整合成可用的結構。&lt;/p&gt;&#xA;&lt;p&gt;這個階段強調資料的「廣度」，因為只有跨域整合，才能看見真正的關聯，正確的資料架構，是所有建模的基礎。&lt;/p&gt;&#xA;&lt;h3 id=&#34;資料清理與前處理-data-cleaning--preprocessing&#34;&gt;資料清理與前處理 (Data Cleaning &amp;amp; Preprocessing)&lt;/h3&gt;&#xA;&lt;p&gt;在現實中，資料是混亂的: 有缺失值、重複值、異常點與不一致的格式，分析師必須進行去噪、正規化、轉換與編碼，使資料能夠被模型正確理解，這個階段決定模型的「穩定性」，就像建築的地基——看不見，但至關重要。&lt;/p&gt;&#xA;&lt;h3 id=&#34;建模與探勘-modeling--mining&#34;&gt;建模與探勘 (Modeling &amp;amp; Mining)&lt;/h3&gt;&#xA;&lt;p&gt;當資料準備就緒，真正的探勘才開始，依照問題性質，我們可能採用不同的建模策略:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;描述性模型 (Descriptive Models) 揭示資料的結構，如分群與關聯規則。&lt;/li&gt;&#xA;&lt;li&gt;預測性模型 (Predictive Models) 預測未來行為，如分類與回歸分析。&lt;/li&gt;&#xA;&lt;li&gt;異常偵測模型 (Anomaly Detection Models) 找出與常態不同的模式，用於詐欺或風險預警。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在這一階段，分析師不僅選擇演算法，更要確保模型的假設與資料特性相符，演算法只是工具，理解資料才是核心。&lt;/p&gt;&#xA;&lt;h3 id=&#34;評估與詮釋-evaluation--interpretation&#34;&gt;評估與詮釋 (Evaluation &amp;amp; Interpretation)&lt;/h3&gt;&#xA;&lt;p&gt;一個模型的成功，取決於它能否「解釋現實」，而不僅是數學上的優越，因此，我們不僅評估準確率或 AUC，更關心模型是否能轉化為可行的洞察。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI 時代下，逐漸消失的初階商業分析師</title>
      <link>https://twcch.io/posts/2025/the-vanishing-junior-business-analyst/</link>
      <pubDate>Sat, 05 Jul 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/the-vanishing-junior-business-analyst/</guid>
      <description>&lt;p&gt;AI 並沒有殺死商業分析師，而是讓人們開始質疑:「商業分析師的價值究竟是什麼?」當企業可以用 ChatGPT、Power BI Copilot 或 Python AutoML 在幾分鐘內完成資料分析報告時，低階 Business Analyst (BA) 的任務，正以驚人的速度被演算法邊緣化。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ai-並非取代而是重新定義工作&#34;&gt;AI 並非取代，而是「重新定義工作」&lt;/h2&gt;&#xA;&lt;p&gt;AI 的力量不在於「取代人」，而在於「重新定義什麼是人應該做的事」，過去的 BA 是資訊流的中介者: 他們收集需求、整理數據、撰寫報告。但在生成式 AI 與自動化報表工具出現後，這些任務已被演算法以更快、更準、更便宜的方式完成，這不只是效率問題，而是結構性變化，所以也許未來的戰場是，如何打造自己的 AI pipeline，讓 AI 成為自己的助理，而不是競爭對手。&lt;/p&gt;&#xA;&lt;p&gt;企業在評估成本時，發現 AI 能生成 80% 的初步報表、70% 的需求文檔、甚至草擬需求訪談總結，因此，初階 BA 的價值被壓縮在最後 20% 的「人工詮釋」中——一個越來越窄、越來越不具議價力的區間。當一個 BA 不思進取，也許未來只剩下做使用者驗收測試的功能，而這樣的角色，隨時可能被廉價勞動力取代。&lt;/p&gt;&#xA;&lt;h2 id=&#34;知識的中介功能正在崩解&#34;&gt;知識的中介功能正在崩解&lt;/h2&gt;&#xA;&lt;p&gt;初階 BA 的工作，本質上是一種「知識中介」: 將資料轉換成報告、將需求轉換成圖表、將複雜的系統語言翻譯成人能理解的文字，但 AI 正在直接壟斷這種中介功能。&lt;/p&gt;&#xA;&lt;p&gt;生成式模型擅長文字與結構轉換——它可以從原始數據生成摘要，從對話生成需求文檔，從 CSV 檔案生成 PowerPoint，在這樣的環境裡，「中介」不再稀缺，「轉譯」不再需要，人類分析師不再是知識的橋樑，而只是被動的確認者。&lt;/p&gt;&#xA;&lt;p&gt;因此，BA 的價值從「生成知識」退化為「驗證知識」，這是一個沉默卻致命的轉變——當人從創造者變成校對者，職業的靈魂也隨之流失。&lt;/p&gt;&#xA;&lt;h2 id=&#34;分析思維的錯位-從邏輯到模板&#34;&gt;分析思維的錯位: 從邏輯到模板&lt;/h2&gt;&#xA;&lt;p&gt;許多初階 BA 其實並不真正「分析」資料，而是依賴模板操作: 根據需求撰寫用例、根據資料套用圖表、根據報告產生建議，這種「模組化思維」極容易被機器學習模仿。&lt;/p&gt;&#xA;&lt;p&gt;AI 並不理解資料的語意，但它能模仿分析的形式，它能生成看似合理的 KPI 報表、風險矩陣與流程分析，因為這些任務本身就被標準化到可機械複製。&lt;/p&gt;&#xA;&lt;p&gt;於是，BA 的競爭對手不再是同事，而是系統內建的「生成分析助手」，當分析成為模板、思維變成巨集，AI 不再只是工具，而是直接侵入職能的定義。&lt;/p&gt;&#xA;&lt;h2 id=&#34;真正不可取代的是思考的維度&#34;&gt;真正不可取代的，是「思考的維度」&lt;/h2&gt;&#xA;&lt;p&gt;這並不意味商業分析師的時代結束，相反地，低階分析正在死去，高階思維才是分析師真正的價值，未來的分析師不再是數據的搬運工，而是問題的設計者。&lt;/p&gt;&#xA;&lt;p&gt;他們的價值在於:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;能定義「應該問什麼問題」&lt;/li&gt;&#xA;&lt;li&gt;能判斷 AI 結果的合理性與偏誤&lt;/li&gt;&#xA;&lt;li&gt;能將數據轉化為策略與組織行動&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;也就是說，真正有價值的分析師，不在於他如何使用工具，而在於他如何重新定義問題、洞察決策邏輯、以及對知識的深層詮釋，AI 可以生成結論，但不能生成意義，這是人類在分析中的最後防線。&lt;/p&gt;</description>
    </item>
    <item>
      <title>當老師只靠 ChatGPT 回信：我們期待的不是答案，而是理解</title>
      <link>https://twcch.io/posts/2025/teaching-without-understanding/</link>
      <pubDate>Wed, 25 Jun 2025 00:00:00 +0800</pubDate>
      <guid>https://twcch.io/posts/2025/teaching-without-understanding/</guid>
      <description>&lt;p&gt;最近，我正在參加一門職訓課程。本來對這堂課滿懷期待，尤其是對某位老師的專業背景很感興趣。不過，隨著課程進行，我漸漸感到一股說不出的落差感：每當我主動提出深入問題，收到的回信卻幾乎都像是 ChatGPT 生成的答案——格式漂亮、邏輯完整、語氣中立，但就是少了「人味」與「針對性」。&lt;/p&gt;&#xA;&lt;p&gt;是的，我知道他不是完全照抄。他有修改、有加註、有整合，但整體感受依然強烈：「這不是一個人對我問題的理解回應，而是一個工具對所有人都能複製的輸出」。&lt;/p&gt;&#xA;&lt;p&gt;這讓我很困惑，甚至有些失望。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我不是反對使用-ai事實上我自己也在用&#34;&gt;我不是反對使用 AI，事實上我自己也在用&lt;/h2&gt;&#xA;&lt;p&gt;先聲明，我並不是那種抗拒 AI 的人。相反地，我本身就是資料分析背景，也有使用 ChatGPT 作為輔助工具的習慣。無論是整理技術架構、釐清概念、或產出初步內容，我完全理解 LLM 在學習與知識組織方面的強大價值。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;但關鍵在於：「角色不同、責任也不同。」&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;身為學習者，我使用 AI 是為了提升效率與學習深度。但作為老師、講師、顧問，使用 AI 不應該只是「產生回答」這麼簡單。&lt;/p&gt;&#xA;&lt;h2 id=&#34;教學不是交付答案而是理解問題的脈絡&#34;&gt;教學不是交付答案，而是理解問題的脈絡&lt;/h2&gt;&#xA;&lt;p&gt;作為學生，我真正期待的，不是單純的一段知識回答，而是來自老師對我所處困境的共鳴與理解。我希望老師能理解我提問背後的「背景」、「盲點」與「問題設計的目的」，並根據這些脈絡回應，而不是直接貼上一段 ChatGPT 輸出的技術解釋。&lt;/p&gt;&#xA;&lt;p&gt;因為我相信，一個真正理解我問題的老師，會根據我當下的能力、背景、甚至目標給出回應——這種回應，不是任何一個 AI 可以「直接」產出的。&lt;/p&gt;&#xA;&lt;p&gt;而當老師只是當 ChatGPT 是一個快捷鍵，那麼學生也很快會意識到：你不是在回答我，你只是在轉寄一份資訊而已。&lt;/p&gt;&#xA;&lt;h2 id=&#34;當教學淪為貼文產出學生會停止問問題&#34;&gt;當教學淪為「貼文產出」，學生會停止問問題&lt;/h2&gt;&#xA;&lt;p&gt;更嚴重的影響是：這樣的互動會直接打擊學生的提問動力。&lt;/p&gt;&#xA;&lt;p&gt;當我發現提問後收到的回應只是套用模板、換個措辭、格式一致卻無深入探討時，我會懷疑：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;我這麼認真思考的問題，真的值得你花時間思考嗎？&lt;/li&gt;&#xA;&lt;li&gt;還是我只是你輸入框中的另一個 prompt？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;久而久之，學生開始不再問問題，也不再相信提問能帶來真正的理解與對話。這對整個學習場域，是一種靜默但致命的傷害。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ai-是輔助不是教學本體&#34;&gt;AI 是輔助，不是教學本體&lt;/h2&gt;&#xA;&lt;p&gt;AI 可以作為老師教學的輔助工具：幫助蒐集資料、釐清知識、快速構思。但它不應該代替老師對學習者的思考與理解責任。在這個知識容易複製的年代，真正無法取代的價值，其實是「對個別學習者的回應能力」。&lt;/p&gt;&#xA;&lt;p&gt;我們當然不會要求每個老師都要一封封親筆手寫、寫出三千字的回信。但至少請不要用 ChatGPT 當成唯一的內容產出來源，更不要用它來「掩蓋」缺乏投入的回應。學習者看得出來，也感受得到。&lt;/p&gt;&#xA;&lt;h2 id=&#34;我寫這篇文章不是為了批評老師而是為了保護教學&#34;&gt;我寫這篇文章，不是為了批評老師，而是為了保護教學&lt;/h2&gt;&#xA;&lt;p&gt;我知道那位老師並不是惡意。他可能工作繁忙、學生太多、壓力很大。我甚至相信他是出於「想要給一個完整答案」的好意才選擇這樣回覆。但我們必須正視一件事：當我們過度依賴工具，而忘記了教學的本質是人與人之間的理解與連結，那麼再強大的 AI 也只會讓教育變得更冷漠、更廉價。&lt;/p&gt;&#xA;&lt;p&gt;我寫這篇文章，是希望提醒每一位教學者：你的價值，不在於你給的答案有多完整，而在於你有多願意理解學生的問題。因為 AI 可以幫你教知識，但唯有你能教會「怎麼成長」。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Archive</title>
      <link>https://twcch.io/archive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/archive/</guid>
      <description></description>
    </item>
    <item>
      <title>Clients</title>
      <link>https://twcch.io/clients/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/clients/</guid>
      <description></description>
    </item>
    <item>
      <title>Elements</title>
      <link>https://twcch.io/elements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/elements/</guid>
      <description>&lt;p&gt;This page serves as a reference for the visual and functional elements used throughout the template. Here you can explore typography, buttons, forms, lists, tables, quotes, and other components to see how they are styled and displayed.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;headings-by-default&#34;&gt;Headings by default:&lt;/h2&gt;&#xA;&lt;h1 id=&#34;h1-default-styles-for-headings&#34;&gt;H1 Default styles for headings&lt;/h1&gt;&#xA;&lt;h2 id=&#34;h2-default-styles-for-headings&#34;&gt;H2 Default styles for headings&lt;/h2&gt;&#xA;&lt;h3 id=&#34;h3-default-styles-for-headings&#34;&gt;H3 Default styles for headings&lt;/h3&gt;&#xA;&lt;h4 id=&#34;h4-default-styles-for-headings&#34;&gt;H4 Default styles for headings&lt;/h4&gt;&#xA;&lt;h5 id=&#34;h5-default-styles-for-headings&#34;&gt;H5 Default styles for headings&lt;/h5&gt;&#xA;&lt;h6 id=&#34;h6-default-styles-for-headings&#34;&gt;H6 Default styles for headings&lt;/h6&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;&#xA;&lt;h3 id=&#34;ordered-list-example&#34;&gt;Ordered list example:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Poutine drinking vinegar bitters.&lt;/li&gt;&#xA;&lt;li&gt;Coloring book distillery fanny pack.&lt;/li&gt;&#xA;&lt;li&gt;Venmo biodiesel gentrify enamel pin meditation.&lt;/li&gt;&#xA;&lt;li&gt;Jean shorts shaman listicle pickled portland.&lt;/li&gt;&#xA;&lt;li&gt;Salvia mumblecore brunch iPhone migas.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;unordered-list-example&#34;&gt;Unordered list example:&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Bitters semiotics vice thundercats synth.&lt;/li&gt;&#xA;&lt;li&gt;Literally cred narwhal bitters wayfarers.&lt;/li&gt;&#xA;&lt;li&gt;Kale chips chartreuse paleo tbh street art marfa.&lt;/li&gt;&#xA;&lt;li&gt;Mlkshk polaroid sriracha brooklyn.&lt;/li&gt;&#xA;&lt;li&gt;Pug you probably haven&amp;rsquo;t heard of them air plant man bun.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;table&#34;&gt;Table&lt;/h2&gt;&#xA;&lt;div class=&#34;table-container&#34;&gt;&#xA;  &lt;table&gt;&#xA;    &lt;tr&gt;&lt;th&gt;Header 1&lt;/th&gt;&lt;th&gt;Header 2&lt;/th&gt;&lt;th&gt;Header 3&lt;/th&gt;&lt;th&gt;Header 4&lt;/th&gt;&lt;th&gt;Header 5&lt;/th&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:1 Cell:1&lt;/td&gt;&lt;td&gt;Row:1 Cell:2&lt;/td&gt;&lt;td&gt;Row:1 Cell:3&lt;/td&gt;&lt;td&gt;Row:1 Cell:4&lt;/td&gt;&lt;td&gt;Row:1 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:2 Cell:1&lt;/td&gt;&lt;td&gt;Row:2 Cell:2&lt;/td&gt;&lt;td&gt;Row:2 Cell:3&lt;/td&gt;&lt;td&gt;Row:2 Cell:4&lt;/td&gt;&lt;td&gt;Row:2 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:3 Cell:1&lt;/td&gt;&lt;td&gt;Row:3 Cell:2&lt;/td&gt;&lt;td&gt;Row:3 Cell:3&lt;/td&gt;&lt;td&gt;Row:3 Cell:4&lt;/td&gt;&lt;td&gt;Row:3 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:4 Cell:1&lt;/td&gt;&lt;td&gt;Row:4 Cell:2&lt;/td&gt;&lt;td&gt;Row:4 Cell:3&lt;/td&gt;&lt;td&gt;Row:4 Cell:4&lt;/td&gt;&lt;td&gt;Row:4 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:5 Cell:1&lt;/td&gt;&lt;td&gt;Row:5 Cell:2&lt;/td&gt;&lt;td&gt;Row:5 Cell:3&lt;/td&gt;&lt;td&gt;Row:5 Cell:4&lt;/td&gt;&lt;td&gt;Row:5 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;tr&gt;&lt;td&gt;Row:6 Cell:1&lt;/td&gt;&lt;td&gt;Row:6 Cell:2&lt;/td&gt;&lt;td&gt;Row:6 Cell:3&lt;/td&gt;&lt;td&gt;Row:6 Cell:4&lt;/td&gt;&lt;td&gt;Row:6 Cell:5&lt;/td&gt;&lt;/tr&gt;&#xA;  &lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;quotes&#34;&gt;Quotes&lt;/h2&gt;&#xA;&lt;h4 id=&#34;a-quote-looks-like-this&#34;&gt;A quote looks like this:&lt;/h4&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Design is not just what it looks like and feels like. Design is how it works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>FAQ</title>
      <link>https://twcch.io/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/faq/</guid>
      <description></description>
    </item>
    <item>
      <title>Hey friends 👋 I’m 志謙</title>
      <link>https://twcch.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/about/</guid>
      <description>&lt;img src=&#34;https://avatars.githubusercontent.com/u/24428408?v=4&#34; alt=&#34;About Me&#34; width=&#34;240px&#34;&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;我是一位居住在雙北的技術愛好者，熱愛探索新科技，喜歡將技術與生活觀察融入文字分享。網站架設是我最早的興趣之一，從早期接觸 Discuz! 6.0 起，曾建立過無數論壇與社群。之後轉向 WordPress，經歷多年實作後，逐漸投入全端網站開發領域，開始享受「從零手刻」構築網站的過程。&lt;/p&gt;&#xA;&lt;p&gt;這個網站主要分享我在 電腦科學、資料科學與人工智慧 相關領域的學習筆記與技術文章，同時也記錄一些關於思考與生活的文字。所有內容皆由本人撰寫，如有任何交流或合作想法，歡迎隨時與我聯繫。&lt;/p&gt;&#xA;&lt;h4 id=&#34;主要學歷&#34;&gt;++主要學歷++&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;國立成功大學 工程科學系 資訊工程與應用組 博士生&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;專業證照&#34;&gt;++專業證照++&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;國際專案管理師 (PMP)&lt;/li&gt;&#xA;&lt;li&gt;美國壽險管理師 (FLMI)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;技術專長&#34;&gt;++技術專長++&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大數據分析、機器學習&lt;/li&gt;&#xA;&lt;li&gt;程式語言: Python (PyTorch)、Java (Spring)、JavaScript (VueJS)&lt;/li&gt;&#xA;&lt;li&gt;資料庫: PostgreSQL&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>iThome 鐵人賽 2025 系列文章</title>
      <link>https://twcch.io/ithome_2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/ithome_2025/</guid>
      <description>&lt;h2 id=&#34;快速掌握資料結構與演算法&#34;&gt;快速掌握資料結構與演算法&lt;/h2&gt;&#xA;&lt;!-- &lt;a href=&#34;https://ithelp.ithome.com.tw/users/20163705/ironman/8468&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;快速掌握資料結構與演算法&lt;/a&gt; --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376664&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-30] (Day 1) 介紹與準備&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376687&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-31] (Day 2) 陣列 (Array)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376831&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-01] (Day 3) 矩陣 (Matrix)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377037&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-02] (Day 4) 鏈表 (Linked List)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377218&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-03] (Day 5) 堆疊 (Stack)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377372&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-04] (Day 6) 隊列 (Queue)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377601&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-05] (Day 7) 二元樹 (Binary Tree)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377826&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-06] (Day 8) 平衡樹 (Balanced Tree)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10377923&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-07] (Day 9) 其他樹 (Other Trees)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10378114&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-08] (Day 10) 圖 (Graph)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10378395&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-09] (Day 11) 演算法評估 (Algorithm Analysis)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10378627&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-10] (Day 12) 氣泡排序 (Bubble Sort)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10378827&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-11] (Day 13) 選擇排序 (Selection Sort)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10379362&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-12] (Day 14) 插入排序 (Insertion Sort)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10379460&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-13] (Day 15) 基礎排序演算法比較&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10379712&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-14] (Day 16) 二元搜尋 (Binary Search)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10379713&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-15] (Day 17) 內插搜尋 (Interpolation Search)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10380112&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-16] (Day 18) 分治法 (Divide and Conquer)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10381704&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-17] (Day 19) 動態規劃 (Dynamic Programming)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10382404&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-18] (Day 20) 貪婪演算法 (Greedy Algorithm)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10382694&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-19] (Day 21) 圖演算法 (Graph Algorithm)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10384113&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-20] (Day 22) Dijkstra 最短路徑演算法 (Dijkstra’s Algorithm)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10384671&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-21] (Day 23) Bellman-Ford 演算法&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10384934&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-22] (Day 24) Floyd-Warshall 演算法&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10386100&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-23] (Day 25) A* 搜尋演算法 (A-star Search)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10386861&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-24] (Day 26) 最小生成樹 (Minimum Spanning Tree)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10387658&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-25] (Day 27) Kruskal 演算法&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10388160&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-26] (Day 28) Prim 演算法&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10388748&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-27] (Day 29) 最小生成樹的實務應用 (Applications of MST)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10389852&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-09-28] (Day 30) 系列結尾&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;30-天入門常見的機器學習演算法&#34;&gt;30 天入門常見的機器學習演算法&lt;/h2&gt;&#xA;&lt;!-- &lt;a href=&#34;https://ithelp.ithome.com.tw/users/20163705/ironman/8136&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;30 天入門常見的機器學習演算法&lt;/a&gt; --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373216&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-01] (Day 1) 介紹與準備&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373217&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-02] (Day 2) 線性迴歸 (Linear Regression)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373364&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-03] (Day 3) 多項式迴歸 (Polynomial Regression)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373418&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-04] (Day 4) 正規化迴歸 (Regularization Regression)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373499&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-05] (Day 5) 邏輯迴歸 (Logistic Regression)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373570&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-06] (Day 6) 邏輯迴歸 (多項式 + 正規化)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373643&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-07] (Day 7) 回顧迴歸：從線性邏輯到學習本質&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373716&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-08] (Day 8) K-近鄰 (K-Nearest Neighbors)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373815&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-09] (Day 9) 樸素貝氏分類器 (Naive Bayes Classifier)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373865&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-10] (Day 10) 支援向量機 (Support Vector Machine)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10373961&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-11] (Day 11) 二元分類任務驗證指標&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374043&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-12] (Day 12) 多元分類任務驗證指標&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374148&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-13] (Day 13) 迴歸任務驗證指標&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374306&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-14] (Day 14) 決策樹 (Decision Tree)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374430&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-15] (Day 15) 隨機森林 (Random Forest)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374527&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-16] (Day 16) K-Means&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374688&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-17] (Day 17) 淺談深度學習 (Deep Learning)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374777&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-18] (Day 18) 全連接神經網絡 (Fully Connected Neural Network)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10374923&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-19] (Day 19) 神經元 (Neuron)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375012&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-20] (Day 20) 激活函數 (Activation Function)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375172&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-21] (Day 21) 卷積神經網絡 (Convolutional Neural Network)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375314&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-22] (Day 22) 深度學習中的正規化與正則化 (Regularization in Deep Learning)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375450&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-23] (Day 23) 深度學習中的優化方法 (Optimization in Deep Learning)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375592&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-24] (Day 24) Adam 優化器 (Adaptive Moment Estimation)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375690&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-25] (Day 25) 循環神經網路 (Recurrent Neural Network)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10375865&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-26] (Day 26) 長短期記憶 (Long Short-Term Memory)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376008&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-27] (Day 27) 閘控循環單元 (Gated Recurrent Unit)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376192&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-28] (Day 28) Seq2Seq (Encoder Decoder with RNN, LSTM, GRU)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376304&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-29] (Day 29) 注意力機制 (Attention Mechanism)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10376490&#34; target=&#34;_blank&#34; ref=&#34;nofollow&#34;&gt;[2025-08-30] (Day 30) 系列結尾&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Videos</title>
      <link>https://twcch.io/videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/videos/</guid>
      <description></description>
    </item>
    <item>
      <title>服務項目</title>
      <link>https://twcch.io/services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/services/</guid>
      <description></description>
    </item>
    <item>
      <title>機器學習</title>
      <link>https://twcch.io/ml_teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/ml_teaching/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Linear Regression&lt;/li&gt;&#xA;&lt;li&gt;Classification&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Logistic Regression&lt;/li&gt;&#xA;&lt;li&gt;K-Nearest Neighbors&lt;/li&gt;&#xA;&lt;li&gt;Navie Bayes&lt;/li&gt;&#xA;&lt;li&gt;[[Support Vector Classifier]]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Tree&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Decision Tree&lt;/li&gt;&#xA;&lt;li&gt;Random Forest&lt;/li&gt;&#xA;&lt;li&gt;Extreme Gradient Descent&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Clustering&#xA;&lt;ul&gt;&#xA;&lt;li&gt;K-Means&lt;/li&gt;&#xA;&lt;li&gt;Density-Based Spatial Clustering of Applications with Noise&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Association Rule&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://twcch.io/posts/machine_learning/apriori/&#34;&gt;Apriori&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FP-Growth&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>統計學</title>
      <link>https://twcch.io/stat_teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/stat_teaching/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Descriptive Statistics&lt;/li&gt;&#xA;&lt;li&gt;Correlation&#xA;&lt;ul&gt;&#xA;&lt;li&gt;++Pearson Correlation (皮爾森相關係數)++&lt;/li&gt;&#xA;&lt;li&gt;Spearman’s Rank Correlation Coefficient (皮斯爾曼等級相關係數)&lt;/li&gt;&#xA;&lt;li&gt;Kendall’s Rank Correlation Coefficient (肯德爾等級相關係數)&lt;/li&gt;&#xA;&lt;li&gt;Cramér’s V Coefficient (克雷莫 V 係數)&lt;/li&gt;&#xA;&lt;li&gt;Estimation and Testing of Correlation Coefficients (相關係數的估計和檢定)&lt;/li&gt;&#xA;&lt;li&gt;Autocorrelation Coefficient (自我相關係數)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Probability&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Events and Probability&lt;/li&gt;&#xA;&lt;li&gt;Principle of Inclusion-Exclusion&lt;/li&gt;&#xA;&lt;li&gt;Random Variables&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Discrete random variables: Bernoulli distribution, Binomial distribution&lt;/li&gt;&#xA;&lt;li&gt;Continuous random variables: Uniform distribution, Normal distribution, Student’s t distribution, F distribution&lt;/li&gt;&#xA;&lt;li&gt;++Probability Density Function (PDF)++&lt;/li&gt;&#xA;&lt;li&gt;++Cumulative Distribution Function (CDF)++&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Expectation and Variance&lt;/li&gt;&#xA;&lt;li&gt;Independent Events and Independent Random Variables&lt;/li&gt;&#xA;&lt;li&gt;++Law of Large Numbers++&lt;/li&gt;&#xA;&lt;li&gt;++Central Limit Theorem++&lt;/li&gt;&#xA;&lt;li&gt;Chebyshev’s Inequality (柴比雪夫不等式)&lt;/li&gt;&#xA;&lt;li&gt;Hoeffding&amp;rsquo;s inequality (霍夫丁不等式)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Probability Distributions&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Bernoulli Distribution, Binomial Distribution&lt;/li&gt;&#xA;&lt;li&gt;Geometric Distribution, Negative Binomial Distribution&lt;/li&gt;&#xA;&lt;li&gt;Poisson distribution (卜瓦松分布)&lt;/li&gt;&#xA;&lt;li&gt;Hypergeometric Distribution (超幾何分布)&lt;/li&gt;&#xA;&lt;li&gt;Uniform Distribution, Exponential Distribution&lt;/li&gt;&#xA;&lt;li&gt;++Normal Distribution++&lt;/li&gt;&#xA;&lt;li&gt;Weibull Distribution, Pareto Distribution, Lognormal Distribution&lt;/li&gt;&#xA;&lt;li&gt;Multinomial Distribution&lt;/li&gt;&#xA;&lt;li&gt;Multivariate Normal Distribution&lt;/li&gt;&#xA;&lt;li&gt;Distribution family tree (visual)&lt;/li&gt;&#xA;&lt;li&gt;Random number generation (RNG)&lt;/li&gt;&#xA;&lt;li&gt;Pseudo-random numbers (Python random)&lt;/li&gt;&#xA;&lt;li&gt;Inverse transform sampling (sampling from a distribution via CDF)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Regression Analysis&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ordinary Least Squares&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Time Series Analysis&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Moving Average&lt;/li&gt;&#xA;&lt;li&gt;Autoregression&lt;/li&gt;&#xA;&lt;li&gt;ARMA/ARIMA Model&lt;/li&gt;&#xA;&lt;li&gt;Seasonality Features&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>聯絡我</title>
      <link>https://twcch.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://twcch.io/contact/</guid>
      <description>&lt;p&gt;聯絡信箱: twcch1218 [at] gmail.com&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
