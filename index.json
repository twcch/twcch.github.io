

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

[{"content":"在前一天，我們整理了深度學習中常見的優化方法，從最基本的隨機梯度下降 (SGD)，到 Momentum、RMSProp、Adagrad 等。今天我們要深入介紹其中最具代表性、也是實務中最常見的優化方法之一——Adam (Adaptive Moment Estimation)。\nAdam 幾乎是深度學習的「預設優化器」。不論是電腦視覺、自然語言處理，還是時間序列預測，只要使用主流深度學習框架 (PyTorch、TensorFlow、Keras)，Adam 幾乎總是第一個被嘗試的選擇。\nAdam 的動機 為什麼會有 Adam? 它的出發點是想同時結合 Momentum 與 RMSProp 的優點:\nMomentum: 透過累積一階動量 (過去梯度的加權平均)，能加速收斂並減少震盪。 RMSProp: 透過維護二階動量 (梯度平方的移動平均)，能自動調整不同維度的學習率。 Adam 的設計思想是: 既要考慮梯度的方向 (Momentum)，又要考慮不同維度的學習率 (RMSProp)，並在數值上進行偏差修正，確保更新穩定。\n數學公式 Adam 的更新規則如下:\n初始化 參數 $\\theta_0$ 一階動量 $m_0 = 0$ 二階動量 $v_0 = 0$ 更新規則 在第 $t$ 次迭代時:\n計算梯度: $$ g_t = \\nabla_\\theta L(\\theta_t) $$\n一階動量 (類似 Momentum): $$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $$\n二階動量 (類似 RMSProp): $$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n","date":"22 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25082201/","title":"(Day 24) Adam 優化器 (Adaptive Moment Estimation)"},{"content":"在前一篇，我們談到了深度學習中的正規化與正則化，重點在於如何避免過擬合並保持訓練穩定。然而，光是解決過擬合還不夠：在龐大的神經網路裡，我們還得面對另一個關鍵問題——如何有效率地找到參數的最佳解。\n這就是「優化方法 (Optimization)」的核心任務。深度學習的訓練，本質上是透過梯度下降類方法來最小化損失函數。但在實務上，單純的梯度下降往往不足，因此衍生出各種改良版演算法。\n問題背景: 為什麼需要不同優化方法？ 在數學理論上，假設我們要最小化的目標函數是損失函數 $L(\\theta)$，其參數更新規則來自於梯度下降:\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t) $$\n其中:\n$\\theta_t$: 第 $t$ 次迭代的參數 $\\eta$: 學習率 (Learning Rate) $\\nabla_\\theta L(\\theta_t)$: 在當前參數下的梯度 這個公式看似簡單，但實務上有幾個挑戰:\n鞍點與局部極小值 高維空間裡，鞍點比局部極小值更常見，導致模型容易「卡住」。 不同方向的梯度尺度差異 某些維度梯度很大、某些維度很小，會造成「之字形震盪」或收斂緩慢。 學習率的設計 學習率太大，模型發散；太小，收斂速度慢甚至陷入次優解。 非凸性 深度網路的損失函數高度非凸，沒有單一「全局最優解」，需要演算法在複雜地形中找到足夠好的解。 因此，雖然梯度下降是核心，但各種優化方法的改進就是針對這些問題而來。\n基本優化方法 批次梯度下降 (Batch Gradient Descent) 一次使用所有樣本計算梯度並更新參數。\n優點: 理論收斂穩定，梯度計算精確。 缺點: 資料量大時，計算昂貴，幾乎不可行。 隨機梯度下降 (Stochastic Gradient Descent, SGD) 每次隨機抽取一筆樣本進行更新:\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t; x^{(i)}, y^{(i)}) $$\n優點: 計算成本低，每次更新快速。 缺點: 梯度估計方差大，收斂過程不穩定。 小批次梯度下降 (Mini-Batch Gradient Descent) 綜合兩者優點，常用一個小批次 (例如 32 或 128 筆樣本) 計算梯度。\n","date":"21 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25082101/","title":"(Day 23) 深度學習中的優化方法 (Optimization in Deep Learning)"},{"content":"在前幾天的文章裡，我們已經從線性迴歸、邏輯迴歸一路走到 CNN (卷積神經網路)，逐步體驗了機器學習與深度學習的不同。到了深度學習階段，模型的複雜度往往大幅增加，參數數量動輒上百萬甚至上億，這也帶來了一個非常嚴重的問題: 過擬合 (Overfitting)。\n今天我們要談的主題「正規化 (Normalization) 與正則化 (Regularization)」，就是專門為了解決這類問題而設計的工具。這兩個詞在中文裡常常被混淆，但在深度學習中有明確的區分:\n正規化 (Normalization): 處理資料或中間層輸出的「分布」，讓訓練更穩定。 正則化 (Regularization): 在模型學習過程中「限制參數自由度」，避免過度擬合。 可以把它們理解成:\n正規化是「讓訓練跑得順暢」 正則化是「讓模型不要學壞」 為什麼需要正規化與正則化? 深度學習的挑戰主要來自於以下幾點:\n參數數量龐大 FCNN、CNN、RNN 等模型的參數動輒上百萬，模型表達能力非常強。這雖然能學習複雜模式，但也極容易記住「訓練資料」而不是「一般化規律」。 梯度傳遞問題 深層網路容易遇到梯度消失或爆炸，導致學習不穩定。 即便是設計良好的激活函數 (如 ReLU)，也可能因資料分布不均而造成某些神經元失效。 資料有限 真實世界中，資料集往往有限，無法支撐一個龐大模型完全「正確」學習。若沒有適當限制，模型就會死記硬背訓練資料，導致測試集表現不佳。 為了應對這些問題，正規化與正則化技術被廣泛應用在深度學習的訓練流程中。\n正規化 (Normalization) 正規化的核心目標是: 讓輸入資料或中間層輸出的數值保持在合理範圍內，以便模型更容易學習。在模型訓練前，我們通常會對輸入資料進行縮放，例如:\nMin-Max Scaling Z-score Standardization 正則化 (Regularization) 正則化的核心目標是：避免模型過擬合，提升泛化能力。\nL1 與 L2 正則化 Dropout 正規化與正則化的互補關係 雖然名稱相似，但正規化與正則化針對的問題不同:\n正規化 → 解決訓練穩定性、加速收斂 正則化 → 解決過擬合、提升泛化 在實務上，它們通常是 同時使用 的。例如:\nCNN: 資料正規化 + Batch Normalization + Dropout + Weight Decay RNN/Transformer: Layer Normalization + Early Stopping + Data Augmentation 結語 深度學習之所以能夠在近十年迅速崛起，不只是因為 GPU 算力提升或資料量增大，還有賴於一系列 正規化與正則化技術 的發展，讓深度模型可以被穩定地訓練並具備良好的泛化能力。\n","date":"20 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25082001/","title":"(Day 22) 深度學習中的正規化與正則化 (Regularization in Deep Learning)"},{"content":"在初步暸解全連接神經網絡 (Fully Connected Neural Network) 後，接下來必須介紹的經典架構就是卷積神經網絡 (Convolutional Neural Network; CNN)。卷積神經網絡可以說是深度學習的代表性架構之一，特別是在電腦視覺 (Computer Vision) 領域幾乎無處不在。\n在我們的日常生活中，從手機的人臉辨識、影像搜尋、自駕車的影像識別，到醫學影像的腫瘤檢測，都可以看到卷積神經網絡的身影。它能在海量影像資料中自動學習特徵，避免了傳統機器學習需要人工設計特徵的困難。\n為什麼需要 CNN？ 在前面介紹的 全連接神經網路 (Fully Connected Neural Network, FCNN) 中，我們知道每一層神經元都與上一層的所有輸入相連。但如果我們把輸入換成圖片，就會發現一個很大的問題:\n假設輸入是一張大小為 $224 \\times 224$ 的彩色圖片，這代表有 $224 \\times 224 \\times 3 = 150,528$ 個像素值。若將這些像素全部展平成一維向量再輸入 FCNN，第一層神經元若有 1000 個，就需要 1.5 億個參數。這樣的參數量幾乎不可訓練，既浪費計算資源，也容易過擬合。 CNN 的設計靈感來自於生物學上對「視覺皮質 (Visual Cortex)」的研究：人類大腦在處理影像時，不是一次性看完整張圖，而是先觀察局部特徵 (邊緣、角落、紋理)，再逐步組合成更高層次的語意 (眼睛、鼻子、車輪等)。 因此 CNN 引入了「局部感受野 (Local Receptive Field)」與「權重共享 (Weight Sharing)」兩個關鍵設計，使模型能夠高效處理圖片，同時降低參數數量。 CNN 的基本組件 CNN 主要由以下幾種層組成:\n卷積層 (Convolutional Layer) 透過多個卷積核從輸入影像中提取特徵。 每個卷積核產生一張特徵圖 (Feature Map)。 通常會搭配激活函數 (例如 ReLU)，引入非線性。 池化層 (Pooling Layer) 池化層的功能是「壓縮資料」並「保留重要特徵」。 常見方法: 最大池化 (Max Pooling): 取區域內的最大值。 平均池化 (Average Pooling)：取區域內的平均值。 池化的好處: 降低資料維度，減少參數數量。 增加特徵的不變性 (例如影像的平移或縮放)。 全連接層 (Fully Connected Layer) 通常位於 CNN 的尾端，將高層特徵輸出轉換為分類結果。 與傳統神經網路類似，最後一層常使用 Softmax 作為多分類輸出。 激活函數 (Activation Function) CNN 常使用 ReLU (Rectified Linear Unit)，定義為:\n","date":"19 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081901/","title":"(Day 21) 卷積神經網絡 (Convolutional Neural Network)"},{"content":"承接昨天的神經元 (Neuron)，神經元的輸出是線性輸出，若僅停留在這個階段，輸出仍然是線性函數，即便我們把很多神經元堆疊在一起，整體模型仍然等效於一個線性轉換，無法捕捉到真實世界中複雜的非線性關係為了解決這個問題，我們需要引入激活函數 (Activation Function) 可以讓模型學習到更複雜的模式 (非線性)，所以激活函數必須是非線性的，這樣才能跟神經元做搭配。\n常見的激活函數 1. Sigmoid 函數 $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n特點:\n輸出範圍在 [0,1] 之間 常用於二元分類問題 缺點: 容易出現梯度消失問題 2. Softmax $$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}} $$\n特點:\n輸出範圍: $(0,1)$，且所有輸出和為 1 特點: 多分類問題的常用輸出層函數 3. ReLU (Rectified Linear Unit) $$ f(x) = max(0, x) $$\n特點：\n輸出範圍：$[0, +\\infty)$ 特點: 簡單高效，目前深度學習最常用的激活函數 缺點: 在 $x \u0026lt; 0$ 的區域梯度為 0，可能導致「神經元死亡」 4. Tanh (雙曲正切函數) $$ \\text{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n","date":"18 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081801/","title":"(Day 20) 激活函數 (Activation Function)"},{"content":"前一篇我們先介紹了全連接神經網絡 (Fully Connected Neural Network)，相信大家還是不太清楚這是什麼，接下來會用幾天的篇幅一一介紹相關的專有名詞，若要理解神經網路，必須先從最小的組成單位 —— 神經元 (Neuron) 開始。\n人工神經元的靈感來自於生物神經科學中「神經元」的概念: 人腦中的神經細胞會接收訊號、處理並傳遞訊號給下一個神經元，形成複雜的網絡。人工神經網路 (Artificial Neural Network, ANN) 正是透過數學方式去模擬這樣的運作。\n神經元數學模型 一個人工神經元的輸入與輸出過程可以寫成:\n$$ z = \\sum\\limits_{i=1}^{n} w_i x_i + b $$\n$$ a = \\sigma(z) $$\n其中:\n$x_i$: 輸入特徵 $w_i$: 權重 (weight)，決定輸入的重要性 $b$: 偏差 (bias)，調整整體的輸入偏移量 $\\sigma(\\cdot)$: 激活函數 (Activation Function)，將線性組合轉換為非線性輸出 $a$: 神經元的輸出 先不看激活函數的部分，是不是第一眼會覺得好像在哪裡看過? 沒錯就是我們 Day 2 介紹的線性迴歸，所以神經元本身並不是什麼太高級的東西，就是一個線性輸出而已，為了要讓神經元能夠模擬更複雜的非線性關係，才會在線性輸出後再加上激活函數 (激活函數是什麼明天介紹)\n神經元的運作流程 以一個簡單的二元分類例子來看\n輸入層: 假設有三個特徵 $x_1$, $x_2$, $x_3$ 加權求和: 每個特徵乘上權重 $w_i$，再加上偏差 $b$ 激活函數：輸入 ReLU 或 Sigmoid，得到輸出 a 傳遞至下一層: 這個輸出作為下一層神經元的輸入 當多層神經元堆疊，就形成所謂的「多層感知機 (MLP)」\n","date":"17 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081701/","title":"(Day 19) 神經元 (Neuron)"},{"content":"在進入深度學習的第一步，必須要先認識最基礎的深度學習架構，這個架構稱為:\n全連接神經網路 (Fully Connected Neural Network; FCNN) 多層感知機 (Multi-Layer Perceptron; MLP) 這兩個都是指同一個東西，這個架構也是所有深度學習架構的基石，CNN、RNN、Transformer 都可以看作是在 FCNN 上加入特殊結構與限制後的延伸\n基本架構 大家可以使用這個 網站 看一下神經網絡的架構與運行過程，簡單來說最基礎的神經網絡架構一定有以下三個 layer:\n輸入層 (Input Layer): 接收原始特徵，大小等於特徵數量 隱藏層 (Hidden Layers): 由多個神經元 (Neurons) 組成，每個神經元與前一層的所有神經元相連 輸出層 (Output Layer): 對應任務需求，分類問題常用 softmax、多類別輸出；回歸問題則可能是單一實值，簡單的區分如下: 回歸問題 輸出層神經元數 = 1 (代表一個連續數值) 常用激活函數: 無激活 (linear) Loss function: MSE, MAE 二元分類 寫法一: 1 個神經元 + Sigmoid 激活 (輸出機率 [0,1]) 寫法二: 2 個神經元 + Softmax (輸出兩類的機率分佈) Loss function: Binary Crossentropy / Categorical Crossentropy ✅ 工程上最常用的是 1 個輸出神經元 + Sigmoid 多元分類 (N 類別) 輸出層神經元數 = N 激活函數: Softmax (保證所有輸出加起來 = 1，形成機率分布) Loss function: Categorical Crossentropy / Sparse Categorical Crossentropy 運作流程 整個神經網絡的運做過程，核心就是「輸入資料 (X) → 正向傳播 (Forward) → 計算 Loss → 反向傳播 (Backward) → 更新參數 (Gradient Descent) → 再正向傳播」，說明如下:\n","date":"16 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081601/","title":"(Day 18) 全連接神經網絡 (Fully Connected Neural Network)"},{"content":"我們機器學習的部分還有 XGBoost、PCA、OneClass SVM 都還沒有談，只是篇幅限制，原本規劃深度學習大概就要花 15 篇左右的內容來談談，如果後續有篇幅我再補充。\n從這篇開始，系列會進入另一個階段: 深度學習 (Deep Learning)。這部分的內容將與前 16 天有明顯不同，因為重點不再是數學公式與演算法推導，而是各種網路架構 (Architectures) 的原理、設計思維與應用。\n機器學習 vs. 深度學習 機器學習 (Machine Learning) 特徵工程往往是關鍵: 要先設計、轉換、挑選特徵，才能把問題丟給模型學習 模型通常較淺，例如: 決策樹、SVM、線性回歸 適合中小型資料集，計算需求相對低 優點: 可解釋性高、對資料量需求不大 深度學習 (Deep Learning) 以多層神經網路為核心，能自動從原始資料中學習特徵 不再依賴繁重的人工特徵設計，例如圖像處理不需要先抽 SIFT/HOG，網路會自己學 特別適合高維度、非結構化資料 (圖像、語音、文字) 需要大量資料與算力支撐，否則容易過擬合 一句話總結: 機器學習的「重點」在於 人設計特徵 → 模型學習規則；深度學習則是 模型自己學特徵 + 規則。\n為什麼需要深度學習？ 處理高維與非結構化資料 傳統 ML 模型處理數值表格非常好用，但面對影像像素矩陣、語音頻譜、自然語言文字序列就顯得力不從心。 深度學習的 CNN、RNN、Transformer 等架構正好能捕捉這些資料的內在結構。 表達能力強大 理論上，一個足夠深的神經網路能近似任何函數 (Universal Approximation Theorem)。 在實務上，這意味著深度學習能擬合遠比傳統演算法更複雜的非線性關係。 自動化特徵抽取 從手動設計特徵到「端到端」學習，深度學習大幅降低了人工干預，提升了跨領域可遷移性。 深度學習的挑戰 資料需求: 需要大量標註資料，否則效果有限 計算需求: 需要 GPU/TPU 等硬體加速 可解釋性低: 模型往往是「黑盒子」，難以直接解釋決策依據 訓練難度: 需要調整大量超參數 (學習率、層數、激活函數、正則化方法等) 結語 深度學習並不是「更高級的機器學習」，而是一種 不同思維：它將特徵學習與規則學習合而為一，讓模型能直接面對複雜且原始的資料。這篇文章作為深度學習的起手式，主要幫助讀者建立心態轉換：從「演算法」的框架，轉向「架構」的世界。\n","date":"15 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081501/","title":"(Day 17) 淺談深度學習 (Deep Learning)"},{"content":"今天本來要說極限梯度提升數 (XGBoost)，但是我發現後面的篇幅可能快不夠了，今天開始的內容會調整成，無監督式學習 → 深度學習 → 如果有時間再回來補充 One-Class SVM 跟 XGBoost。\nK-Means 是一種無監督學習中的聚類演算法，旨在將資料分為 K 個群集，使同一群集內的資料點之間相似度最高，而不同群集之間相似度最低。到這邊可能會有疑問，分類跟聚類差在哪? 分類要有標籤 (監督式學習)，而聚類不需要有標籤 (無監督式學習)，可以想像一下原始資料，如果要訓練分類模型，你在訓練之前就會知道每筆資料要分成什麼類別，但是到了聚類，你的資料完全分不出來該筆資料要分成什麼類別，只知道我這組資料要分成幾群。\n模型介紹 模型邏輯與核心概念 依照 K 的數量，隨機選出 K 個資料點作為初始質心 (centroids) 每筆資料進行窮舉比較與 centroids 的歐幾里得距離，與距離最小的 centroids 為一群 計算每個群集的平均值，為該群集所有資料點的 centroids 重複指派與更新步驟，直到 centroids 不再顯著變動或達到最大迭代次數 Cost Function $$ J = \\sum_{i=1}^K \\sum_{x \\in C_i} |x - \\mu_i|^2 $$\n肘部法 K 值得決定，可以依照肘部法進行判斷 K-Means 的核心弱點 初始中心點敏感問題 (Initialization Sensitivity): 當第一次初始化結果不好，導致誤差就很大，且沒辦法降低怎麼辦，即使資料本身具有明確的群聚特徵，也可能被錯誤分群，可以透過 K-Means++ 解決 替代隨機初始化，透過機率機制選出「較分散」的中心點 能顯著降低收斂到壞解的風險，推薦預設使用 sklearn 中內建支援: KMeans(init=\u0026lsquo;k-means++\u0026rsquo;) (預設就是這個) 優缺點 優點 缺點 計算快速、可擴展、易平行 需先給定 k，對初始化與尺度敏感 易實作、常作為分群基線 假設球形/大小相近群，對離群值非常敏感 可搭配 MiniBatchKMeans 處理大規模資料 對非凸形狀或重疊群表現不佳 模型實作 # Day 16 - K-Means on Iris (library dataset, full pipeline) import seaborn as sns import pandas as pd import numpy as np from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, MiniBatchKMeans from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn.model_selection import ParameterGrid from sklearn.decomposition import PCA import matplotlib.pyplot as plt # 1) 載入資料（無需外部檔案） iris = sns.load_dataset(\u0026#34;iris\u0026#34;) # columns: sepal_length, sepal_width, petal_length, petal_width, species X = iris.drop(columns=[\u0026#34;species\u0026#34;]) y_true = iris[\u0026#34;species\u0026#34;] # 僅作對照評估，不參與訓練（非監督） # 2) 工程化 Pipeline：標準化 + KMeans def make_kmeans_pipe(k): return Pipeline([ (\u0026#34;scaler\u0026#34;, StandardScaler()), (\u0026#34;kmeans\u0026#34;, KMeans( n_clusters=k, init=\u0026#34;k-means++\u0026#34;, n_init=10, # 提升穩定性 max_iter=300, random_state=42 )) ]) # 3) 用 Elbow 與 Silhouette 輔助挑 k K_RANGE = range(2, 8) # Iris 通常 2~6 即可觀察 wcss, sils = [], [] for k in K_RANGE: pipe = make_kmeans_pipe(k).fit(X) labels = pipe.named_steps[\u0026#34;kmeans\u0026#34;].labels_ wcss.append(pipe.named_steps[\u0026#34;kmeans\u0026#34;].inertia_) # WCSS sils.append(silhouette_score(pipe.named_steps[\u0026#34;scaler\u0026#34;].transform(X), labels)) print(\u0026#34;k vs WCSS:\u0026#34;, list(zip(K_RANGE, [round(v,2) for v in wcss]))) print(\u0026#34;k vs Silhouette:\u0026#34;, list(zip(K_RANGE, [round(v,4) for v in sils]))) # 4) 以 k=3 做主實驗（Iris 有 3 個物種） k = 3 pipe = make_kmeans_pipe(k) pipe.fit(X) labels = pipe.named_steps[\u0026#34;kmeans\u0026#34;].labels_ # 5) 非監督情境下的評估指標 sil = silhouette_score(pipe.named_steps[\u0026#34;scaler\u0026#34;].transform(X), labels) # 若有真實標籤可作參考（僅作對照用）：Adjusted Rand Index（ARI） ari = adjusted_rand_score(y_true, labels) print(f\u0026#34;Silhouette Score (k={k}): {sil:.4f}\u0026#34;) print(f\u0026#34;Adjusted Rand Index vs species (k={k}): {ari:.4f}\u0026#34;) # 6) 簡易視覺化（PCA 2D） Z = PCA(n_components=2, random_state=42).fit_transform(pipe.named_steps[\u0026#34;scaler\u0026#34;].transform(X)) plt.figure(figsize=(6,5)) plt.scatter(Z[:,0], Z[:,1], c=labels, s=30) plt.title(f\u0026#34;K-Means (k={k}) on Iris (PCA 2D)\\nSilhouette={sil:.3f}, ARI={ari:.3f}\u0026#34;) plt.xlabel(\u0026#34;PC1\u0026#34;); plt.ylabel(\u0026#34;PC2\u0026#34;) plt.tight_layout(); plt.show() # 7) 進一步：MiniBatchKMeans（大資料時） mbk_pipe = Pipeline([ (\u0026#34;scaler\u0026#34;, StandardScaler()), (\u0026#34;kmeans\u0026#34;, MiniBatchKMeans( n_clusters=k, init=\u0026#34;k-means++\u0026#34;, random_state=42, batch_size=64, n_init=10 )) ]).fit(X) mbk_labels = mbk_pipe.named_steps[\u0026#34;kmeans\u0026#34;].labels_ mbk_sil = silhouette_score(mbk_pipe.named_steps[\u0026#34;scaler\u0026#34;].transform(X), mbk_labels) print(f\u0026#34;MiniBatchKMeans Silhouette (k={k}): {mbk_sil:.4f}\u0026#34;) # 8) 粗略參數掃描（僅示例） grid = {\u0026#34;kmeans__n_clusters\u0026#34;: [2,3,4], \u0026#34;kmeans__n_init\u0026#34;: [10,20]} best = None for params in ParameterGrid(grid): candidate = Pipeline([(\u0026#34;scaler\u0026#34;, StandardScaler()), (\u0026#34;kmeans\u0026#34;, KMeans(init=\u0026#34;k-means++\u0026#34;, max_iter=300, random_state=42))]) candidate.set_params(**params).fit(X) labels_c = candidate.named_steps[\u0026#34;kmeans\u0026#34;].labels_ sil_c = silhouette_score(candidate.named_steps[\u0026#34;scaler\u0026#34;].transform(X), labels_c) if (best is None) or (sil_c \u0026gt; best[0]): best = (sil_c, params) print(\u0026#34;Best by Silhouette:\u0026#34;, round(best[0],4), best[1]) 執行結果 k vs WCSS: [(2, 222.36), (3, 139.82), (4, 114.09), (5, 90.93), (6, 81.54), (7, 72.63)] k vs Silhouette: [(2, 0.5818), (3, 0.4599), (4, 0.3869), (5, 0.3459), (6, 0.3171), (7, 0.3202)] Silhouette Score (k=3): 0.4599 Adjusted Rand Index vs species (k=3): 0.6201 MiniBatchKMeans Silhouette (k=3): 0.4557 Best by Silhouette: 0.5818 {\u0026#39;kmeans__n_clusters\u0026#39;: 2, \u0026#39;kmeans__n_init 結果評估 k 值與 WCSS (Elbow 法) WCSS 從 k=2 到 k=7 持續下降，且在 k=3 後下降幅度明顯趨緩 (139.82 → 114.09 → 90.93) 這符合 Elbow 法的典型訊號：在 k=3 之後邊際效益降低 k 值與 Silhouette Score 最高分在 k=2 (0.5818)，k=3 則下降至 0.4599，之後隨著 k 增加分數持續降低 這意味著，若純粹以內部結構 (凝聚度與分離度) 來衡量，k=2 是最緊密且分離度最佳的分群方案 但 Silhouette 只考慮幾何結構，不代表與實際業務需求或真實標籤完全對應 k=3 的分群表現 Silhouette Score = 0.4599，屬於中等偏弱的群分離度，叢集邊界可能有交疊 Adjusted Rand Index (ARI) = 0.6201：與真實 species 標籤有中等對齊程度，表示模型能部分復原真實物種結構，但錯分率仍顯著 與 MiniBatchKMeans 相比，Silhouette 由 0.4599 降至 0.4557，效能損失不大，但 MiniBatch 在大資料場景下能顯著節省計算時間 參數搜尋結果 最佳 Silhouette 方案是 k=2, n_init=10 (0.5818) 這與初步觀察一致，顯示在內部幾何結構下，Iris 資料更適合分成兩群，但這不符合三物種的領域先驗 結語 K-Means 在本篇範例中清楚展現了它作為無監督式分群基線的特性: 計算效率高、實作簡單、可快速提供初步的資料結構洞察。透過 Elbow 法與 Silhouette Score 的雙重分析，我們觀察到在 Iris 資料集上，幾何結構最佳的分群數為 k=2，而符合領域知識的 k=3 則在分群品質上略顯不足，顯示資料真實分佈與理想分群假設之間的落差。這種差異提醒我們，分群結果不能僅憑數學指標決定，還需結合業務目標與領域先驗做取捨。\n","date":"14 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081401/","title":"(Day 16) K-Means"},{"content":"隨機森林是以「多棵弱學習器 (決策樹)」為基底的集成學習 (Ensemble) 方法，透過資料抽樣 (Bagging) 與特徵隨機子抽樣 (Random Subspace) 降低單棵樹的方差與不穩定性。直覺上，它像是一群專家各自投票: 每位專家 (樹) 看見的資料與特徵都不完全相同，最後以多數決 (分類) 或平均 (回歸) 給出更穩健的預測。\n模型介紹 模型邏輯與核心概念 Bagging (Bootstrap Aggregating): 對原始訓練集做 有放回抽樣，為每棵樹準備一份不同的訓練子集；未被抽到的樣本稱 OOB (Out-of-Bag)，可用來近似泛化誤差。 隨機特徵子集 (max_features): 每個節點分裂時，僅在隨機抽出的特徵子集中尋找最佳分裂，打破樹之間的強相關，進一步降低方差。 投票 / 平均: 分類任務以多數決投票；回歸任務取平均值。 偏差—方差權衡: 相較單棵樹，隨機森林以增加偏差換取顯著降低方差，整體泛化表現通常更佳且更穩定。 模型建構說明 基學習器: 通常使用未剪枝或淺剪枝的 DecisionTree(Classifier|Regressor)。 訓練流程 (分類為例) 進行 B 次 bootstrap 抽樣，得到 B 個訓練子集； 對於第 b 棵樹，每個節點僅在 max_features 個隨機特徵中找最佳分裂； 訓練完成後，以 多數決 聚合各樹的預測； 可用 OOB 樣本估計泛化表現 (oob_score_)。 重要超參數 (分類) n_estimators: 樹的數量 (越多越穩，但成本上升) max_depth、min_samples_leaf: 限制複雜度，抑制過擬合 max_features: 每次分裂可用特徵數 (分類預設 sqrt(p) 常為穩健選擇) class_weight: 處理類別不平衡 oob_score: 啟用 OOB 估計 模型優缺點 優點 缺點 對高維與雜訊相對穩健，較不易過擬合 (相對單樹) 訓練與推論成本高於單樹，難以極致壓縮 幾乎不需特徵縮放，能處理數值＋類別混合 全模型可解釋性較低 (可用特徵重要度、Permutation Importance 緩解) 內建 OOB 評估、特徵重要度 對極度不平衡資料仍可能偏向多數類 (需調 class_weight 或重抽樣) 易於平行化、對超參數敏感度相對低 單棵樹可視覺化但「整體」難以直觀解釋 模型實作 import seaborn as sns import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import ( accuracy_score, classification_report, roc_auc_score, average_precision_score ) from sklearn.inspection import permutation_importance # ----------------------------- # 1) 載入資料（無需外部檔案） # ----------------------------- df = sns.load_dataset(\u0026#34;titanic\u0026#34;) # 2) 目標與特徵（seaborn titanic 欄位） target = \u0026#34;survived\u0026#34; num_features = [\u0026#34;age\u0026#34;, \u0026#34;fare\u0026#34;, \u0026#34;pclass\u0026#34;, \u0026#34;sibsp\u0026#34;, \u0026#34;parch\u0026#34;] cat_features = [\u0026#34;sex\u0026#34;, \u0026#34;class\u0026#34;, \u0026#34;embarked\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;adult_male\u0026#34;, \u0026#34;alone\u0026#34;] X = df[num_features + cat_features] y = df[target].astype(int) # ----------------------------- # 3) 前處理：數值補中位數、類別補眾數 + One-Hot # ----------------------------- numeric_transformer = Pipeline(steps=[ (\u0026#34;imputer\u0026#34;, SimpleImputer(strategy=\u0026#34;median\u0026#34;)) ]) categorical_transformer = Pipeline(steps=[ (\u0026#34;imputer\u0026#34;, SimpleImputer(strategy=\u0026#34;most_frequent\u0026#34;)), (\u0026#34;onehot\u0026#34;, OneHotEncoder(handle_unknown=\u0026#34;ignore\u0026#34;)) ]) preprocess = ColumnTransformer( transformers=[ (\u0026#34;num\u0026#34;, numeric_transformer, num_features), (\u0026#34;cat\u0026#34;, categorical_transformer, cat_features), ] ) # ----------------------------- # 4) 隨機森林模型 # ----------------------------- rf = RandomForestClassifier( n_estimators=400, max_depth=None, min_samples_leaf=2, max_features=\u0026#34;sqrt\u0026#34;, class_weight=\u0026#34;balanced\u0026#34;, # Titanic 類別略不平衡 oob_score=True, n_jobs=-1, random_state=42 ) pipe = Pipeline(steps=[(\u0026#34;preprocess\u0026#34;, preprocess), (\u0026#34;model\u0026#34;, rf)]) # ----------------------------- # 5) 切分資料與訓練 # ----------------------------- X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=42 ) pipe.fit(X_train, y_train) # ----------------------------- # 6) 評估指標 # ----------------------------- y_pred = pipe.predict(X_test) y_proba = pipe.predict_proba(X_test)[:, 1] print(\u0026#34;Test Accuracy:\u0026#34;, round(accuracy_score(y_test, y_pred), 4)) print(classification_report(y_test, y_pred, digits=4)) print(\u0026#34;ROC-AUC:\u0026#34;, round(roc_auc_score(y_test, y_proba), 4)) print(\u0026#34;PR-AUC (Average Precision):\u0026#34;, round(average_precision_score(y_test, y_proba), 4)) rf_model = pipe.named_steps[\u0026#34;model\u0026#34;] if hasattr(rf_model, \u0026#34;oob_score_\u0026#34;): print(\u0026#34;OOB Score:\u0026#34;, round(rf_model.oob_score_, 4)) # ----------------------------- # 7) 取得「展開後特徵名」 # - 與模型看到的列數對齊，避免長度不一致錯誤 # ----------------------------- # One-Hot 展開後的類別名稱 ohe = pipe.named_steps[\u0026#34;preprocess\u0026#34;].named_transformers_[\u0026#34;cat\u0026#34;].named_steps[\u0026#34;onehot\u0026#34;] ohe_names = list(ohe.get_feature_names_out(cat_features)) all_feature_names = num_features + ohe_names # 與模型實際特徵數對齊（保險做法） n_model_features = rf_model.n_features_in_ if len(all_feature_names) != n_model_features: # 若 ColumnTransformer 產生的欄位數與推定不一致（理論上不會），則切齊 all_feature_names = all_feature_names[:n_model_features] # ----------------------------- # 8) 特徵重要度（MDI） # ----------------------------- mdi_importance = pd.DataFrame({ \u0026#34;feature\u0026#34;: all_feature_names, \u0026#34;importance\u0026#34;: rf_model.feature_importances_[:len(all_feature_names)] }).sort_values(\u0026#34;importance\u0026#34;, ascending=False).head(20) print(\u0026#34;\\nTop MDI Importances:\u0026#34;) print(mdi_importance.to_string(index=False)) # ----------------------------- # 9) Permutation Importance（在「前處理後特徵空間」計算） # 這樣 perm.importances_* 的長度就會與 all_feature_names 完全一致 # ----------------------------- X_test_trans = pipe.named_steps[\u0026#34;preprocess\u0026#34;].transform(X_test) # 稀疏或稠密皆可 estimator = pipe.named_steps[\u0026#34;model\u0026#34;] # 直接用 RF 在 transformed space 上做 PI perm = permutation_importance( estimator, X_test_trans, y_test, n_repeats=10, random_state=42, n_jobs=-1 ) perm_importance = pd.DataFrame({ \u0026#34;feature\u0026#34;: all_feature_names, \u0026#34;importance_mean\u0026#34;: perm.importances_mean[:len(all_feature_names)], \u0026#34;importance_std\u0026#34;: perm.importances_std[:len(all_feature_names)], }).sort_values(\u0026#34;importance_mean\u0026#34;, ascending=False).head(20) print(\u0026#34;\\nTop Permutation Importances (transformed space):\u0026#34;) print(perm_importance.to_string(index=False)) 執行結果 Test Accuracy: 0.8324 precision recall f1-score support 0 0.8509 0.8818 0.8661 110 1 0.8000 0.7536 0.7761 69 accuracy 0.8324 179 macro avg 0.8254 0.8177 0.8211 179 weighted avg 0.8313 0.8324 0.8314 179 ROC-AUC: 0.8515 PR-AUC (Average Precision): 0.8335 OOB Score: 0.8216 Top MDI Importances: feature importance fare 0.194484 age 0.145927 adult_male_False 0.115839 adult_male_True 0.080458 sex_female 0.075092 who_man 0.074631 pclass 0.047228 sex_male 0.046897 sibsp 0.037505 class_Third 0.035858 who_woman 0.029911 class_First 0.025534 parch 0.022043 embarked_S 0.017173 embarked_C 0.011033 class_Second 0.010601 alone_False 0.009373 alone_True 0.007693 embarked_Q 0.006361 who_child 0.006358 Top Permutation Importances (transformed space): feature importance_mean importance_std fare 0.070950 0.013466 age 0.031285 0.014823 adult_male_True 0.021788 0.008815 adult_male_False 0.021788 0.008815 who_man 0.021788 0.008815 embarked_S 0.012849 0.006634 class_Third 0.006145 0.008076 embarked_C 0.005587 0.002498 sex_male 0.005587 0.004327 who_woman 0.005028 0.005833 sibsp 0.004469 0.007821 sex_female 0.003911 0.005028 who_child 0.003352 0.003706 parch 0.002793 0.005151 embarked_Q 0.001676 0.003577 pclass -0.000559 0.010133 class_Second -0.004469 0.006017 alone_True -0.005028 0.009497 class_First -0.006704 0.008939 alone_False -0.008939 0.008727 結果評估 整體表現 測試集 Accuracy=0.8324，相較於單棵決策樹的 0.8268 有小幅提升，且 ROC-AUC=0.8515、PR-AUC=0.8335 顯示模型在排序與正類預測能力上更穩定。OOB 分數 0.8216 與測試集表現接近，顯示隨機森林在控制方差與避免過擬合上運作正常。 類別分析 負類 (未存活) Precision=0.8509、Recall=0.8818，正類 (存活) Precision=0.8000、Recall=0.7536，雖然正類召回率較單棵樹略高，但仍有 約 24.6% 的存活樣本被誤判為未存活，在成本敏感場景下 (漏抓正類成本高) 仍需改善。 特徵重要度觀察 MDI (樹內部的 Gini 減少量): fare、age、adult_male_False/True、sex_female、who_man 是主要決策依據，且重要度分佈較單棵樹分散，顯示森林結構引入更多多樣化特徵分裂。 Permutation Importance (泛化貢獻): fare 與 age 在測試集的實際貢獻最高，其餘特徵影響幅度顯著降低，甚至有部分特徵出現負值 (打亂該特徵反而略提升模型表現，代表該特徵在測試集可能引入雜訊)。這提醒我們 MDI 可能高估某些特徵的作用，需同時參考兩種方法做解讀。 下一步建議 提升正類召回率 調整決策閾值 (predict_proba 輸出)，在 Precision 與 Recall 間找到業務可接受的平衡。 結合 class_weight 微調 (如 {0:1, 1:1.5}) 強化正類權重，觀察召回率與 FPR 的變化。 進一步驗證特徵貢獻 對前五大特徵 (fare、age、adult_male、sex_female、who_man) 繪製 Partial Dependence Plot (PDP) 或 Individual Conditional Expectation (ICE)，檢查模型對特徵變化的響應是否符合邏輯與領域知識。 移除 Permutation Importance 為負值的特徵，重新訓練模型，比較 AUC 與穩定性。 模型穩定性檢驗 使用 Stratified K-Fold (k=5 或 10) 交叉驗證回報平均值與標準差，量化隨機森林在不同資料切分下的波動性。 檢查 OOB 與交叉驗證分數的差距，確保模型泛化能力穩定。 集成與對照實驗 與 Gradient Boosting (XGBoost、LightGBM、CatBoost) 比較，特別是在正類召回率與 PR-AUC 上的表現。 若業務需求重視可解釋性，可建立 surrogate decision tree 模型近似隨機森林的決策邏輯，協助非技術人員理解。 結語 隨機森林作為一種集成學習方法，在本篇示例中展現了穩健的泛化能力與優異的整體效能。透過 Bagging 與 隨機特徵子抽樣，它有效降低了單棵決策樹易受資料擾動影響的缺點，並減少了過擬合的風險。與單樹相比，隨機森林在測試集的 Accuracy、ROC-AUC、PR-AUC 及穩定性上皆有顯著提升，且 OOB 分數與測試表現接近，反映出其在實務環境中的可靠性。\n","date":"13 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081301/","title":"(Day 15) 隨機森林 (Random Forest)"},{"content":"Decision Tree 是一種基於條件分支的監督式學習模型，可用於分類與回歸任務。它透過一連串的「是/否」判斷，將資料不斷切分成更純淨的子集，最終形成一個由節點 (Nodes) 與邊 (Edges) 組成的樹狀結構。直覺上，你可以將它想成一連串的決策問句: 「乘客的性別是女性嗎？」 → 是 → 「年齡是否小於 12 歲？」 → 是 → 存活機率高。其最大特色是 可解釋性高，每個決策規則都能清楚對應到特徵與閾值，便於與非技術背景的利害關係人溝通。\n模型介紹 模型邏輯與核心概念 決策樹 (Decision Tree) 是一種監督式學習演算法，可用於分類與回歸任務。模型的結構類似一棵樹，由節點 (Node) 與分支 (Branch) 組成\n內部節點 (Internal Node): 表示一個特徵的條件判斷 葉節點 (Leaf Node): 對應模型的預測結果 (類別或數值) 建構過程是「貪婪式遞迴分割 (Greedy Recursive Splitting)」，根據資訊增益 (Information Gain) 最大或吉尼不純度 (Gini Impurity) 最小的原則進行\n模型建構說明 決策樹使用「貪婪演算法」建構模型，並無使用梯度下降類優化器，主要特徵:\n每次分裂僅考慮當前最佳特徵 (不做全局最優) 使用遞迴方式構建整棵樹 終止條件包括: 節點樣本數過小 不再能提升資訊增益 最大深度限制 節點的特徵選擇: 每次分裂節點時，會從候選特徵中選出最佳分割特徵，其依據如下: DecisionTreeClassifier Information Gain Gini Impurity DecisionTreeRegressor MSE 減少量 MAE 減少量 模型優缺點 優點 缺點 模型結構直觀，可視覺化 易過擬合，尤其是深樹 對特徵縮放不敏感 (不需標準化) 對資料擾動敏感 (小變化可能改變樹結構) 可處理數值與類別型特徵 單棵樹表現有限 (通常需配合集成方法如 Random Forest, XGBoost) 支援特徵重要性評估 分裂偏向多取值的特徵 (需調整) 模型實作 程式實例 import matplotlib.pyplot as plt import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree # 讀取資料 data = pd.read_csv(\u0026#39;titanic.csv\u0026#39;) # 特徵與標籤 features = [\u0026#34;Pclass\u0026#34;, \u0026#34;Sex\u0026#34;, \u0026#34;Age\u0026#34;, \u0026#34;Fare\u0026#34;] data = data.dropna(subset=[\u0026#34;Age\u0026#34;]) # 簡單處理缺失值 X = pd.get_dummies(data[features]) # One-Hot Encoding y = data[\u0026#34;Survived\u0026#34;] # 訓練 / 測試切分 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 建立模型 clf = DecisionTreeClassifier( criterion=\u0026#39;gini\u0026#39;, max_depth=4, random_state=42 ) clf.fit(X_train, y_train) # 評估 print(\u0026#34;Accuracy: \u0026#34;, clf.score(X_test, y_test)) # 視覺化 plt.figure(figsize=(15,8)) plot_tree(clf, feature_names=X.columns, class_names=[\u0026#39;Not Survived\u0026#39;, \u0026#39;Survived\u0026#39;], filled=True) plt.show() 執行結果 Test Accuracy: 0.8268 precision recall f1-score support 0 0.8319 0.9000 0.8646 110 1 0.8167 0.7101 0.7597 69 accuracy 0.8268 179 macro avg 0.8243 0.8051 0.8122 179 weighted avg 0.8260 0.8268 0.8242 179 Top Feature Importances: feature importance adult_male_True 0.588332 class_Third 0.185393 fare 0.156906 age 0.045641 class_Second 0.014194 parch 0.009535 embarked_S 0.000000 alone_False 0.000000 adult_male_False 0.000000 who_woman 0.000000 who_man 0.000000 who_child 0.000000 embarked_C 0.000000 embarked_Q 0.000000 class_First 0.000000 結果評估 模型整體準確率尚可，但對「正類 (存活)」的 召回率僅 0.71，代表仍有 約 29% 的存活樣本被判成未存活；若你的業務目標重視「找出潛在存活 (或正類)」的能力，現況偏保守。 對正類的錯失 (FN) 明顯多於將負類誤判為正類 (FP)。在成本敏感情境（例如「漏抓到的正類」代價更高）下，這是不理想的權衡。 下一步建議 我會先驗證的 6 件事:\n","date":"12 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081201/","title":"(Day 14) 決策樹 (Decision Tree)"},{"content":"分類任務有混淆矩陣作為指標的核心基礎，迴歸任務則建立在誤差分佈 (Error Distribution) 之上。所有迴歸指標，都是在真實值與預測值的差異上進行數學運算。迴歸的評估相對分類簡單，沒有多種 TP、FP 的組合，但每個指標關注的面向、對異常值的敏感度、在商業決策上的意義卻各有不同。\n誤差的基本概念 回歸任務中，誤差 (Error) 定義為: $$ e_i = y_i - \\hat{y}_i $$\n其中:\n$y_i$: 第 i 筆資料的真實值 $\\hat{y}_i$: 模型的預測值 $e_i$: 第 i 筆的殘差 (Residual) 所有回歸評估指標，都是將 $e_i$ 做數學運算後的結果。\n常見回歸驗證指標 Mean Squared Error (MSE) 將每個誤差平方後取平均。平方的動作會讓大誤差的影響成倍放大，因此 MSE 對異常值 (outlier) 非常敏感。\n$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n特性分析\n誤差平方 → 大誤差懲罰重，小誤差影響相對被稀釋。 單位為「原單位的平方」，如價格 (元) 預測的 MSE 單位是「元²」，因此不易直接解讀大小。 優點\n適合用在不能容忍大誤差的場景，例如財務風險控管 (預測錯 10 倍金額的後果極其嚴重)。 在模型優化 (特別是最小平方法回歸) 中，MSE 作為損失函數具有良好數學性質 (平滑、可微分)。 缺點\n","date":"11 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081101/","title":"(Day 13) 迴歸任務驗證指標"},{"content":"多元分類任務驗證指標就只是從二元分類任務驗證指標延伸而來，核心概念一樣是二元分類任務驗證指標，而兩者只是在計算內容上有些許差異，所以指標仍然是 Accuracy、Recall、F1-score 等，所以請讀者先將二元分類任務驗證指標熟練後再來看。\n指標介紹 混淆矩陣 (Confusion Matrix) 假設我們有 3 個類別 [A, B, C]，模型的預測結果如下:\n實際 \\ 預測 A B C A 50 2 3 B 4 45 1 C 5 2 43 列 (row) = 實際標籤 行 (column) = 模型預測 對角線 (50、45、43) = 預測正確的數量 從多元分類矩陣取出 TP / FP / FN / TN 對單一類別 (One-vs-All)，例如要計算「類別 A」的指標:\nTP_A = 混淆矩陣中 A–A 的數字 = 50 FP_A = 預測為 A 但實際不是 A 的數字總和 = 4 (B→A) + 5 (C→A) = 9 FN_A = 實際為 A 但預測不是 A 的數字總和 = 2 (A→B) + 3 (A→C) = 5 TN_A = 其他所有正確分類的數字 = 全部總數 - (TP_A + FP_A + FN_A) 這樣每個類別都能得到對應的 TP、FP、FN、TN\n","date":"10 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25081001/","title":"(Day 12) 多元分類任務驗證指標"},{"content":"今天要介紹的是常見的分類任務驗證指標，會以二元分類問題為例，因為多元分類也是用相同的指標，只是計算方式會有所不同而已，預計會用 2-3 天的篇幅介紹完，分類與迴歸任務的驗證指標；先給各位讀者一個正確的觀念，選指標時必須回到業務背景與資料特性，不要迷信某個數值越高越好，真正有價值的模型評估，是能在技術表現與業務需求之間找到平衡。\n指標介紹 混淆矩陣 (Confusion Matrix) 分類任務的所有核心指標，幾乎都來自 Confusion Matrix，它是用來統計分類模型在測試集上的結果，Confusion Matrix 在 Binary Classification 問題上，它是一個 2x2 表格:\nTrue Condition - Positive True Condition - Negative Predict Outcome - Positive TP (True Positve) FP (False Positve) (誤報) Predict Outcome - Negative FN (False Negative) (漏報) TN (True Negative) TP: 實際是 Positive，模型也預測 Positive (預測正確) TN: 實際是 Negative，模型也預測 Negative (預測正確) FP: 實際是 Negative，但模型預測 Positive (誤報 / 假警報) (Type I error) FN: 實際是 Positive，但模型預測 Negative (漏報 / 漏檢) (Type II error) Accuracy (準確率) $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n","date":"9 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080901/","title":"(Day 11) 二元分類任務驗證指標"},{"content":"終於來到 SVM，這也是本系列介紹 Machine Learning 中分類演算法的最後一個，當然在機器學習中還有很多的監督式分類演算法，我個人認為相對沒我介紹的這幾個經典，就留給讀者自行學習。從明天開始到進入樹模型之前，我會補充一下，模型 Validation Index 的內容 (用來衡量模型結果好不好)，因為前面飆的有點快，後來有發現這部分也很重要，預計會花 2 ~ 3 天的篇幅來介紹。\n我們就進入正題，支援向量機 (Support Vector Machine) 是一種監督式學習演算法，泛指支援向量機演算法框架，透過在特徵空間中尋找最能分隔不同類別的超平面 (hyperplane)，並最大化分類邊界 (margin)，可應用於:\n分類 (Classification) 回歸 (Regression) 異常檢測 (Anomaly Detection) 但是回歸的部分非常少用到 Support Vector Regression 本系列就不說明這塊；至於異常檢測的應用又稱 OneClass SVM 目前沒有規劃，這是一種無監督式學習的技術，專門在做 Anomaly Detection 的任務，因為本系列規劃在樹模型介紹完成後，會進入深度學習篇章，所以 OneClass SVM 的部分如果後續有篇幅的話會再補充，如果沒有也請讀者自行學習；所以本篇會以 SVM 應用在分類任務 (Support Vector Classification) 上來詳細說明。\nSVM 解決了什麼問題? 在詳細介紹 SVM 之前，要先說明一下 SVM 到底要解決什麼問題，我們先回到 Day 5 介紹的 Logistic Regression，假設同一組數據做分類，可能會發生以下狀況，我們先看到 Logistic Regression 的部分，大家會發現看起來分類正確，但是那條線怎麼切得怪怪的，這也是 Logistic Regression 的問題，會造成模型泛化性不夠好，因為 Logistic Regression 對於這部分沒有進行處理。\n圖片來源: https://b5031631512567.medium.com/logistic-regression-羅吉斯回歸-support-vector-machine-svm-做a-b分類-82aa5e5edaf8\n","date":"8 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080801/","title":"(Day 10) 支援向量機 (Support Vector Machine)"},{"content":"前幾天的討論中，我們已經探討了迴歸分析、邏輯迴歸，以及最近兩天介紹的 K-Nearest Neighbors (KNN)。今天要討論的是另一種基礎且直覺性極強的分類演算法: 樸素貝氏分類器 (Naive Bayes Classifier)。儘管樸素貝氏分類器的基本原理非常簡單，甚至經常被視為基礎模型，但在實務應用中，它仍然是許多場合的首選，尤其是在文本分類領域，例如垃圾郵件分類與情感分析。\n模型介紹 模型邏輯與核心概念 Naive Bayes 的核心思想來自貝氏定理 (Bayes\u0026rsquo; Theorem):\n$$ P(y|X) = \\frac{P(X|y)P(y)}{P(X)} $$\n$P(y|X)$: 在給定特徵 $X$ 下的目標 $y$ 的後驗機率 (posterior probability) $P(X|y)$: 在已知目標 $y$ 下觀察到特徵 $X$ 的可能性 (likelihood) $P(y)$: 目標 $y$ 的先驗機率 (prior probability) $P(X)$: 觀察到特徵 $X$ 的總體機率 但直接計算 $P(X|y)$ 是困難的，尤其當特徵數量龐大且互相關聯時。因此 Naive Bayes 做了一個極簡的假設——「條件獨立假設 (Conditional Independence Assumption)」，即假設特徵之間彼此獨立:\n$$ P(X|y) = P(x_1|y) \\times P(x_2|y) \\times \\cdots \\times P(x_n|y) $$\n這個假設大幅簡化了問題，讓計算變得非常快速且易於實現。雖然這個假設在現實世界中往往不成立，但 Naive Bayes 的實務表現卻通常仍然相當穩健。\nNaive Bayes 常見種類 Gaussian Naive Bayes (高斯樸素貝氏): 假設特徵為連續數值，並服從高斯分布。 Multinomial Naive Bayes (多項式樸素貝氏): 特別適用於文本數據，特徵通常為計數 (例如詞頻)。 Bernoulli Naive Bayes (伯努利樸素貝氏): 特徵為二元變數 (例如詞的出現與否)。 適用情境 特徵數量大且離散，尤其文本分類 需要模型快速訓練與預測 基準模型 (Baseline Model) 的建立 限制條件 特徵之間存在強烈相關性時，效果可能較差 無法捕捉特徵之間的交互作用 模型實作 本次實作會以多項式 Naive Bayes 為例，因為它在文本分類中表現卓越，並且可展示 Naive Bayes 的強項: 速度快、表現穩定且容易理解。我們將使用經典的 SMS Spam Collection 資料集，透過 Naive Bayes 分辨垃圾訊息與正常訊息，這個過程就不過多敘述。\n","date":"7 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080701/","title":"(Day 9) 樸素貝氏分類器 (Naive Bayes Classifier)"},{"content":"K-近鄰 (K-Nearest Neighbors; KNN) 是一種很直學的機器學習演算法。它沒有模型參數、沒有訓練過程，卻可以在某些任務上有不錯的效果。它的核心理念只有一句話: 「你是誰，由你周圍最像你的人決定。」\nK-近鄰的預測邏輯其實就是投票機制。當一筆新資料進來時，K-近鄰會計算它與訓練集中每一筆資料的距離，選出最近的 K 筆，根據這些鄰居的標籤來進行分類或回歸。\n舉個例子，如果你住進一個新的社區，而這個社區 5 戶人家中有 4 戶都是教師，那麼你很可能也被視為教師。這就是K-近鄰的基本邏輯：用「距離」定義相似度，用「投票」進行預測。\n無需訓練、實作簡單 可處理多類別分類問題 非常適合 baseline 模型或少量資料的場景 模型介紹 模型邏輯與核心概念 運作原理 定義距離度量: 最常見的是歐幾里得距離。 標準化資料: 避免不同特徵尺度影響距離計算。 選擇 K 值: K 值太小容易過擬合，太大容易欠擬合。 查找最近鄰: 找出距離最近的 K 筆資料。 分類或回歸: 分類就多數決，回歸就取平均。 模型評估指標 Accuracy: 整體正確率 Precision / Recall / F1-score: 評估正例預測品質與召回 適用情境 資料量不大、特徵數量低的任務 資料本身具備明顯群聚性質 需要快速做出初步 baseline 的時候 限制條件 計算成本高 (尤其資料量大時) 對資料標準化非常敏感 高維度下效果會大幅下降 (維度災難) 模型實作 這個 K-近鄰的案例，我們來聊聊簡單的操參數實驗，我們先準備一組資料，這個過程就不過多敘述。\nimport numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score, classification_report from sklearn.model_selection import cross_val_score # 資料產生 X, y = make_classification( n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep=1.2, random_state=42 ) # 資料分割與標準化 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train_std = scaler.fit_transform(X_train) 在建模的部分就跟之前不一樣，而是在外層寫了一個迴圈，因為 K-近鄰的 K 值，沒有人知道要用多少，K=1 表示我只抓最近的一個來比，完全就沒有那種投票的概念，所以 k 不應該選 1，再來是怕有平票的問題所以 k 會以奇數為主，而且 k 如果太小會有個問題，容易過擬合，越小越準，那怎麼辦? 所以這邊搭配了 Cross Validation 做設計，可以避免這個問題 (Cross Validation 請讀者自行找資源學習)。\n","date":"6 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080601/","title":"(Day 8) K-近鄰 (K-Nearest Neighbors)"},{"content":"前面 5 天我們聚焦於「回歸系列」模型: 線性迴歸 (Linear Regression)、多項式迴歸 (Polynomial Regression)、正則化迴歸 (Lasso / Ridge / ElasticNet Regression) 以及邏輯迴歸 (Logistic Regression)。雖然它們名稱上都掛著「Regression」，實則涵蓋了連續值預測與分類任務兩大主題。\n在正式進入其他學習範式前，我想透過這篇文章做一個小結，幫助讀者重新理解「迴歸模型的核心精神」，並進一步延伸思考「什麼是機器學習的學習」。\n迴歸模型統整與對比 模型 任務類型 是否可擴展非線性 是否有正則化 適用場景 代表限制 Linear Regression 迴歸 否 否 數據關係明確線性、特徵少時 對離群值、共線性敏感 Polynomial Regression 迴歸 ✅ 否 存在非線性曲線關係時 過度擬合風險高 Lasso / Ridge / ElasticNet 迴歸 ✅ ✅ 高維度資料、需特徵選擇時 模型可解釋性略減 Logistic Regression 分類 否 ✅ (可搭配) 二元分類、機率預測、可解釋性要求高場景 不適合複雜非線性邊界 這四種模型本質上都假設資料可以被一個「參數化的函數」所建模，且可以透過某種「最小化損失」的方式來進行學習。而這種最小化行為，正是機器學習中最常見的學習模式: 梯度下降法 (Gradient Descent)。\n為什麼梯度下降能「學習」? 這是一個我自己也還在思考的問題。梯度下降看似只是數學上的最小化技巧，但其實它蘊含了學習的邏輯核心: 錯誤導向的自我修正。\n每一次模型的預測錯了，就利用這個錯誤的方向與程度，去修正模型的參數，使下一次預測更好。這種機制背後隱含的三個條件，值得特別點出:\n✅ 存在可微分的損失函數 ✅ 模型是參數化的 (parameters 可調整) ✅ 可以反覆試誤 (迭代優化) 符合上述條件，模型便可以「學習」。也正因如此，這四個回歸模型雖然類型不同 (分類 / 迴歸)、形式不同 (線性 / 非線性 / 正則化)，但都共享「透過梯度下降調整參數」這一關鍵本質。\n","date":"5 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080501/","title":"(Day 7) 回顧迴歸：從線性邏輯到學習本質"},{"content":"在上一篇中，我們深入介紹了邏輯迴歸的模型邏輯、損失函數與分類行為。這篇則要進一步延伸這個經典模型，回答一個關鍵問題: 邏輯迴歸能否結合多項式特徵與正規化機制，來對抗非線性與過擬合問題?\n在實務中，這樣的需求非常常見，但你可能很少看到「多項式邏輯迴歸」或「正規化邏輯迴歸」這樣的說法。雖然命名不常見，但本質上邏輯迴歸完全可以與這兩個技巧結合使用，而且這種搭配在複雜資料下是極具威力的實務技巧。\n為什麼邏輯迴歸可以搭配多項式與正規化? 邏輯迴歸其實是線性模型 邏輯迴歸雖然應用在分類任務f，但本質仍是一種「線性模型」:\n$$ \\hat{y} = \\sigma(\\beta_0 + \\mathbf{x}^\\top \\boldsymbol{\\beta}) $$\n這表示它只能建構一條線性的 decision boundary。當你的資料本身具有非線性邊界時，例如 XOR 類型的資料，這條邏輯迴歸線就顯得力不從心。\n解法之一，就是在原始特徵上做多項式擴展 (Polynomial Feature Expansion)——也就是增加特徵空間的非線性組合，例如 $x_1^2$、$x_1 \\cdot x_2$ 等，來幫助模型在更高維度中建立線性可分的邊界。\n這與之前我們在線性迴歸所談的邏輯迴歸原理一樣，只是這次應用在分類問題中。\n邏輯迴歸也容易過擬合 一旦你使用多項式特徵，特徵數暴增，就可能發生過擬合，這時就需要正規化 (Regularization) 機制來抑制模型複雜度。\n與 Linear Regression 一樣，邏輯迴歸可以透過 L1 或 L2 懲罰項達到正規化的目的:\nL2 (Ridge): 抑制權重值變得太大 L1 (Lasso): 推動部分權重變為 0，具有特徵選擇效果 Elastic Net: L1 + L2 混合調整 值得注意的是，在 PyTorch 中，optimizer 的 weight_decay 只對應 L2，若要做 L1，則需自行加上額外的懲罰項。\n模型實作 這個案例也一樣，使用 PyTorch 來實現，透過這段程式碼來窺探 Logistic Regression 的細節。但是還是要再次聲明一下，不論是機器學習演算法，還是說什麼排序的那些算法，你自己寫的打概率打不過這種主流套件做出來的方法，因為這些方法可能經過 10 幾年以上的迭代，不斷地維護與優化產生的，所以如果是學習的話可以自己做，但是正式要使用的話還是建議直接用這些現成的方法，表現往往更加優秀。\n","date":"4 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080401/","title":"(Day 6) 邏輯迴歸 (多項式 + 正規化)"},{"content":"邏輯迴歸 (Logistic Regression) 是一種常見的分類模型，主要用於預測二元分類或多元分類，有別於先前的線性迴歸是用來預測無邊界的連數據值，而邏輯迴歸間單來說就是預測有邊界的不連續數值，如 [0, 1], [1, 2, 3]。\n模型介紹 模型邏輯與核心概念 那邏輯回歸是如何運作? 其實不論是哪種邏輯迴歸，底層都是先透過線性迴歸來預測，只是分別透過不同的激活函數與損失函數來處理，但是邏輯迴歸一般來說還是比較常用於二元分類，來看看以下流程:\n假設有一條線性迴歸方程式: $\\hat{y} = \\beta_0 + \\mathbf{x}^\\top \\boldsymbol{\\beta}$。(注意: 這條不是最佳的線性迴歸線) 會針對前述的線性迴歸方程式結果，透過 sigmoid 函數，將結果轉換成 [0, 1] 假設損失函數 (Cost Function): Binary Cross Entropy 最後使用梯度下降 (Batch Gradient Descent) 來最小化損失函數，找出最佳的邏輯迴歸線 以上就是二元分類邏輯迴歸的原理，那麼我們來看看多元分類邏輯迴歸是如何處理\n假設有一條線性迴歸方程式: $\\hat{y} = \\beta_0 + \\mathbf{x}^\\top \\boldsymbol{\\beta}$。(注意: 這條不是最佳的線性迴歸線) 會針對前述的線性迴歸方程式結果，透過 softmax 函數，將結果轉換成機率總和為 1 的組合 假設損失函數 (Cost Function): Categorical Cross Entropy 最後使用梯度下降 (Batch Gradient Descent) 來最小化損失函數，找出最佳的邏輯迴歸線 可以看出不同的邏輯迴歸，只是分別透過不同的激活函數與損失函數來處理，雖然邏輯迴歸可以用於多元分類，但是一般來說還是比較常用於二元分類。\n模型評估指標 Accuracy: 整體正確率 Precision / Recall / F1-score: 評估正例預測品質與召回 ROC-AUC: 考量不同閾值下模型分類能力 Confusion Matrix: TP、TN、FP、FN 分佈 Log Loss: 概率預測與實際標籤差異 適用情境 Target 為二元分類 (0/1、是/否) 或多元分類 需要同時獲得概率估計與可解釋性 限制條件 多重共線性: 高度相關特徵會影響係數穩定性 極端值敏感: 離群點可能顯著扭曲模型 模型實作 這個案例開始為了讓讀者有更好的感覺模型的過程，會分別使用 sklearn 與 PyTorch 來建模。但是必須先聲明，無論是手動撰寫或是透過 PyTorch 來模擬出來，都不一定有辦法比 sklearn 提供的演算法來得更優秀，所以除非有特殊目的，否則使用 sklearn 提供的演算法效能與準確性都會較高。\n","date":"3 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080301/","title":"(Day 5) 邏輯迴歸 (Logistic Regression)"},{"content":"延續昨日的多項式迴歸中，我們觀察到一個現象: 雖然二次特徵提升了模型的表現，但同時也引入過擬合 (Overfitting) 風險。這是因為當特徵數量暴增，模型就會變得過於「貪婪」，試圖將每個資料點都擬合得極好，結果反而喪失了在新資料上的泛化 (Generalization) 能力。\n那怎麼辦? 就是在多項式迴歸的基礎上，限制模型的自由度，也就是今天要介紹的——正則化回歸 (Regularized Regression)。\n這是一種透過在模型參數加上限制，以提升泛化能力 (該操作並非為了提高準確度)，讓它在「解釋資料」與「控制複雜度」間取得平衡。最常見的三種正則化技術分別為:\n套索回歸 (Lasso Regression): L1 Normalization 脊回歸 (Ridge Regression): L2 Normalization Elastic Net Regression: L1 + L2 Normalization 模型介紹 模型邏輯與核心概念 先回到 Day 2 的線性迴歸，線性迴歸如何找出最佳的迴歸線?\n先設定損失函數 (Cost Function) 假設為 $MSE = \\frac{1}{2n} \\sum\\limits_{i=1}^{n} (y_{i} - \\hat{y}_{i})^{2}$。 再使用梯度下降 (Batch Gradient Descent) 來最小化損失函數。 而所謂的正規化迴歸就是在損失函數加上懲罰項，而前述那些不同的正規化迴歸名稱，就只是懲罰項的差異而已，以下是正規化迴歸的懲罰項:\n套索迴歸: $\\lambda \\sum |\\beta_i|$ 脊迴歸: $\\lambda \\sum \\beta_i^2$ Elastic Net Regression: $\\lambda_1 \\sum |\\beta_i| + \\lambda_2 \\sum \\beta_i^2$ 我們先來看看這幾種正規化的效果差異:\n","date":"2 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080201/","title":"(Day 4) 正規化迴歸 (Regularization Regression)"},{"content":"昨天介紹了線性迴歸 (Linear Regression)，它適合用來處理特徵與目標之間為線性關係的情境。然而，真實世界的資料往往並非純粹線性，而是呈現複雜的非線性關係，例如曲線、拋物線、甚至更複雜的波動趨勢。\n就有了多項式特徵 (Polynomial Feature) 的出現，而線性迴歸搭配多項式特徵，就是所謂的多項式迴歸 (Polynomial Regression)，便是為了解決線性模型難以處理的非線性問題。它的核心概念非常簡單就是透過對特徵進行多項式轉換，使模型能夠捕捉非線性趨勢。\n模型介紹 模型邏輯與核心概念 這塊幾乎與昨天介紹的線性迴歸一樣，重複的部分就不多做介紹。因為多項式迴歸本質上仍是線性迴歸，但特徵空間經過非線性轉換，讓模型能擬合更複雜的曲線。以下為多項式迴歸的公式:\n$$ \\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d $$\nd 稱為 polynomial degree (多項式階數)，是模型中最重要的超參數之一。 特徵不只可以加入單一變數的高次項，也可加入多個變數間的交互項 (例如 $x_1x_2$)。 運作原理 假設方程式 (degree = 3): $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$ 透過將輸入特徵 $x$ 映射為高階次多項式 (如 $x^2, x^3, \\dots$)，使模型能擬合彎曲或非線性趨勢，特徵會經過變換形成新的變數，然後再應用一般線性回歸模型進行估計。 degree = 3 (對所有 features 做所有「總次數 ≤ 3」的項次組合) 多項式特徵的處理會產生新的特徵 要特別注意，如果在特徵工程有人工建立交互項，不可直接使用 PolynomialFeatures 來處理，因為不會辨識你手動做出的交互項，會產生重複或邏輯不一致的問題，要特別處理。 舉例: 假設有一組資料，特徵有 [\u0026lsquo;x1\u0026rsquo;, \u0026lsquo;x2\u0026rsquo;]，設定 degree=3 做 PolynomialFeatures，這組資料的特徵會變成 [\u0026lsquo;x1\u0026rsquo;, \u0026lsquo;x2\u0026rsquo;, \u0026lsquo;x1^2\u0026rsquo;, \u0026lsquo;x1 x2\u0026rsquo;, \u0026lsquo;x2^2\u0026rsquo;, \u0026lsquo;x1^3\u0026rsquo;, \u0026lsquo;x1^2 x2\u0026rsquo;, \u0026lsquo;x1 x2^2\u0026rsquo;, \u0026lsquo;x2^3\u0026rsquo;]，他會自動做交互項處理，如果有手動生成交互項就不能再做 PolynomialFeatures 適用情境 當資料呈現曲線趨勢時，線性回歸無法捕捉其變化 限制條件 degree 過高，容易導致 Overfitting (尤其在資料量小時) 高維度下容易產生特徵爆炸 對比 Linear Regression 其模型可解釋性下降 模型實作 資料集介紹 將使用經典的 Boston Housing Dataset 為例。由於 scikit-learn 已移除該資料集，我們改採自 Carnegie Mellon University 所提供的公開版本。樣本內容如下:\n","date":"1 August 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25080101/","title":"(Day 3) 多項式迴歸 (Polynomial Regression)"},{"content":"線性迴歸 (Linear Regression) 是統計學中的一種預測方法，主要分為簡單線性迴歸 (Simple Linear Regression) 與多元線性迴歸 (Multiple Linear Regression)，又稱複迴歸，以及其他變形的迴歸等，但在線性迴歸中，通常會有 1~N 個自變數 (Independent Variable) X，也可以稱作特徵 (Feature)；和 1 個因變數 (Dependent Variable) Y，也可以稱作目標 (Target)。而最終目的就是找出一條最佳迴歸線，來擬合這些數據點，便可以用來預測未來的數據點。\n模型介紹 模型邏輯與核心概念 線性迴歸假設 統計學線性迴歸的經典的五大假設:\n線性關係: 自變數與因變數之間存在線性關係 誤差項獨立 (Independence): 誤差項之間沒有相互關係 同標準差性 (Homoscedasticity): 對於所有的自變數，誤差項具有相同的標準差 誤差項常態性 (Normality of Errors): 誤差項應該成常態分佈 高度共線性 (Multicollinearity): 自變數間高度線性相關 看到這邊會想說，為什麼要特別註明統計學? 跟機器學習無關? 先記住一句話「統計學重推論，機器學習重預測」，很多假設跟機器學習中的線性迴歸模型還真的沒有太大的關係，但是也不代表，機器學習模型完全沒有假設，但是相對比較不重要，這也是為什麼很多仿間的機器學習教材都會忽略假設這塊。\n總而言之，機器學習模型不像統計學模型需要那麼嚴謹的假設，但是若違反某些假設，也是會影響機器學習模型的表現，也會使得模型只能用於預測，無法用於推論，以下簡單整理假設對統計模型與機器學習模型的影響:\n假設 對傳統統計模型影響 對機器學習影響 建議處理方式 線性關係 ✅ 極高 (核心假設) ❌ 可忽略 (可透過特徵轉換處理) 用非線性模型 / 特徵轉換 誤差獨立性 ✅ 高 (推論與解釋需此條件支持) ✅ 高 (對 generalization 有直接影響) 使用適當資料分割策略 同變異性 ✅ 中高 (影響參數估計的信度) ❌ 可忽略 (模型的估計值仍然準，但 p-value、CI 失真) 變數轉換、加權最小平方法 誤差常態性 ✅ 中高 (特定推論工具須常態性支持) ❌ 可忽略 若僅做預測可忽略 共線性 ✅ 高 (嚴重影響模型可解釋性與推論) ❌ 可忽略 (但建議修正以利解釋) VIF、降維、正則化 運作原理 我們先回到線性迴歸的用途與目的，簡單來說就是「找出一條最佳直線，來擬合這些數據點，便可以用來預測未來的數據點」，如何找出最佳直線? 本文會簡單的介紹一下，詳細過程與原理，再請讀者自行尋找其他資源暸解。\n","date":"31 July 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073101/","title":"(Day 2) 線性迴歸 (Linear Regression)"},{"content":"在學習機器學習 (Machine Learning) 的過程中，可能會陷入兩種極端，一種是只會調用套件 (套模)，模型背後的機制一知半解，遇到問題只能「換模型試試看」，或者是過度陷入數學細節，花大量時間推導公式，卻無法轉化為實際應用與模型選擇能力。\n我本身是從商業分析背景轉入人工智慧領域的研究者。這段轉型過程中，逐漸體會到: 真正困難的不是學會用模型，而是理解模型為什麼有效、什麼時候該用、什麼時候該換、用了之後該觀察什麼訊號。這促使我開始重新梳理各類常見演算法的行為與應用邏輯。\n因此，我決定透過這次 iThome 鐵人賽的機會，整理與統整常見演算法的核心概念，並將每一篇視為一場與模型的深度對談。\n系列架構說明 本系列分為兩大部分:\n經典機器學習模型: 聚焦於 Regression、Classification、Clustering 等常見方法，強調模型背後的核心邏輯、適用情境與評估指標。 深度學習模型: 介紹常見神經網路架構，如全連接神經網路 (FCNN)、CNN、RNN、Transformer 等，並探討它們對資料型態、任務種類的適應性與限制。 每篇文章皆會包含模型概念說明與簡潔的 Python 範例實作，並聚焦於模型本身的行為與選擇策略，不深入探討資料前處理、特徵工程、模型調參、數學推導等高階內容，以避免模糊焦點。\n技術範圍與預期對象 本系列預設讀者已具備以下條件:\n具備基礎統計學與資料科學知識 具備基本 Python 語法能力 具備 scikit-learn, PyTorch, TensorFlow, Keras 基本建模流程 學習深度定位: 聚焦在 Level 2–3 之間 等級 定義 在本系列的實踐目標 Level 1 會用套件建模 ✅ 使用 sklearn 等工具快速建模 Level 2 理解模型的概念與原理 ✅ 說得出每個模型的邏輯與核心機制 Level 3 能比較模型優劣與應用場景選擇 ✅ 理解適用時機、模型之間的 trade-off Level 4+ 深入優化與理論推導 🚫 本系列不會深入涵蓋，建議另尋高階資源 系列預告與進展節奏 本系列將以「一日一模型」為目標，每篇聚焦於一個經典或常見模型，從實用視角出發說明其:\n核心邏輯與設計理念 適用情境與限制條件 與其他模型的比較與選擇策略 Python 範例實作與評估觀察 預計涵蓋模型範圍包括: Linear Regression、Polynomial Regression、Logistic Regression、SVM、KNN、Decision Tree、Random Forest、XGBoost、PCA、KMeans、FCNN、CNN、RNN、Transformer \u0026hellip; 等。\n","date":"30 July 2025","permalink":"http://twcch.io/posts/column_article/ironman_2025_30%E5%A4%A9%E5%85%A5%E9%96%80%E5%B8%B8%E8%A6%8B%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95/articles_25073001/","title":"(Day 1) 介紹與準備"},{"content":"近年來，我觀察到無論在台灣還是中國，「學歷無用論」的聲音愈發強烈。許多人開始質疑讀書是否還有意義，認為不靠學歷反而更能致富，網紅、直播、投資客、白手起家的商人充斥版面，讀書人反倒被視為落後者、被剝削者、社會規訓的犧牲品，但我有不同的看法。\n為什麼「學歷無用論」會出現？ 我認為這不是單純的個人選擇，而是結構性問題:\n教育擴張讓學歷貶值，碩博士已成基本門檻 階級複製讓弱勢者難以翻身，「努力不再保證回報」 高回報的機會集中在少數風口行業，炒短線者當道 社群媒體製造「一夜暴富」神話，反智氛圍蔓延 「讀書沒用」已經不只是判斷，更是一種情緒對制度失望、對未來無感、對努力失信。\n那讀書究竟還有沒有價值? 如果只探討「賺多少錢」作為唯一標準，那的確讀書沒有用，因為讀書不一定有最即時的回報。但若把時間尺度拉長、把價值層次拉高，會發現:\n學習不是為了立刻賺錢，而是為了讓你能夠分辨真偽、建立邏輯、理解世界、保有尊嚴地思考與行動\n工具層面: 讓你擁有專業能力，立足於社會 認知層面: 訓練你思辨、整合、表達的能力 存在層面: 引導你認識自己、世界與人生的關係 簡單來說，我 30 年的人生觀告訴我，讀書可以讓我的境界提升，能夠自我昇華\n我們為什麼會覺得「努力應該有回報」? 許多陷入犬儒的人會說：「我不是不努力，我努力過了，沒用。」\n這恰恰反映出對努力的誤解，努力是會失敗的，而且也從來不會立即兌現，它更像是一種長期累積的複利，過程中你會改變視角、強化心智，最終與眾不同\n真正的努力，不只是行為上的執行，更是認知上的轉變。不是用熱血硬幹，而是用策略、用反思、用節奏走出自己的路。\n那我們應該怎麼做？ 不要盲目追求文憑，但也不要過早放棄學習 看懂社會結構的變動，但也要打造自己可控的核心能力 知道短期內「投機」可能勝出，但長期是價值與認知力的勝利 一段送給未來自己的話 讀書不是為了成功，而是為了不被世界輕易騙走 當社會用最廉價的快樂來交換你一生的時間時 教育與自省，是你最後的防線\n哪怕這個時代看起來對知識不再禮遇，我仍願意選擇學習，因為我相信厚積才能薄發，深耕才能穿透表象，真正理解世界，並在其中活出自己的姿態\n","date":"16 July 2025","permalink":"http://twcch.io/posts/articles_25071601/","title":"學歷無用? 我仍相信學習的價值"},{"content":"這是一個資料視覺化專案——「Dynamic Visualization: 200 Countries, 200 Years, 4 Minutes」。它將涵蓋 1816 至 2016 年，200 個國家的歷史變遷以互動動畫呈現，整體動畫長度約四分鐘，旨在結合「時間」與「地理」維度，提供用戶沉浸式的歷史視覺體驗。\n成品呈現頁面: https://twcch.io/TwoHundredYearsTwoHundredCountries/views.html\nGitHub 原始碼: https://github.com/twcch/TwoHundredYearsTwoHundredCountries\n專案目標: 動態傳遞跨時代趨勢 我這次的核心目的，是打造一段「高品質又美觀」的互動式動畫。相比靜態圖表，此動畫能讓使用者更直覺地感受到全球歷史變化的脈絡與節奏。\n跨國維度: 一次呈現 200 國家在相同指標上的變化 跨年代視角: 覆蓋整整兩個世紀 互動與美感: 最終以 Plotly Express 強化動畫的動態感與互動性 這是一個典型的「Proof of Concept」，驗證我能用純 Python 開源工具在本地完成動態資料視覺化，而不是依賴商業軟體。\n處理流程解析 資料擷取與清理 使用 pandas 從 Gapminder 或其他開源來源讀入年份、國家與指標。 透過 core/data.py 標準化欄位名稱、處理缺值、並轉換為長型結構，以利後續分析。 寫入 SQLite 為了方便查詢與存取，我用 core/sqlite_db.py 將清理後的資料匯入 SQLite 資料庫，一併記錄 metadata。 產生視覺化資料表 scripts/build_view_table.py 將資料按年與國家展開，組合成完整用於視覺化的 DataFrame。 動態驗證：matplotlib 原型 在 proof_of_concept.py 中，以 matplotlib 建立由靜態圖逐幀拼湊的基本動畫，確認播放邏輯與視覺節奏。 互動動畫：Plotly Express 最終在 plot_with_px.py 中改以 Plotly Express，產出包含滑動條、國家標籤、時間軸與音效的四分鐘互動畫面，並輸出至 docs/views.html。 技術選擇與實務考量 資料處理: pandas 濾除缺值、重塑表格、處理 metadata，全套操作都在 pandas 中完成。 儲存管理: 使用 SQLite 儲存資料，方便查詢與重複執行，而不用每次都從頭開 CSV。 動畫原型: matplotlib 可迅速驗證概念、調整幀率與時間間隔。 互動視覺化: Plotly Express 能更快速加入滑桿、hover 標籤，動畫更加流暢美觀，也更適合網頁展示。 展示成果 最終輸出是一個 HTML 檔，內嵌動態 html5 視覺化:\n","date":"9 July 2025","permalink":"http://twcch.io/posts/projects/articles_25070901/","title":"視覺化專案 - 200 個國家 200 百年 4 分鐘"},{"content":"這是一個資料科學專案，目標是透過 Kaggle 經典的 Titanic 生存預測題目，建立一套結構清晰、模組化的預測系統。我不只是想交出一份準確的預測結果，更希望藉由這個專案練習:\n如何設計可擴充、可維護的資料分析架構 如何把模型訓練與推論流程標準化 如何用設定檔 (config-driven) 控制整個 pipeline 如何實踐工程導向的資料科學流程 GitHub 原始碼: https://github.com/twcch/TitanicSurvivalPrediction\n專案定位：不只是「解題」，而是「設計一套解法系統」 我不滿足於單純把資料丟進模型調整參數。我希望打造的是一個「可重複使用的機器學習預測框架」，因此我做了以下幾點設計:\n架構模組化: 依照功能拆分為 data/, features/, models/, utils/，程式碼清楚分工 流程自動化: 所有步驟都由 main.py 控制，方便一鍵執行與重現實驗 設定檔驅動: 核心設定集中管理於 config.json，可以快速切換特徵、模型參數與輸出路徑 可擴充性設計: 未來若要換模型、加特徵、改評估指標，幾乎不需改動主程式碼 這些設計不只是在技術上提升效率，也讓我在做資料科學時，更接近實務工作者的思維模式\n資料前處理與特徵工程: 每個欄位都要能「解釋」 我對特徵的要求是: 不只要對模型有用，更要有邏輯、可解釋\n處理缺失值 Age 用中位數填補 Embarked 用眾數填補 Fare 缺值極少，仍完整處理 創造新特徵 FamilySize = SibSp + Parch: 模擬家庭是否有互助效果 選定使用特徵 類別型: Pclass, Sex, Embarked, Title 數值型: Age, Fare, FamilySize 模型設定: 我選擇 XGBoost，但更重視可控性 雖然這個任務可以用很多模型解，但我選擇以 XGBoost 為主模型，理由如下:\nTree-based 模型不需要特徵標準化，工程處理更簡潔 對類別特徵與數值特徵的混合表現良好 在 Kaggle 類似任務中表現穩定，可作為 baseline 模型訓練與推論流程 整個流程包含以下幾步，由 main.py 控制:\n","date":"30 June 2025","permalink":"http://twcch.io/posts/projects/articles_25063001/","title":"實戰專案 - Titanic 生存預測專案"},{"content":"最近，我正在參加一門職訓課程。本來對這堂課滿懷期待，尤其是對某位老師的專業背景很感興趣。不過，隨著課程進行，我漸漸感到一股說不出的落差感：每當我主動提出深入問題，收到的回信卻幾乎都像是 ChatGPT 生成的答案——格式漂亮、邏輯完整、語氣中立，但就是少了「人味」與「針對性」。\n是的，我知道他不是完全照抄。他有修改、有加註、有整合，但整體感受依然強烈：「這不是一個人對我問題的理解回應，而是一個工具對所有人都能複製的輸出。」\n這讓我很困惑，甚至有些失望。\n我不是反對使用 AI，事實上我自己也在用 先聲明，我並不是那種抗拒 AI 的人。相反地，我本身就是資料分析背景，也有使用 ChatGPT 作為輔助工具的習慣。無論是整理技術架構、釐清概念、或產出初步內容，我完全理解 LLM 在學習與知識組織方面的強大價值。\n但關鍵在於：「角色不同、責任也不同。」\n身為學習者，我使用 AI 是為了提升效率與學習深度。但作為老師、講師、顧問，使用 AI 不應該只是「產生回答」這麼簡單。\n教學不是交付答案，而是理解問題的脈絡 作為學生，我真正期待的，不是單純的一段知識回答，而是來自老師對我所處困境的共鳴與理解。我希望老師能理解我提問背後的「背景」、「盲點」與「問題設計的目的」，並根據這些脈絡回應，而不是直接貼上一段 ChatGPT 輸出的技術解釋。\n因為我相信，一個真正理解我問題的老師，會根據我當下的能力、背景、甚至目標給出回應——這種回應，不是任何一個 AI 可以「直接」產出的。\n而當老師只是當 ChatGPT 是一個快捷鍵，那麼學生也很快會意識到：你不是在回答我，你只是在轉寄一份資訊而已。\n當教學淪為「貼文產出」，學生會停止問問題 更嚴重的影響是：這樣的互動會直接打擊學生的提問動力。\n當我發現提問後收到的回應只是套用模板、換個措辭、格式一致卻無深入探討時，我會懷疑：\n我這麼認真思考的問題，真的值得你花時間思考嗎？ 還是我只是你輸入框中的另一個 prompt？ 久而久之，學生開始不再問問題，也不再相信提問能帶來真正的理解與對話。這對整個學習場域，是一種靜默但致命的傷害。\nAI 是輔助，不是教學本體 AI 可以作為老師教學的輔助工具：幫助蒐集資料、釐清知識、快速構思。但它不應該代替老師對學習者的思考與理解責任。在這個知識容易複製的年代，真正無法取代的價值，其實是「對個別學習者的回應能力」。\n我們當然不會要求每個老師都要一封封親筆手寫、寫出三千字的回信。但至少請不要用 ChatGPT 當成唯一的內容產出來源，更不要用它來「掩蓋」缺乏投入的回應。學習者看得出來，也感受得到。\n我寫這篇文章，不是為了批評老師，而是為了保護教學 我知道那位老師並不是惡意。他可能工作繁忙、學生太多、壓力很大。我甚至相信他是出於「想要給一個完整答案」的好意才選擇這樣回覆。但我們必須正視一件事：當我們過度依賴工具，而忘記了教學的本質是人與人之間的理解與連結，那麼再強大的 AI 也只會讓教育變得更冷漠、更廉價。\n我寫這篇文章，是希望提醒每一位教學者：你的價值，不在於你給的答案有多完整，而在於你有多願意理解學生的問題。因為 AI 可以幫你教知識，但唯有你能教會「怎麼成長」。\n","date":"25 June 2025","permalink":"http://twcch.io/posts/articles_25062501/","title":"當老師只靠 ChatGPT 回信：我們期待的不是答案，而是理解"},{"content":"在資料科學領域中，對企業進行舞弊檢測 (Fraud Detection) 被視為是一種分類問題: 輸入企業相關的數據，輸出舞弊或非舞弊。然而，真正投入研究後會發現，這個問題很難解決，非常具挑戰性。\n我目前主要研究方向，是運用人工智慧 (Artificial Intelligence) 技術，來解決企業進行財務報表舞弊的問題。這類型的議題與銀行信用卡詐欺、保險業中的理賠舞弊、甚至洗錢行為有相似之處，都是稀有事件、後知後覺、動態進化的「敵對性問題」。\n為什麼這不是一個單純的分類問題？ 在傳統機器學習框架下，分類問題的成功往往來自於充足的標記數據、清晰的邊界條件與相對穩定的資料分佈。然而，舞弊行為恰恰違反了這三項假設。\n可以從以下幾點具體說明：\n極度不平衡的資料 (Class Imbalance)\n在實務資料中，舞弊案件往往只佔所有資料的極小比例，可能是千分之一、甚至萬分之一。這意味著如果你採用傳統的精確度 (accuracy) 作為衡量指標，模型即使完全忽略舞弊也能達到 99% 以上的準確率，但這顯然毫無意義。\n標籤不完整且滯後揭露 (Label Latency \u0026amp; Missing Labels)\n很多舞弊行為要經過數月、甚至數年後才會被調查揭露，更遑論那些永遠未被發現的案件。這使得訓練資料的標籤具有高度不確定性，導致模型容易學到錯誤的決策邊界。\n舞弊技術持續演化 (Concept Drift)\n犯罪者會根據監管與模型檢測方式持續更新手法，導致模型在部署後迅速失效。這使得即使當下訓練準確的模型，也難以長期維持效能。\n異常並非來自單一特徵，而是整體脈絡的矛盾 (Contextual Inconsistency)\n財報舞弊往往不是單一財務指標異常，而是多個指標之間出現結構性不一致。例如: 營收大增但現金流卻大減、獲利提升但存貨異常膨脹。這種多變量脈絡異常，遠比簡單的 outlier detection 更為複雜。\n問題不是模型選得不夠好，而是問題設定錯了 如果僅停留在「用哪個模型比較準」、「要不要用 XGBoost 還是 LSTM」這種層級的思考，只會陷入技術細節的死胡同，無法解決核心困難。相反地，我認為更關鍵的兩個研究方向是：\n如何讓 AI 自己找到潛在的舞弊標籤？\n採用自監督學習 (Self-Supervised Learning)，不依賴人工標註，而是讓模型自行從大量正常樣本中學習「常態結構」，再對偏離常態的資料進行異常評分，進一步推論出可能的舞弊行為。\n如何讓深度模型的決策可以被人類審計人員理解？\n深度學習模型雖然強大，但往往是黑箱。導入可解釋性方法 (如 SHAP、LIME、Attention 可視化)，可以提升金融監理與內部稽核部門的信任與採用意願，也為模型導入實務場域鋪路。\n這不只是建模問題，更是科學問題 用 AI 解決舞弊，不是一場簡單的技術堆疊競賽，而是對整個金融風險邏輯、舞弊行為模式、以及資料特性深刻理解的綜合挑戰。這將是我博士研究的起點，從理解問題本質出發，探索如何用 AI 技術建立可行的風險偵測系統，不只是要「分類得準」，更要讓人「信得過」。我認為這是一條難走的路，但也因此充滿價值。\n","date":"2 June 2025","permalink":"http://twcch.io/posts/articles_25060201/","title":"為什麼用 AI 技術檢測企業舞弊，比想像中更困難？"},{"content":"五年前，我踏入壽險產業，成為一名 Business Analyst (BA)。當時的工作內容相當清晰：需求文件撰寫、報表製作、簡單的數據分析與溝通協調，是我每天的日常。那時候，這些任務仍需靠人力一項一項完成，效率與品質全憑個人經驗與熟練度。但如今，這些「核心能力」正快速被人工智慧工具重塑，甚至取代。\n我親身感受到的衝擊：工具進步得比我想像中快 當我首次使用大型語言模型 (LLM) 工具進行報告撰寫與 Python 代碼產出時，內心的震撼難以言喻。曾經需要數小時才能完成的分析報告，在幾分鐘內生成雛形；曾經為了釐清邏輯關係而反覆修改的流程圖，如今只需一句指令就能完成。\n我逐漸意識到，這並不是單一任務被加速，而是整個 BA 工作流程正被結構性重塑。換言之，AI 正在壓縮 BA 的邊際價值。這不只是主觀體感，更有明確的研究支持。根據美國 OpenAI 與賓州大學的聯合研究，約有 80% 的職業至少有 10% 的工作內容將受到 LLM 工具的影響；其中，高達 19% 的職業，其超過一半的工作可由 AI 完成。而 BA——尤其是負責初階文件處理、標準報表、流程規劃等工作的分析師，被列為高曝險族群。\n原因很明確因為 BA 所擅長的文字組織、資料彙整與需求敘述，正是 AI 最擅長模仿與執行的任務。\n市場變化正在發生，不是未來式，而是現在進行式 這樣的趨勢已經開始反映在勞動市場上：\nBA 的職缺增速放緩\n企業在內部導入 LLM 工具後，發現許多重複性任務可由 AI 初步完成，人力需求自然下降。\n初階 BA 的薪資競爭力下降\n對於只熟悉基本分析任務、無法主動創造洞察的人才，企業的願意支付薪資上升空間有限。\n高階 BA 的需求反而上升\n企業更看重能駕馭 AI 工具、快速整合資訊、提出具策略意義建議的分析人才。所謂「會用 AI 的人」，正逐步取代「被 AI 取代的人」。\n這代表，AI 並不是取代所有 BA，而是取代了不進化的 BA。\n我的轉型：從反應式執行者，到主動創造者 面對這樣的劇變，我無法視若無睹。\n於是我開始盤點自己的能力，明白單靠產業 Know-how 或不夠專精的技能，將難以應對未來的競爭。因此，我選擇投入更深層的技能學習: 博士班訓練，深度的掌握資料科學、機器學習與深度學習領域，建立自身的核心競爭力。不只是會用工具或套模，而是要能理解這些如何運作、能應用在哪些情境、又有何種侷限。這樣的技能，不僅能讓我在日常分析中脫穎而出，也為我開啟進入 AI 應用領域的可能性。\n結語：AI 不會毀滅職涯，但它會重寫價值分佈 AI 並不會取代 BA，但它會重新定義 BA 的角色。\n","date":"7 April 2025","permalink":"http://twcch.io/posts/articles_25040701/","title":"為什麼 AI 正在快速削弱低階 Business Analyst 的價值？"}]